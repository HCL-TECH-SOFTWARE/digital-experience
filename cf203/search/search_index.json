{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"overview/","text":"Product Overview HCL Digital Experience (DX) is the trusted solution for an enterprise's digital experience at the intersection of processes, content, and applications. HCL DX is architected to build more than just websites. It is built Cloud Native to serve your customers, employees, and partners and to deliver the functionality and stability that they need and expect. You can customize HCL DX to meet the needs of your organization, users, and user groups. You can adapt the look and feel of the site to fit the standards of your organization and to customize page content for users and groups in accordance with business rules and user profiles. Users, such as business partners, customers, or employees, can further customize their own views of the sites, as permitted. Users can add applications to pages and arrange them as they want. By aggregating portlets in one place and giving users the power to customize their own desktops, HCL DX gives users a means for doing business efficiently and with high satisfaction. For Practitioners HCL DX includes a range of tools for managing content and maintaining web and mobile digital experiences. Those sites can blend applications and content together, in a targeted way according to the end user. Tools for practitioners include content management, asset management, site building via Design Studio and Site Manager, personalization management and more. For Developers HCL DX provides a wide range of APIs and tools to enable developers to customize the digital experience. Developers can use REST and Java APIs to access the HCL DX capabilities. They can use Script Applications to produce JavaScript-based applications. Java Portlets written to the JSR168 or JSR286 specification can be deployed too. The DXClient tool can be used to deploy and manage applications, and to manage servers. For System Administrators HCL DX can be deployed in a traditional IBM\u00ae WebSphere Application Server\u00ae cluster, or it can be deployed to a Kubernetes environment. HCL DX provides a range of scriptable interfaces to enable automation of your infrastructure management. Note that some services, such as Digital Asset Management, Design Studio, Experience API, and Content Composer, are available only in the Kubernetes environment.","title":"Product Overview"},{"location":"overview/#product-overview","text":"HCL Digital Experience (DX) is the trusted solution for an enterprise's digital experience at the intersection of processes, content, and applications. HCL DX is architected to build more than just websites. It is built Cloud Native to serve your customers, employees, and partners and to deliver the functionality and stability that they need and expect. You can customize HCL DX to meet the needs of your organization, users, and user groups. You can adapt the look and feel of the site to fit the standards of your organization and to customize page content for users and groups in accordance with business rules and user profiles. Users, such as business partners, customers, or employees, can further customize their own views of the sites, as permitted. Users can add applications to pages and arrange them as they want. By aggregating portlets in one place and giving users the power to customize their own desktops, HCL DX gives users a means for doing business efficiently and with high satisfaction.","title":"Product Overview"},{"location":"overview/#for-practitioners","text":"HCL DX includes a range of tools for managing content and maintaining web and mobile digital experiences. Those sites can blend applications and content together, in a targeted way according to the end user. Tools for practitioners include content management, asset management, site building via Design Studio and Site Manager, personalization management and more.","title":"For Practitioners"},{"location":"overview/#for-developers","text":"HCL DX provides a wide range of APIs and tools to enable developers to customize the digital experience. Developers can use REST and Java APIs to access the HCL DX capabilities. They can use Script Applications to produce JavaScript-based applications. Java Portlets written to the JSR168 or JSR286 specification can be deployed too. The DXClient tool can be used to deploy and manage applications, and to manage servers.","title":"For Developers"},{"location":"overview/#for-system-administrators","text":"HCL DX can be deployed in a traditional IBM\u00ae WebSphere Application Server\u00ae cluster, or it can be deployed to a Kubernetes environment. HCL DX provides a range of scriptable interfaces to enable automation of your infrastructure management. Note that some services, such as Digital Asset Management, Design Studio, Experience API, and Content Composer, are available only in the Kubernetes environment.","title":"For System Administrators"},{"location":"platform/getting_the_software95/","text":"Getting the software New and existing users need to register at the HCL Software License Portal and download their entitled HCL Digital Experience package(s). Getting the software Product software can be accessed from the HCL Software Licensing Portal . For additional guidance, refer to the Step-by-step guide on downloading HCL Digital Experience products and watch the video tutorial on How to download HCL Digital Experience Software from the License Server . HCL Digital Experience software is available through several product offerings and associated software licenses. Depending on the product offering that you purchased, your product might include some or all of the following HCL Digital Experience and related programs: Digital Experience Manager, HCL Portal, HCL Portal Express, HCL Web Content Manager, HCL Portal Enable, HCL Portal Extend, HCL Customer Experience Suite, HCL Employee Experience Suite, IBM WebSphere Application Server Network Deployment, IBM DB2 Universal Database Workgroup Server Edition, HCL Connections, and HCL Leap. Additional HCL Digital Experience product offering and license details may be found at the HCL Software Product License site . Installation paths There are different paths that you will start from depending on your current scenario: Fresh full installation (for new customers or for a new system) HCL Digital Experience 9.5 deployment to supported platforms, including Docker or OpenShift/Kubernetes with Docker Updating an existing HCL Digital Experience 8.5 or 9.0 system Users can also test the new HCL Digital Experience API Docker image and run it. Visit the Experience API documentation to learn more. Fresh full installation For a fresh full installation, follow the installation path using the components below which may be found in the HCL Software Licensing Portal (FlexNet) with HCL Digital Experience software packages: IBM\u00ae Installation Manager. IBM WebSphere\u00ae Application Server 9.0.5. HCL Portal 8.5 HCL Digital Experience CF17 HCL Digital Experience 9.5 Corresponding edition files according to your HCL Digital Experience entitlements (HCL Portal Enable, HCL Portal Extend, HCL Portal Server, and HCL Portal Express) After installing the IBM Installation Manager , users need to configure the repositories for IBM WebSphere Application Server 9.0.5, HCL Portal 8.5, HCL Digital Experience CF17, and HCL Digital Experience 9.5, and the corresponding edition files. Update an existing HCL Digital Experience 8.5 or 9.0 system The path from an existing HCL Digital Experience 8.5 or 9.0 system to HCL Digital Experience 9.5 is to download CF17 as well as the 9.5 files. Then install the cumulative fix via the usual CF process. Afterwards, users start the IBM Installation Manager, then configure the 9.5 repository, and add 9.5. Users who are using HCL Portal Enable, HCL Portal Extend, or HCL Web Content Manager need to add both the HCL Portal 9.5 Server and the applicable edition via IBM Installation Manager. Documentation resource: Fixlist of fixes included in CF17 Documentation resource: Combined Cumulative fix strategy for 9.5 Deployment of HCL Digital Experience to Docker and supported Kubernetes platforms The following is the installation path from an existing HCL Digital Experience 8.5 or 9.0 system: Download the HCL Digital Experience 9.5 container image files that contains the following: HCL Digital Experience Docker container; and The Operator or Ambassador Docker container with a set of scripts to deploy the supported Kubernetes platforms. Follow the documentation here to deploy to Docker or supported Kubernetes platforms. Stage the content to the new environment to move from an existing system to HCL Digital Experience on Docker or supported Kubernetes platforms.","title":"Getting the software"},{"location":"platform/getting_the_software95/#getting-the-software","text":"New and existing users need to register at the HCL Software License Portal and download their entitled HCL Digital Experience package(s).","title":"Getting the software"},{"location":"platform/getting_the_software95/#getting-the-software_1","text":"Product software can be accessed from the HCL Software Licensing Portal . For additional guidance, refer to the Step-by-step guide on downloading HCL Digital Experience products and watch the video tutorial on How to download HCL Digital Experience Software from the License Server . HCL Digital Experience software is available through several product offerings and associated software licenses. Depending on the product offering that you purchased, your product might include some or all of the following HCL Digital Experience and related programs: Digital Experience Manager, HCL Portal, HCL Portal Express, HCL Web Content Manager, HCL Portal Enable, HCL Portal Extend, HCL Customer Experience Suite, HCL Employee Experience Suite, IBM WebSphere Application Server Network Deployment, IBM DB2 Universal Database Workgroup Server Edition, HCL Connections, and HCL Leap. Additional HCL Digital Experience product offering and license details may be found at the HCL Software Product License site .","title":"Getting the software"},{"location":"platform/getting_the_software95/#installation-paths","text":"There are different paths that you will start from depending on your current scenario: Fresh full installation (for new customers or for a new system) HCL Digital Experience 9.5 deployment to supported platforms, including Docker or OpenShift/Kubernetes with Docker Updating an existing HCL Digital Experience 8.5 or 9.0 system Users can also test the new HCL Digital Experience API Docker image and run it. Visit the Experience API documentation to learn more.","title":"Installation paths"},{"location":"platform/getting_the_software95/#fresh-full-installation","text":"For a fresh full installation, follow the installation path using the components below which may be found in the HCL Software Licensing Portal (FlexNet) with HCL Digital Experience software packages: IBM\u00ae Installation Manager. IBM WebSphere\u00ae Application Server 9.0.5. HCL Portal 8.5 HCL Digital Experience CF17 HCL Digital Experience 9.5 Corresponding edition files according to your HCL Digital Experience entitlements (HCL Portal Enable, HCL Portal Extend, HCL Portal Server, and HCL Portal Express) After installing the IBM Installation Manager , users need to configure the repositories for IBM WebSphere Application Server 9.0.5, HCL Portal 8.5, HCL Digital Experience CF17, and HCL Digital Experience 9.5, and the corresponding edition files.","title":"Fresh full installation"},{"location":"platform/getting_the_software95/#update-an-existing-hcl-digital-experience-85-or-90-system","text":"The path from an existing HCL Digital Experience 8.5 or 9.0 system to HCL Digital Experience 9.5 is to download CF17 as well as the 9.5 files. Then install the cumulative fix via the usual CF process. Afterwards, users start the IBM Installation Manager, then configure the 9.5 repository, and add 9.5. Users who are using HCL Portal Enable, HCL Portal Extend, or HCL Web Content Manager need to add both the HCL Portal 9.5 Server and the applicable edition via IBM Installation Manager. Documentation resource: Fixlist of fixes included in CF17 Documentation resource: Combined Cumulative fix strategy for 9.5","title":"Update an existing HCL Digital Experience 8.5 or 9.0 system"},{"location":"platform/getting_the_software95/#deployment-of-hcl-digital-experience-to-docker-and-supported-kubernetes-platforms","text":"The following is the installation path from an existing HCL Digital Experience 8.5 or 9.0 system: Download the HCL Digital Experience 9.5 container image files that contains the following: HCL Digital Experience Docker container; and The Operator or Ambassador Docker container with a set of scripts to deploy the supported Kubernetes platforms. Follow the documentation here to deploy to Docker or supported Kubernetes platforms. Stage the content to the new environment to move from an existing system to HCL Digital Experience on Docker or supported Kubernetes platforms.","title":"Deployment of HCL Digital Experience to Docker and supported Kubernetes platforms"},{"location":"platform/intro_platforms/","text":"Supported Platforms HCL DX provides several deployment options for various purposes. Kubernetes Info This is the preferred option for new deployments HCL DX supports deployment to a range of Kubernetes platforms. Choose this option to access the widest range of DX features. Information about architecture, deployment and operations of this topic can be found in this site here Traditional HCL DX supports deployment to a non-containerised environment via WebSphere clustering. This topic is not yet in this site and can be accessed in the Help Centre Docker Docker Compose can be used for developer workstations. Information is available here","title":"Supported Platforms"},{"location":"platform/intro_platforms/#supported-platforms","text":"HCL DX provides several deployment options for various purposes.","title":"Supported Platforms"},{"location":"platform/intro_platforms/#kubernetes","text":"Info This is the preferred option for new deployments HCL DX supports deployment to a range of Kubernetes platforms. Choose this option to access the widest range of DX features. Information about architecture, deployment and operations of this topic can be found in this site here","title":"Kubernetes"},{"location":"platform/intro_platforms/#traditional","text":"HCL DX supports deployment to a non-containerised environment via WebSphere clustering. This topic is not yet in this site and can be accessed in the Help Centre","title":"Traditional"},{"location":"platform/intro_platforms/#docker","text":"Docker Compose can be used for developer workstations. Information is available here","title":"Docker"},{"location":"platform/docker/docker_compose/","text":"Docker image deployment using Docker Compose This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"platform/docker/docker_compose/#docker-image-deployment-using-docker-compose","text":"This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"platform/docker/docker_image_deployment/","text":"Docker image deployment This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx -OR- docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd Note Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence (for using -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx ). To use the HCL DX Configuration Wizard, start the Java virtual machine (JVM) within the running container with the following command: docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. Note For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: How to upload HCL Digital Experience 9.5 CF container images to a private repository Container image list Containerization Limitations/Requirements Customizing the container deployment","title":"Docker image deployment"},{"location":"platform/docker/docker_image_deployment/#docker-image-deployment","text":"This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx -OR- docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd Note Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence (for using -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx ). To use the HCL DX Configuration Wizard, start the Java virtual machine (JVM) within the running container with the following command: docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. Note For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: How to upload HCL Digital Experience 9.5 CF container images to a private repository Container image list Containerization Limitations/Requirements Customizing the container deployment","title":"Docker image deployment"},{"location":"platform/docker/docker_overview/","text":"Docker images for HCL Digital Experience 9.5 HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section. Overview Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process. Installation, Deployment and Migration Guidance Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance","title":"Digital Experience on Docker"},{"location":"platform/docker/docker_overview/#docker-images-for-hcl-digital-experience-95","text":"HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section.","title":"Docker images for HCL Digital Experience 9.5"},{"location":"platform/docker/docker_overview/#overview","text":"Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process.","title":"Overview"},{"location":"platform/docker/docker_overview/#installation-deployment-and-migration-guidance","text":"Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance","title":"Installation, Deployment and Migration Guidance"},{"location":"platform/kubernetes/","text":"Introduction to DX Cloud Native Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment. Containerization overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Container platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. DX on Docker Helm-based deployment Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Customizing the HCL DX URL for hybrid deployment HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment. Upgrade options for containerized deployments HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path. Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption.","title":"Introduction to DX Cloud Native"},{"location":"platform/kubernetes/#introduction-to-dx-cloud-native","text":"Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment. Containerization overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Container platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. DX on Docker Helm-based deployment Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Customizing the HCL DX URL for hybrid deployment HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment. Upgrade options for containerized deployments HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path. Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption.","title":"Introduction to DX Cloud Native"},{"location":"platform/kubernetes/c_kubesupportstatement/","text":"DX Kubernetes Support Matrix View the latest Kubernetes versions and platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Attention Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination. Kubernetes platform support policy: HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated, with the intent of staying up-to-date as possible. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See Table 1 for the list of Kubernetes Platforms that HCL tested with. Table 1: Tested Kubernetes Platforms on Full Container Deployment This table provides information about the sample Kubernetes Platforms that are tested with DX. Kubernetes platforms on full deployments Kubernetes platforms on hybrid deployments - Amazon EKS - Google GKE - Microsoft Azure AKS - Red Hat OpenShift - Amazon EKS / AWS EC2 - Red Hat OpenShift on AWS / AWS EC2 Kubernetes Version Support Policy The list of Kubernetes versions that are tested and supported by HCL are included in Table 2 . From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes version, you may be asked to install a supported level. Table 2: Tested and supported Kubernetes versions on Full Container Deployment This table provides information about the Kubernetes versions that are tested and supported in DX CF releases. Note Review your chosen Kubernetes Platform and ensure that it supports the following Kubernetes Versions: CF Level Kubernetes versions CF203 Kube 1.21 Kube 1.20 Kube 1.19 CF202 Kube 1.21 Kube 1.20 Kube 1.19 CF201 Kube 1.21 Kube 1.20 Kube 1.19","title":"DX Kubernetes Support Matrix"},{"location":"platform/kubernetes/c_kubesupportstatement/#dx-kubernetes-support-matrix","text":"View the latest Kubernetes versions and platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Attention Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination.","title":"DX Kubernetes Support Matrix"},{"location":"platform/kubernetes/c_kubesupportstatement/#kubernetes-platform-support-policy","text":"HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated, with the intent of staying up-to-date as possible. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See Table 1 for the list of Kubernetes Platforms that HCL tested with.","title":"Kubernetes platform support policy:"},{"location":"platform/kubernetes/c_kubesupportstatement/#table-1-tested-kubernetes-platforms-on-full-container-deployment","text":"This table provides information about the sample Kubernetes Platforms that are tested with DX. Kubernetes platforms on full deployments Kubernetes platforms on hybrid deployments - Amazon EKS - Google GKE - Microsoft Azure AKS - Red Hat OpenShift - Amazon EKS / AWS EC2 - Red Hat OpenShift on AWS / AWS EC2","title":"Table 1: Tested Kubernetes Platforms on Full Container Deployment"},{"location":"platform/kubernetes/c_kubesupportstatement/#kubernetes-version-support-policy","text":"The list of Kubernetes versions that are tested and supported by HCL are included in Table 2 . From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes version, you may be asked to install a supported level.","title":"Kubernetes Version Support Policy"},{"location":"platform/kubernetes/c_kubesupportstatement/#table-2-tested-and-supported-kubernetes-versions-on-full-container-deployment","text":"This table provides information about the Kubernetes versions that are tested and supported in DX CF releases. Note Review your chosen Kubernetes Platform and ensure that it supports the following Kubernetes Versions: CF Level Kubernetes versions CF203 Kube 1.21 Kube 1.20 Kube 1.19 CF202 Kube 1.21 Kube 1.20 Kube 1.19 CF201 Kube 1.21 Kube 1.20 Kube 1.19","title":"Table 2: Tested and supported Kubernetes versions on Full Container Deployment"},{"location":"platform/kubernetes/docker/","text":"Container image list This section presents the latest HCL DX 9.5 Container images available. The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry, and the HCL_DX_9.5_Container_Updates download package entry for HCL Digital Experience Cloud Native 9.5 entitlements. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF203 Container Update CF203 If deploying the HCL DX 9.5 CF203 release, the package name and images are as follows: HCL DX 9.5 CF203 DXClient files : hcl-dxclient-image-v95_CF203_20220429-1049.zip hcl-dxclient-v95_CF203_20220429-1409.zip hcl-dx-kubernetes-v95-CF203.zip : HCL DX notices V9.5 CF203.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.16.0_20220429-1042.tar.gz hcl-dx-core-image-v95_CF203_20220429-1033.tar.gz hcl-dx-deployment-v2.6.12_20220503-1239.tgz hcl-dx-design-studio-image-v0.9.0_20220429-1029.tar.gz hcl-dx-digital-asset-manager-image-v1.15.0_20220429-1035.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220429-1030.zip hcl-dx-haproxy-image-v0.1.0_20220429-1036.tar.gz hcl-dx-image-processor-image-v1.16.0_20220429-1034.tar.gz hcl-dx-openldap-image-v1.2.0_20220429-1041.tar.gz hcl-dx-persistence-connection-pool-image-v1.16.0_20220429-1031.tar.gz hcl-dx-persistence-image-v1.16.0_20220429-1032.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.14.0_20220429-1031.tar.gz hcl-dx-persistence-node-image-v1.6_20220429-1030.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF203_20220429-1033.tar.gz hcl-dx-ringapi-image-v1.16.0_20220429-1033.tar.gz hcl-dx-runtime-controller-image-v95_CF203_20220429-1038.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF202 Container Update CF202 If deploying the HCL DX 9.5 CF202 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : hcl-dxclient-image-v95_CF202_20220319-1416.zip hcl-dxclient-v95_CF202_20220319-1409.zip hcl-dx-kubernetes-v95-CF202.zip : HCL DX notices V9.5 CF202.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.15.0_20220319-1357.tar.gz hcl-dx-core-image-v95_CF202_20220319-1358.tar.gz hcl-dx-deployment-v2.5.9_20220319-1358.tgz hcl-dx-design-studio-image-v0.8.0_20220319-1409.tar.gz hcl-dx-digital-asset-manager-image-v1.14.0_20220319-1405.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220319-1409.zip hcl-dx-image-processor-image-v1.15.0_20220319-1358.tar.gz hcl-dx-openldap-image-v1.2.0_20220319-1357.tar.gz hcl-dx-persistence-connection-pool-image-v1.15.0_20220319-1407.tar.gz hcl-dx-persistence-image-v1.15.0_20220319-1402.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.13.0_20220319-1408.tar.gz hcl-dx-persistence-node-image-v1.5_20220319-1408.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF202_20220319-1358.tar.gz hcl-dx-ringapi-image-v1.15.0_20220319-1413.tar.gz hcl-dx-runtime-controller-image-v95_CF202_20220319-1357.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF201 Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : hcl-dxclient-image-v95_CF201_20220207-1614.zip hcl-dxclient-v95_CF201_20220207-1613.zip HCL DX 9.5 CF_201-hcl-dx-kubernetes-v95-CF201.zip : hCL DX notices V9.5 CF201.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz hcl-dx-deployment-v2.4.0_20220207-1606.tgz hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF200 Container Update CF200 If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: HCL DX 9.5 CF200 DXClient files : hcl-dxclient-image-v95_CF200_20211213-1500.zip hcl-dxclient-v95_CF200_20211213-1459.zip HCL DX 9.5 CF_200-hcl-dx-kubernetes-v95-CF200.zip Important With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see List of image files and changes from CF198 and later . hCL DX notices V9.5 CF200.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz hcl-dx-deployment-v2.2.0_20211213-1446.tgz hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : hcl-dxclient-image-v95_CF199_20211029-1357.zip hcl-dxclient-v95_CF199_20211029-1357.zip HCL DX 9.5 CF_199-hcl-dx-kubernetes-v95-CF199.zip hCL DX notices V9.5 CF199.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz hcl-dx-deployment-v2.1.0_20211029-1346.tgz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note The new image files and the change in file names are highlighted in the table. HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : hcl-dxclient-image-v95_CF198_20210917-1455.zip hcl-dxclient-v95_CF198_20210917-1455.zip HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip : hCL DX notices V9.5 CF198.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : hcl-dxclient-image-v95_CF197_20210806-1311.zip hcl-dxclient-v95_CF197_20210806-1311.zip HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip : hCL DX notices V9.5 CF197.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz hcl-dx-deployment-v2.0.0_20210806-1300.tgz hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : hcl-dxclient-image-v95_CF196_20210625-2028.zip hcl-dxclient-v95_CF196_20210625-2029.zip HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip : hCL DX notices V9.5 CF196.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz hcl-dx-deployment-v1.0.0_20210625-2026.tgz hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : dxclient_v1.4.0_20210514-1713.zip HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip : hCL DX notices V9.5 CF195.txt dxclient_v1.4.0_20210514-1713.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz HCL DX 9.5 CF194 CF194 Important Please consult the HCL DX Support Knowledge Base article, Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update , to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip : dxclient_v1.3.0_20210415-2128.zip HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip : hCL DX notices V9.5 CF194.txt dxclient_v1.3.0_20210415-2128.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : dxclient_v1.3.0_20210331-1335.zip HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip : hCL DX notices V9.5 CF193.txt dxclient_v1.3.0_20210331-1335.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz HCL DX 9.5 CF192 CF192 If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF192.zip : hCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz HCL DX 9.5 CF191 CF191 If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF191.zip file: hCL DX notices V9.5 CF191.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz dxclient_v1.1.0_20201211-2153.zip Note HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. HCL DX 9.5 CF19 CF19 If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF19.zip file : hCL DX notices V9.5 CF19.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz dxclient_v1.0.0_20201110-2010.zip HCL DX 9.5 CF184 HCL DX 9.5 Container Update CF184 If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF184.zip files : hCL DX notices V9.5 CF184.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz HCL DX 9.5 CF183 HCL DX 9.5 Container Update CF183 If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: CF183-core.zip files : hCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files : hCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : hCL DX notices V9.5 CF182.txt hcl-dx-ambassador-image-0850.tar.gz hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz hcl-dx-kubernetes-v95-CF182-other.zip : hCL DX notices V9.5 CF182.txt hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-ambassador-image-xxxx.tar.gz hCL DX notices V9.5 CF181.txt hcl-dx-kubernetes-v95-CF181-other.zip : hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz HCL DX 9.5 CF18 CF18 If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: hcl-dx-kubernetes-v95-CF18.zip hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Requirements and limitations Customizing the container deployment","title":"Container image list"},{"location":"platform/kubernetes/docker/#container-image-list","text":"This section presents the latest HCL DX 9.5 Container images available. The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry, and the HCL_DX_9.5_Container_Updates download package entry for HCL Digital Experience Cloud Native 9.5 entitlements. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Container image list"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf203","text":"Container Update CF203 If deploying the HCL DX 9.5 CF203 release, the package name and images are as follows: HCL DX 9.5 CF203 DXClient files : hcl-dxclient-image-v95_CF203_20220429-1049.zip hcl-dxclient-v95_CF203_20220429-1409.zip hcl-dx-kubernetes-v95-CF203.zip : HCL DX notices V9.5 CF203.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.16.0_20220429-1042.tar.gz hcl-dx-core-image-v95_CF203_20220429-1033.tar.gz hcl-dx-deployment-v2.6.12_20220503-1239.tgz hcl-dx-design-studio-image-v0.9.0_20220429-1029.tar.gz hcl-dx-digital-asset-manager-image-v1.15.0_20220429-1035.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220429-1030.zip hcl-dx-haproxy-image-v0.1.0_20220429-1036.tar.gz hcl-dx-image-processor-image-v1.16.0_20220429-1034.tar.gz hcl-dx-openldap-image-v1.2.0_20220429-1041.tar.gz hcl-dx-persistence-connection-pool-image-v1.16.0_20220429-1031.tar.gz hcl-dx-persistence-image-v1.16.0_20220429-1032.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.14.0_20220429-1031.tar.gz hcl-dx-persistence-node-image-v1.6_20220429-1030.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF203_20220429-1033.tar.gz hcl-dx-ringapi-image-v1.16.0_20220429-1033.tar.gz hcl-dx-runtime-controller-image-v95_CF203_20220429-1038.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF203"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf202","text":"Container Update CF202 If deploying the HCL DX 9.5 CF202 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : hcl-dxclient-image-v95_CF202_20220319-1416.zip hcl-dxclient-v95_CF202_20220319-1409.zip hcl-dx-kubernetes-v95-CF202.zip : HCL DX notices V9.5 CF202.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.15.0_20220319-1357.tar.gz hcl-dx-core-image-v95_CF202_20220319-1358.tar.gz hcl-dx-deployment-v2.5.9_20220319-1358.tgz hcl-dx-design-studio-image-v0.8.0_20220319-1409.tar.gz hcl-dx-digital-asset-manager-image-v1.14.0_20220319-1405.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220319-1409.zip hcl-dx-image-processor-image-v1.15.0_20220319-1358.tar.gz hcl-dx-openldap-image-v1.2.0_20220319-1357.tar.gz hcl-dx-persistence-connection-pool-image-v1.15.0_20220319-1407.tar.gz hcl-dx-persistence-image-v1.15.0_20220319-1402.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.13.0_20220319-1408.tar.gz hcl-dx-persistence-node-image-v1.5_20220319-1408.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF202_20220319-1358.tar.gz hcl-dx-ringapi-image-v1.15.0_20220319-1413.tar.gz hcl-dx-runtime-controller-image-v95_CF202_20220319-1357.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF202"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf201","text":"Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : hcl-dxclient-image-v95_CF201_20220207-1614.zip hcl-dxclient-v95_CF201_20220207-1613.zip HCL DX 9.5 CF_201-hcl-dx-kubernetes-v95-CF201.zip : hCL DX notices V9.5 CF201.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz hcl-dx-deployment-v2.4.0_20220207-1606.tgz hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF201"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf200","text":"Container Update CF200 If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: HCL DX 9.5 CF200 DXClient files : hcl-dxclient-image-v95_CF200_20211213-1500.zip hcl-dxclient-v95_CF200_20211213-1459.zip HCL DX 9.5 CF_200-hcl-dx-kubernetes-v95-CF200.zip Important With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see List of image files and changes from CF198 and later . hCL DX notices V9.5 CF200.txt hcl-dx-ambassador-image-1142.tar.gz hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz hcl-dx-deployment-v2.2.0_20211213-1446.tgz hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz hcl-dx-redis-image-5.0.14.tar.gz hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF200"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : hcl-dxclient-image-v95_CF199_20211029-1357.zip hcl-dxclient-v95_CF199_20211029-1357.zip HCL DX 9.5 CF_199-hcl-dx-kubernetes-v95-CF199.zip hCL DX notices V9.5 CF199.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz hcl-dx-deployment-v2.1.0_20211029-1346.tgz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note The new image files and the change in file names are highlighted in the table.","title":"HCL DX 9.5 CF199"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : hcl-dxclient-image-v95_CF198_20210917-1455.zip hcl-dxclient-v95_CF198_20210917-1455.zip HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip : hCL DX notices V9.5 CF198.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : hcl-dxclient-image-v95_CF197_20210806-1311.zip hcl-dxclient-v95_CF197_20210806-1311.zip HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip : hCL DX notices V9.5 CF197.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz hcl-dx-deployment-v2.0.0_20210806-1300.tgz hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : hcl-dxclient-image-v95_CF196_20210625-2028.zip hcl-dxclient-v95_CF196_20210625-2029.zip HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip : hCL DX notices V9.5 CF196.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz hcl-dx-deployment-v1.0.0_20210625-2026.tgz hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : dxclient_v1.4.0_20210514-1713.zip HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip : hCL DX notices V9.5 CF195.txt dxclient_v1.4.0_20210514-1713.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf194","text":"CF194 Important Please consult the HCL DX Support Knowledge Base article, Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update , to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip : dxclient_v1.3.0_20210415-2128.zip HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip : hCL DX notices V9.5 CF194.txt dxclient_v1.3.0_20210415-2128.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF194"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : dxclient_v1.3.0_20210331-1335.zip HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip : hCL DX notices V9.5 CF193.txt dxclient_v1.3.0_20210331-1335.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf192","text":"CF192 If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF192.zip : hCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz","title":"HCL DX 9.5 CF192"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf191","text":"CF191 If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF191.zip file: hCL DX notices V9.5 CF191.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz dxclient_v1.1.0_20201211-2153.zip Note HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release.","title":"HCL DX 9.5 CF191"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf19","text":"CF19 If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF19.zip file : hCL DX notices V9.5 CF19.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz dxclient_v1.0.0_20201110-2010.zip","title":"HCL DX 9.5 CF19"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf184","text":"HCL DX 9.5 Container Update CF184 If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: hcl-dx-kubernetes-v95-CF184.zip files : hCL DX notices V9.5 CF184.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz","title":"HCL DX 9.5 CF184"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf183","text":"HCL DX 9.5 Container Update CF183 If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: CF183-core.zip files : hCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files : hCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF183"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : hCL DX notices V9.5 CF182.txt hcl-dx-ambassador-image-0850.tar.gz hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz hcl-dx-kubernetes-v95-CF182-other.zip : hCL DX notices V9.5 CF182.txt hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-ambassador-image-xxxx.tar.gz hCL DX notices V9.5 CF181.txt hcl-dx-kubernetes-v95-CF181-other.zip : hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz","title":"HCL DX 9.5 CF181"},{"location":"platform/kubernetes/docker/#hcl-dx-95-cf18","text":"CF18 If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: hcl-dx-kubernetes-v95-CF18.zip hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Requirements and limitations Customizing the container deployment","title":"HCL DX 9.5 CF18"},{"location":"platform/kubernetes/limitations_requirements/","text":"Requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Discontinuation of Operator Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels. Requirements for supported file systems This section describes the requirements for supported file systems. DX requires two (2) ReadWriteMany volumes: One volume for Core. One volume for Digital Asset Management. All the other pods require ReadWriteOnce volumes. DX is input-output (I/O) intensive and requires a high-performance file system for optimization. A persistence-node relies on PostgreSQL which requires the use of hard links. Storage systems (like Azure Files) that do not support the use of hard links cannot be used. For more information, see the Microsoft documentation for features not supported by the Azure File service . All DX applications require the use of symbolic links and soft links. Storage systems must support the use of symbolic links and soft links. If you are using Azure Files, you must enable mountOptions of the StorageClass using mfsymlinks . For more information, see the Microsoft documentation on troubleshooting Azure Files on Linux (SMB) . You can configure volume sizes individually per volume and these are dependent of the respective usage. For more information, see the following Help Center topics: Configuring PVCs in a Helm deployment Customizing the container for Operator-based deployments Requirements and Limitations for Helm-based deployments This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments, as well as the number of Pods required of each component. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0 HAProxy haproxy 1 1024MB 1 Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . !!! note HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Related information: Customizing the container for Operator-based deployments Deployment using dxtools","title":"Requirements and limitations"},{"location":"platform/kubernetes/limitations_requirements/#requirements-and-limitations","text":"This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Discontinuation of Operator Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels.","title":"Requirements and limitations"},{"location":"platform/kubernetes/limitations_requirements/#requirements-for-supported-file-systems","text":"This section describes the requirements for supported file systems. DX requires two (2) ReadWriteMany volumes: One volume for Core. One volume for Digital Asset Management. All the other pods require ReadWriteOnce volumes. DX is input-output (I/O) intensive and requires a high-performance file system for optimization. A persistence-node relies on PostgreSQL which requires the use of hard links. Storage systems (like Azure Files) that do not support the use of hard links cannot be used. For more information, see the Microsoft documentation for features not supported by the Azure File service . All DX applications require the use of symbolic links and soft links. Storage systems must support the use of symbolic links and soft links. If you are using Azure Files, you must enable mountOptions of the StorageClass using mfsymlinks . For more information, see the Microsoft documentation on troubleshooting Azure Files on Linux (SMB) . You can configure volume sizes individually per volume and these are dependent of the respective usage. For more information, see the following Help Center topics: Configuring PVCs in a Helm deployment Customizing the container for Operator-based deployments","title":"Requirements for supported file systems"},{"location":"platform/kubernetes/limitations_requirements/#requirements-and-limitations-for-helm-based-deployments","text":"This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments, as well as the number of Pods required of each component. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0 HAProxy haproxy 1 1024MB 1 Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . !!! note HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Related information: Customizing the container for Operator-based deployments Deployment using dxtools","title":"Requirements and Limitations for Helm-based deployments"},{"location":"platform/kubernetes/overview/","text":"Overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Overview HCL introduced full support for containerization in Digital Experience (DX) 9.5. HCL Digital Experience users can deploy HCL Digital Experience images in Docker, Red Hat OpenShift 4.1 and higher, also Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE) for test, development, staging, and production environments. Whether developing, testing, or running a full production environment, use of Docker images and containers are preferred for the ease of deploying applications, including the latest version of HCL Digital Experience. Deploy in a fraction of the time than what it takes in the traditional deployment models. Please see the Containerization Limitations/Requirements and Deployment sections of the documentation before you begin.","title":"Overview"},{"location":"platform/kubernetes/overview/#overview","text":"Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment.","title":"Overview"},{"location":"platform/kubernetes/overview/#overview_1","text":"HCL introduced full support for containerization in Digital Experience (DX) 9.5. HCL Digital Experience users can deploy HCL Digital Experience images in Docker, Red Hat OpenShift 4.1 and higher, also Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE) for test, development, staging, and production environments. Whether developing, testing, or running a full production environment, use of Docker images and containers are preferred for the ease of deploying applications, including the latest version of HCL Digital Experience. Deploy in a fraction of the time than what it takes in the traditional deployment models. Please see the Containerization Limitations/Requirements and Deployment sections of the documentation before you begin.","title":"Overview"},{"location":"platform/kubernetes/architecture/application_architecture/","text":"DX Applications This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file listing topic. Do note that each application defined only lists its next direct dependencies. Sub-dependencies are not explicitly listed. DX 9.5 Core Depends on: No dependencies Type: Stateful HCL DX 9.5 CF196 and later Core contains the primary Portal and Web Content Manager HCL Digital Experience functionality. The standard deployment deploys at least one Pod of Core. If you have an existing on-premise installation of DX 9.5 Core, you can also use that one for your deployment using the Hybrid deployment pattern as described in the DX 9.5 Hybrid deployment topic. Note Application of the hybrid deployment pattern is not yet supported with Helm in HCL DX 9.5 Container Update CF196, and will be added in a later update release. Ring API Depends on: Core (deployed to OpenShift, Kubernetes or Hybrid on-premise) Type: Stateless The Ring API, a component of the HCL DX Experience API , is a REST API wrapping Core functionality. It provides easy-to-use API endpoints and requires that a DX Core 9.5 instance is deployed and started. That instance can either be running inside the Kubernetes or OpenShift deployment or be an existing external on-premise DX-Core installation using the Hybrid pattern. Content Composer Depends on: Ring API Type: Stateless Content Composer requires the Ring API to be deployed to execute Content authoring capabilities. Digital Asset Management Depends on: Ring API, Image Processor, Persistence Type: Stateful Digital Asset Management requires the Ring API be deployed and operational to communicate with the DX Core, and Image Processor components to perform image manipulation, and Persistence to store its application data. Persistence Depends on: No dependencies Type: Stateful Persistence is used by the Digital Asset Management component to store application data. It consists of a read/write primary node and at least one standby read-only node. The switch between the read/write primary and the read-only nodes is automatically performed by the Runtime Controller. Image Processor Depends on: No dependencies Type: Stateless The Image Processor provides image manipulation capabilities that are leveraged by Digital Asset Management. Design Studio (Beta) Depends on: Core, Ring API Type: Stateless Refer to the Design Studio (Beta) topic section for more information about this application. Runtime Controller Depends on: No dependencies Type: Stateless The Runtime Controller incorporates runtime management functionality for the entire HCL DX 9.5 Container hcl-dx-deployment . It enables automated rollout of configuration changes during runtime and acts as a \u201cwatchdog\u201d to monitor for the automated read/write to read-only fallback of Persistence. Interdependency Matrix This matrix shows which HCL DX applications have dependencies on other applications. This also includes sub-dependencies. For example, if an application uses Ring API, it is also dependent on an operational DX 9.5 Core instance. Application names are defined as follows: Shortname Full Name Core HCL DX 9.5 Core Ring API Ring API CC Content Composer DAM Digital Asset Management DS Design Studio PER Persistence IMG Image Processor","title":"DX Applications"},{"location":"platform/kubernetes/architecture/application_architecture/#dx-applications","text":"This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file listing topic. Do note that each application defined only lists its next direct dependencies. Sub-dependencies are not explicitly listed.","title":"DX Applications"},{"location":"platform/kubernetes/architecture/application_architecture/#dx-95-core","text":"Depends on: No dependencies Type: Stateful HCL DX 9.5 CF196 and later Core contains the primary Portal and Web Content Manager HCL Digital Experience functionality. The standard deployment deploys at least one Pod of Core. If you have an existing on-premise installation of DX 9.5 Core, you can also use that one for your deployment using the Hybrid deployment pattern as described in the DX 9.5 Hybrid deployment topic. Note Application of the hybrid deployment pattern is not yet supported with Helm in HCL DX 9.5 Container Update CF196, and will be added in a later update release.","title":"DX 9.5 Core"},{"location":"platform/kubernetes/architecture/application_architecture/#ring-api","text":"Depends on: Core (deployed to OpenShift, Kubernetes or Hybrid on-premise) Type: Stateless The Ring API, a component of the HCL DX Experience API , is a REST API wrapping Core functionality. It provides easy-to-use API endpoints and requires that a DX Core 9.5 instance is deployed and started. That instance can either be running inside the Kubernetes or OpenShift deployment or be an existing external on-premise DX-Core installation using the Hybrid pattern.","title":"Ring API"},{"location":"platform/kubernetes/architecture/application_architecture/#content-composer","text":"Depends on: Ring API Type: Stateless Content Composer requires the Ring API to be deployed to execute Content authoring capabilities.","title":"Content Composer"},{"location":"platform/kubernetes/architecture/application_architecture/#digital-asset-management","text":"Depends on: Ring API, Image Processor, Persistence Type: Stateful Digital Asset Management requires the Ring API be deployed and operational to communicate with the DX Core, and Image Processor components to perform image manipulation, and Persistence to store its application data.","title":"Digital Asset Management"},{"location":"platform/kubernetes/architecture/application_architecture/#persistence","text":"Depends on: No dependencies Type: Stateful Persistence is used by the Digital Asset Management component to store application data. It consists of a read/write primary node and at least one standby read-only node. The switch between the read/write primary and the read-only nodes is automatically performed by the Runtime Controller.","title":"Persistence"},{"location":"platform/kubernetes/architecture/application_architecture/#image-processor","text":"Depends on: No dependencies Type: Stateless The Image Processor provides image manipulation capabilities that are leveraged by Digital Asset Management.","title":"Image Processor"},{"location":"platform/kubernetes/architecture/application_architecture/#design-studio-beta","text":"Depends on: Core, Ring API Type: Stateless Refer to the Design Studio (Beta) topic section for more information about this application.","title":"Design Studio (Beta)"},{"location":"platform/kubernetes/architecture/application_architecture/#runtime-controller","text":"Depends on: No dependencies Type: Stateless The Runtime Controller incorporates runtime management functionality for the entire HCL DX 9.5 Container hcl-dx-deployment . It enables automated rollout of configuration changes during runtime and acts as a \u201cwatchdog\u201d to monitor for the automated read/write to read-only fallback of Persistence.","title":"Runtime Controller"},{"location":"platform/kubernetes/architecture/application_architecture/#interdependency-matrix","text":"This matrix shows which HCL DX applications have dependencies on other applications. This also includes sub-dependencies. For example, if an application uses Ring API, it is also dependent on an operational DX 9.5 Core instance. Application names are defined as follows: Shortname Full Name Core HCL DX 9.5 Core Ring API Ring API CC Content Composer DAM Digital Asset Management DS Design Studio PER Persistence IMG Image Processor","title":"Interdependency Matrix"},{"location":"platform/kubernetes/architecture/configure_deployment/","text":"Configuration This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Configuration overview In the DX 9.5 component definitions, each application has a dedicated ConfigMap. They contain application specific key/value pairs that are derived from the templates in the DX 9.5 Helm Chart and the values inside the values.yaml (and/or your custom value overrides). Rollout of configuration changes Configuration changes are automatically processed by the HCL Digital Experience 9.5 OpenShift or Kubernetes deployment with Helm. DX 9.5 Container Upgrades Via Helm Operations Details Beginning with HCL DX 9.5 Container Upgrade CF196, Helm deployment is supported on the Google Kubernetes Engine platform (GKE). As Helm deployment methods are supported on the additional DX 9.5 supported OpenShift or Kubernetes platform of choice, it is recommended that administrators use Helm deployment to manage upgrade processes. Note that use of dxctl to manage these operations is also supported, but will be deprecated over time in favor of Helm. When supported on the additional DX 9.5 container platforms, the recommended way to change the configuration of a running deployment is via a Helm upgrade. Once the upgrade command is executed, it calculates and apply all changes that derive from the changes that have been made to the values definitions. The DX Helm deployment uses annotations on each application to share the checksum of the last ConfigMap. This checksum is updated as soon as Helm upgrade is performed and that there has been a change to the configuration of an application. When the checksum is updated, OpenShift or Kubernetes proceeds to roll out the new configuration. Based on the count of Pods per application that you are running, this may cause a downtime in operations. To minimize the impact of operations to configuration changes processing, consider running at least a quorum of 3 Pods per application. This allows OpenShift and Kubernetes to properly roll out the new configurations and maintain availability of the applications operations throughout the entire upgrade processing cycle. Direct Configuration changes in Kubernetes or OpenShift For development and testing, sometimes it is useful to directly adjust configuration in ConfigMaps via Kubernetes (for example, kubectl edit cm ) or OpenShift commands. In that case, the runtime controller is to notice that there has been a change to a ConfigMap and calculates the checksum for the Pod specification of the affected application. Kubernetes or OpenShift thens proceed to roll out the new configuration. This is the same processing methodology applies as when using Helm upgrade definitions. Note It is recommended that administrators use Helm upgrade for configuration changes, as the DX 9.5 Helm chart contains logic to calculate certain values. If you choose to edit ConfigMaps directly in the Kubernetes or OpenShift console, do note that running a Helm upgrade overwrites the settings you inserted directly/manually in your deployment. It is recommended to apply configuration changes directly on an as-needed basis and only for development and testing purposes. Please also ensure that the keys used inside the ConfigMap are ordered alphabetically . List of HCL DX 9.5 ConfigMaps Name Application <RELEASE-NAME>-content-composer Content Composer <RELEASE-NAME>-core Core <RELEASE-NAME>-design-studio Design Studio (beta) <RELEASE-NAME>-digital-asset-management Digital Asset Management <RELEASE-NAME>-image-processor Image Processor <RELEASE-NAME>-persistence Persistence <RELEASE-NAME>-ring-api Ring API <RELEASE-NAME>-runtime-controller Runtime Controller","title":"Configuration"},{"location":"platform/kubernetes/architecture/configure_deployment/#configuration","text":"This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm.","title":"Configuration"},{"location":"platform/kubernetes/architecture/configure_deployment/#configuration-overview","text":"In the DX 9.5 component definitions, each application has a dedicated ConfigMap. They contain application specific key/value pairs that are derived from the templates in the DX 9.5 Helm Chart and the values inside the values.yaml (and/or your custom value overrides).","title":"Configuration overview"},{"location":"platform/kubernetes/architecture/configure_deployment/#rollout-of-configuration-changes","text":"Configuration changes are automatically processed by the HCL Digital Experience 9.5 OpenShift or Kubernetes deployment with Helm. DX 9.5 Container Upgrades Via Helm Operations Details Beginning with HCL DX 9.5 Container Upgrade CF196, Helm deployment is supported on the Google Kubernetes Engine platform (GKE). As Helm deployment methods are supported on the additional DX 9.5 supported OpenShift or Kubernetes platform of choice, it is recommended that administrators use Helm deployment to manage upgrade processes. Note that use of dxctl to manage these operations is also supported, but will be deprecated over time in favor of Helm. When supported on the additional DX 9.5 container platforms, the recommended way to change the configuration of a running deployment is via a Helm upgrade. Once the upgrade command is executed, it calculates and apply all changes that derive from the changes that have been made to the values definitions. The DX Helm deployment uses annotations on each application to share the checksum of the last ConfigMap. This checksum is updated as soon as Helm upgrade is performed and that there has been a change to the configuration of an application. When the checksum is updated, OpenShift or Kubernetes proceeds to roll out the new configuration. Based on the count of Pods per application that you are running, this may cause a downtime in operations. To minimize the impact of operations to configuration changes processing, consider running at least a quorum of 3 Pods per application. This allows OpenShift and Kubernetes to properly roll out the new configurations and maintain availability of the applications operations throughout the entire upgrade processing cycle. Direct Configuration changes in Kubernetes or OpenShift For development and testing, sometimes it is useful to directly adjust configuration in ConfigMaps via Kubernetes (for example, kubectl edit cm ) or OpenShift commands. In that case, the runtime controller is to notice that there has been a change to a ConfigMap and calculates the checksum for the Pod specification of the affected application. Kubernetes or OpenShift thens proceed to roll out the new configuration. This is the same processing methodology applies as when using Helm upgrade definitions. Note It is recommended that administrators use Helm upgrade for configuration changes, as the DX 9.5 Helm chart contains logic to calculate certain values. If you choose to edit ConfigMaps directly in the Kubernetes or OpenShift console, do note that running a Helm upgrade overwrites the settings you inserted directly/manually in your deployment. It is recommended to apply configuration changes directly on an as-needed basis and only for development and testing purposes. Please also ensure that the keys used inside the ConfigMap are ordered alphabetically .","title":"Rollout of configuration changes"},{"location":"platform/kubernetes/architecture/configure_deployment/#list-of-hcl-dx-95-configmaps","text":"Name Application <RELEASE-NAME>-content-composer Content Composer <RELEASE-NAME>-core Core <RELEASE-NAME>-design-studio Design Studio (beta) <RELEASE-NAME>-digital-asset-management Digital Asset Management <RELEASE-NAME>-image-processor Image Processor <RELEASE-NAME>-persistence Persistence <RELEASE-NAME>-ring-api Ring API <RELEASE-NAME>-runtime-controller Runtime Controller","title":"List of HCL DX 9.5 ConfigMaps"},{"location":"platform/kubernetes/architecture/container_scaling/","text":"Scaling This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Use of HorizontalPodAutoscalers in DX 9.5 Deployments using Helm The following DX 9.5 applications can be configured to leverage HorizontalPodAutoscalers for Kubernetes and OpenShift based automated scaling: Core Content Composer Design Studio (beta) Digital Asset Management Image Processor Ring API HorizontalPodAutoscalers monitor Pod resources such as CPU and Memory usage, and automatically scales up/down applications based on specific thresholds defined and scaling limits. For the above mentioned DX applications, the maximum and minimum count of Replicas can be configured via the values.yaml. The thresholds for CPU and Memory usage are also configurable allowing for load-based automated scaling of these applications. Per default, the automated scaling is not active and needs to be enabled before taking effect. Known limitations: Core The HCL Digital Experience 9.5 Core can only be scaled to more than one Pod if you have performed a database transfer from the default packaged Derby database. Prior to that, any other additional Pod except for Pod-0 fails to start, since the default packaged Derby database does not allow for multiple Pods connecting to it. Persistence The Persistence used for Digital Asset Management currently supports scaling only with the updated persistence feature. For more information, see Digital Asset Management persistence architecture .","title":"Scaling"},{"location":"platform/kubernetes/architecture/container_scaling/#scaling","text":"This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services.","title":"Scaling"},{"location":"platform/kubernetes/architecture/container_scaling/#use-of-horizontalpodautoscalers-in-dx-95-deployments-using-helm","text":"The following DX 9.5 applications can be configured to leverage HorizontalPodAutoscalers for Kubernetes and OpenShift based automated scaling: Core Content Composer Design Studio (beta) Digital Asset Management Image Processor Ring API HorizontalPodAutoscalers monitor Pod resources such as CPU and Memory usage, and automatically scales up/down applications based on specific thresholds defined and scaling limits. For the above mentioned DX applications, the maximum and minimum count of Replicas can be configured via the values.yaml. The thresholds for CPU and Memory usage are also configurable allowing for load-based automated scaling of these applications. Per default, the automated scaling is not active and needs to be enabled before taking effect. Known limitations: Core The HCL Digital Experience 9.5 Core can only be scaled to more than one Pod if you have performed a database transfer from the default packaged Derby database. Prior to that, any other additional Pod except for Pod-0 fails to start, since the default packaged Derby database does not allow for multiple Pods connecting to it. Persistence The Persistence used for Digital Asset Management currently supports scaling only with the updated persistence feature. For more information, see Digital Asset Management persistence architecture .","title":"Use of HorizontalPodAutoscalers in DX 9.5 Deployments using Helm"},{"location":"platform/kubernetes/architecture/core_interactions_kubernetes/","text":"Core Behavior This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Volume mount points The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog. Additional Information about profile directories The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile. Core container Version-to-Version upgrade When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories).","title":"Core Behavior"},{"location":"platform/kubernetes/architecture/core_interactions_kubernetes/#core-behavior","text":"This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm.","title":"Core Behavior"},{"location":"platform/kubernetes/architecture/core_interactions_kubernetes/#volume-mount-points","text":"The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog.","title":"Volume mount points"},{"location":"platform/kubernetes/architecture/core_interactions_kubernetes/#additional-information-about-profile-directories","text":"The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile.","title":"Additional Information about profile directories"},{"location":"platform/kubernetes/architecture/core_interactions_kubernetes/#core-container-version-to-version-upgrade","text":"When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories).","title":"Core container Version-to-Version upgrade"},{"location":"platform/kubernetes/architecture/dam_persistence_architecture/","text":"Digital Asset Management persistence This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. persistence-node persistence-node provides the database functionality for HCL Digital Asset Management . The persistence-node is a DX Red Hat Universal Base Image (UBI) container image installed with PostgreSQL and the Replication Manager Service. [repmgr](https://repmgr.org/) is an open-source tool suite for managing replication and failover in a cluster of PostgreSQL servers. repmgr enhances the built-in hot-standby capabilities of PostgreSQL with tools to set up standby servers, monitor replication, and perform administrative tasks, such as failover or manual switchover operations. In case of PostgreSQL master server failure, the repmgr service switches the server role from master to standby. The persistence-node configurations are available in the Helm Chart values.yaml file as persistenceNode . The administrator can configure number of persistence-node under scaling configuration. # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 Note Scaling affects only the read requests and ensures fail-over capabilities. Write requests are always directed only to the primary pod. The persistence-node is a stateful application and it requires a volume. The configuration must have a dynamic volume class to start the container. The storageClassName and storage must be updated according to the cloud service provider and project requirement. # Persistent Volumes for Persistence Node persistenceNode: # Database PVC, one per Persistence Node database: storageClassName: \"manual\" requests: storage: \"2Gi\" # Optional volume name to specifically map to. volumeName: persistence-connection-pool The persistence-connection-pool container runs the Pg-pool service. Pg-pool is a middleware that works between persistence-node and HCL Digital Asset Management . The service provides: Connection pooling Load balancing For better performance, administrator can scale the persistence-connection-pool to more than one pod. The persistence-connection-pool configurations are available in Helm Chart values.yaml file as persistenceConnectionPool . # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 The following is an example of a persistence cluster in a successful deployment.","title":"Digital Asset Management persistence"},{"location":"platform/kubernetes/architecture/dam_persistence_architecture/#digital-asset-management-persistence","text":"This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later.","title":"Digital Asset Management persistence"},{"location":"platform/kubernetes/architecture/dam_persistence_architecture/#persistence-node","text":"persistence-node provides the database functionality for HCL Digital Asset Management . The persistence-node is a DX Red Hat Universal Base Image (UBI) container image installed with PostgreSQL and the Replication Manager Service. [repmgr](https://repmgr.org/) is an open-source tool suite for managing replication and failover in a cluster of PostgreSQL servers. repmgr enhances the built-in hot-standby capabilities of PostgreSQL with tools to set up standby servers, monitor replication, and perform administrative tasks, such as failover or manual switchover operations. In case of PostgreSQL master server failure, the repmgr service switches the server role from master to standby. The persistence-node configurations are available in the Helm Chart values.yaml file as persistenceNode . The administrator can configure number of persistence-node under scaling configuration. # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 Note Scaling affects only the read requests and ensures fail-over capabilities. Write requests are always directed only to the primary pod. The persistence-node is a stateful application and it requires a volume. The configuration must have a dynamic volume class to start the container. The storageClassName and storage must be updated according to the cloud service provider and project requirement. # Persistent Volumes for Persistence Node persistenceNode: # Database PVC, one per Persistence Node database: storageClassName: \"manual\" requests: storage: \"2Gi\" # Optional volume name to specifically map to. volumeName:","title":"persistence-node"},{"location":"platform/kubernetes/architecture/dam_persistence_architecture/#persistence-connection-pool","text":"The persistence-connection-pool container runs the Pg-pool service. Pg-pool is a middleware that works between persistence-node and HCL Digital Asset Management . The service provides: Connection pooling Load balancing For better performance, administrator can scale the persistence-connection-pool to more than one pod. The persistence-connection-pool configurations are available in Helm Chart values.yaml file as persistenceConnectionPool . # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 The following is an example of a persistence cluster in a successful deployment.","title":"persistence-connection-pool"},{"location":"platform/kubernetes/architecture/helm_overview/","text":"Overview This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process. Overview Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms. Helm Chart contents The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates for e.g. Kubernetes resources value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm. Helm deployment flow As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace. Deployment structure Basics per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used. DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Deploying DX 9.5 applications to container platforms using Helm This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. PersistentVolumes and related operations considerations This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Configuring DX 9.5 deployments to container platforms using Helm This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Scaling DX 9.5 container deployments using Helm This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Digital Asset Management persistence architecture This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. Related information: HCL DX dxctl process","title":"Overview"},{"location":"platform/kubernetes/architecture/helm_overview/#overview","text":"This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process.","title":"Overview"},{"location":"platform/kubernetes/architecture/helm_overview/#overview_1","text":"Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms.","title":"Overview"},{"location":"platform/kubernetes/architecture/helm_overview/#helm-chart-contents","text":"The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates for e.g. Kubernetes resources value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm.","title":"Helm Chart contents"},{"location":"platform/kubernetes/architecture/helm_overview/#helm-deployment-flow","text":"As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace.","title":"Helm deployment flow"},{"location":"platform/kubernetes/architecture/helm_overview/#deployment-structure","text":"Basics per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used. DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Deploying DX 9.5 applications to container platforms using Helm This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. PersistentVolumes and related operations considerations This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Configuring DX 9.5 deployments to container platforms using Helm This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Scaling DX 9.5 container deployments using Helm This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Digital Asset Management persistence architecture This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. Related information: HCL DX dxctl process","title":"Deployment structure"},{"location":"platform/kubernetes/architecture/persistent_volumes/","text":"Persistent Volumes This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Digital Experience 9.5 container-based stateful applications (DX 9.5 Core, Digital Asset Management, and Persistence) require PersistentVolumes (PVs) to store their data. Refer to the Deploy DX 9.5 applications to container platforms using Helm topic for a description of the DX 9.5 Applications details. As Kubernetes and OpenShift Pods do not have their own persistent file storage, the use of PVs is a must. For more information on PVs, consult the Kubernetes documentation . Note You need to consider the type of PVs before you perform an installation. The type of volumes used depends on the type of deployment you are performing. If you are using only ReadWriteOnce (RWO) persistent volumes for all applications, you will not be able to scale them up to multiple Pods. Core The DX 9.5 Core application requires multiple PersistentVolumes (PVs) of different types and sizes for its operation. From an application perspective, there are three different things that need to be persisted: Profiles (called wp_profile and cw_profile ) Logs Transaction logs While the profile needs to be shared between all DX 9.5 Core Pods, the logs and transaction logs are per Pod and not shared. This means that the persistent volume used for the profile must be ReadWriteMany (RWX). The persistent volumes used for logs and transaction logs are established as ReadWriteOnce (RWO) for proper operations performance. For example, in a DX 9.5 Core deployment: To deploy one Core Pod, 3 PVs will be needed. To deploy two Core Pods, the number of required PVs increases by two, resulting in 5 PVs needed, since the second Pod shares the existing profile PV with the first Pod, but requires its own log and transaction log PVs. The following formula example can be used to calculate the required PV count per Core Pods to be deployed: # Formula to calculate PV count n(PV) = 1 + m(Core Pods) * 2 # E.g. for 3 Pods: 1 + 3 * 2 = 7 PVs In typical operations, the persistent volumes for logs and transaction logs are relatively small. Digital Asset Management The Digital Asset Management (DAM) application requires one (1) PV for storing binary asset data. This persistent volume is shared between all Digital Asset Management Pods. The PV used must be ReadWriteMany (RWX) . Persistence Persistence consists of at least two Pods. One which acts as a read/write primary node, and at least one that acts as a read-only fallback. All Persistence Pods work with ReadWriteOnce (RWO) persistent volumes, since there is no sharing of storage between the Pods. Therefore, the minimum required amount of PVs for Persistence is 2. Remote Search Remote Search requires 1 persistent volume for storing the profile (called prs_profile ) with the type ReadWriteOnce (RWO). Remote Search is limited to only one Pod, therefore, requires one PV for that Pod. Refer to Networking configuration for next steps.","title":"Persistent Volumes"},{"location":"platform/kubernetes/architecture/persistent_volumes/#persistent-volumes","text":"This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Digital Experience 9.5 container-based stateful applications (DX 9.5 Core, Digital Asset Management, and Persistence) require PersistentVolumes (PVs) to store their data. Refer to the Deploy DX 9.5 applications to container platforms using Helm topic for a description of the DX 9.5 Applications details. As Kubernetes and OpenShift Pods do not have their own persistent file storage, the use of PVs is a must. For more information on PVs, consult the Kubernetes documentation . Note You need to consider the type of PVs before you perform an installation. The type of volumes used depends on the type of deployment you are performing. If you are using only ReadWriteOnce (RWO) persistent volumes for all applications, you will not be able to scale them up to multiple Pods.","title":"Persistent Volumes"},{"location":"platform/kubernetes/architecture/persistent_volumes/#core","text":"The DX 9.5 Core application requires multiple PersistentVolumes (PVs) of different types and sizes for its operation. From an application perspective, there are three different things that need to be persisted: Profiles (called wp_profile and cw_profile ) Logs Transaction logs While the profile needs to be shared between all DX 9.5 Core Pods, the logs and transaction logs are per Pod and not shared. This means that the persistent volume used for the profile must be ReadWriteMany (RWX). The persistent volumes used for logs and transaction logs are established as ReadWriteOnce (RWO) for proper operations performance. For example, in a DX 9.5 Core deployment: To deploy one Core Pod, 3 PVs will be needed. To deploy two Core Pods, the number of required PVs increases by two, resulting in 5 PVs needed, since the second Pod shares the existing profile PV with the first Pod, but requires its own log and transaction log PVs. The following formula example can be used to calculate the required PV count per Core Pods to be deployed: # Formula to calculate PV count n(PV) = 1 + m(Core Pods) * 2 # E.g. for 3 Pods: 1 + 3 * 2 = 7 PVs In typical operations, the persistent volumes for logs and transaction logs are relatively small.","title":"Core"},{"location":"platform/kubernetes/architecture/persistent_volumes/#digital-asset-management","text":"The Digital Asset Management (DAM) application requires one (1) PV for storing binary asset data. This persistent volume is shared between all Digital Asset Management Pods. The PV used must be ReadWriteMany (RWX) .","title":"Digital Asset Management"},{"location":"platform/kubernetes/architecture/persistent_volumes/#persistence","text":"Persistence consists of at least two Pods. One which acts as a read/write primary node, and at least one that acts as a read-only fallback. All Persistence Pods work with ReadWriteOnce (RWO) persistent volumes, since there is no sharing of storage between the Pods. Therefore, the minimum required amount of PVs for Persistence is 2.","title":"Persistence"},{"location":"platform/kubernetes/architecture/persistent_volumes/#remote-search","text":"Remote Search requires 1 persistent volume for storing the profile (called prs_profile ) with the type ReadWriteOnce (RWO). Remote Search is limited to only one Pod, therefore, requires one PV for that Pod. Refer to Networking configuration for next steps.","title":"Remote Search"},{"location":"platform/kubernetes/deployment/helm_deployment/","text":"Deploying container platforms using Helm Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm on the following as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Note For more information, you can also refer to deploy HCL DX 9.5 CF197 on Azure AKS using HELM Chart . Note HCL DX V9.5 V200 and later is designed to run on any Certified Kubernetes platform with some conditions, documented in Container platform support matrix Refer video: Deploy HCL DX 9.5 Container using Helm . Update HCL DX 9.5 Container to a later version using Helm About this task This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Important Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic. Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version.","title":"Deploying container platforms using Helm"},{"location":"platform/kubernetes/deployment/helm_deployment/#deploying-container-platforms-using-helm","text":"Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm on the following as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Note For more information, you can also refer to deploy HCL DX 9.5 CF197 on Azure AKS using HELM Chart . Note HCL DX V9.5 V200 and later is designed to run on any Certified Kubernetes platform with some conditions, documented in Container platform support matrix Refer video: Deploy HCL DX 9.5 Container using Helm . Update HCL DX 9.5 Container to a later version using Helm","title":"Deploying container platforms using Helm"},{"location":"platform/kubernetes/deployment/helm_deployment/#about-this-task","text":"This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Important Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic. Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version.","title":"About this task"},{"location":"platform/kubernetes/deployment/helm_install_commands/","text":"Install The following are install commands that are used to deploy or uninstall HCL Digital Experience 9.5 Helm Charts. Install command Important Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. To run the installation of your prepared configurations using Helm, use the following command: # Helm install command helm install -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. The -f path/to/your/custom-values.yaml must point to the custom-values.yaml you have created, which contains all deployment configuration. your-release-name is the Helm release name and prefixes all resources created in that installation, such as Pods, Services, and others. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Helm Chart that you have extracted as described earlier in the planning and preparation steps. After a successful deployment, Helm responds with the following message: NAME: dx LAST DEPLOYED: Thu Jun 17 14:27:58 2021 NAMESPACE: my-namespace STATUS: deployed REVISION: 1 TEST SUITE: None Hybrid deployment configuration If you are installing in a Hybrid deployment pattern, after successfully running the Helm deployment, you need to configure services in DX Core to be aware of the external host name of your hybrid DX environment. To complete these steps, log on to the DX Core Server and run the following Config Engine tasks to enable other DX applications. In the following examples, replace <host-url> with your corresponding external host name. Content Composer: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-headless-content -Dstatic.ui.url=https://<host-url>/dx/ui/content-ui/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Digital Asset Manager: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-media-library -Dstatic.ui.url=https://<host-url>/dx/ui/dam/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Design Studio (Beta): <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-content-sites -Dcontentsites.static.ui.url=https://<host-url>/dx/ui/site-manager/static -DWasPassword=<was-password> -DPortalAdminPwd=<admin-password> Default URLs post installation During the configuration process, you might need the following URLs to access different administration user interfaces. Use the following default URLs to access HCL Digital Experience (Portal and WCM), the WebSphere\u00ae Integrated Solutions Console, and the Configuration Wizard: HCL Digital Experience (Portal and WCM) https://yourserver/wps/portal IBM WebSphere\u00ae Integrated Solutions Console https://yourserver/ibm/console IBM WebSphere\u00ae Integrated Solutions Console for Remote Search https://yourserver:9043/ibm/console HCL Digital Experience Configuration Wizard https://yourserver/hcl/wizard (Optional) External DNS configuration After a successful Helm deployment in a hybrid deployment, you can add a load balancer or an external IP to the DNS registry. Run the following command to get a load balancer external IP: kubectl get services -n <your-namespace> For Amazon EKS, you must add the external IP to route53. (Optional) Automated host extraction As described in the Configure networking topic, there are instances wherein you do not know the resulting external IP or FQDN for your deployment and you left the host value empty. In that case, run a Helm upgrade command, and it automatically polls the Ambassador Ingress and extract the found IP or FQDN from there. The Helm Chart logic goes ahead and populates all application configuration with the correct settings. An example is provided below. You may use the following Helm upgrade command to trigger the automated host extraction: # Helm upgrade command helm upgrade -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz Additional reference Step by Step Guide: How to deploy HCL DX Container Update CF197 and higher to Microsoft Azure Kubernetes Service (AKS) using Helm","title":"Install"},{"location":"platform/kubernetes/deployment/helm_install_commands/#install","text":"The following are install commands that are used to deploy or uninstall HCL Digital Experience 9.5 Helm Charts.","title":"Install"},{"location":"platform/kubernetes/deployment/helm_install_commands/#install-command","text":"Important Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. To run the installation of your prepared configurations using Helm, use the following command: # Helm install command helm install -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. The -f path/to/your/custom-values.yaml must point to the custom-values.yaml you have created, which contains all deployment configuration. your-release-name is the Helm release name and prefixes all resources created in that installation, such as Pods, Services, and others. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Helm Chart that you have extracted as described earlier in the planning and preparation steps. After a successful deployment, Helm responds with the following message: NAME: dx LAST DEPLOYED: Thu Jun 17 14:27:58 2021 NAMESPACE: my-namespace STATUS: deployed REVISION: 1 TEST SUITE: None","title":"Install command"},{"location":"platform/kubernetes/deployment/helm_install_commands/#hybrid-deployment-configuration","text":"If you are installing in a Hybrid deployment pattern, after successfully running the Helm deployment, you need to configure services in DX Core to be aware of the external host name of your hybrid DX environment. To complete these steps, log on to the DX Core Server and run the following Config Engine tasks to enable other DX applications. In the following examples, replace <host-url> with your corresponding external host name. Content Composer: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-headless-content -Dstatic.ui.url=https://<host-url>/dx/ui/content-ui/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Digital Asset Manager: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-media-library -Dstatic.ui.url=https://<host-url>/dx/ui/dam/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Design Studio (Beta): <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-content-sites -Dcontentsites.static.ui.url=https://<host-url>/dx/ui/site-manager/static -DWasPassword=<was-password> -DPortalAdminPwd=<admin-password>","title":"Hybrid deployment configuration"},{"location":"platform/kubernetes/deployment/helm_install_commands/#default-urls-post-installation","text":"During the configuration process, you might need the following URLs to access different administration user interfaces. Use the following default URLs to access HCL Digital Experience (Portal and WCM), the WebSphere\u00ae Integrated Solutions Console, and the Configuration Wizard: HCL Digital Experience (Portal and WCM) https://yourserver/wps/portal IBM WebSphere\u00ae Integrated Solutions Console https://yourserver/ibm/console IBM WebSphere\u00ae Integrated Solutions Console for Remote Search https://yourserver:9043/ibm/console HCL Digital Experience Configuration Wizard https://yourserver/hcl/wizard","title":"Default URLs post installation"},{"location":"platform/kubernetes/deployment/helm_install_commands/#optional-external-dns-configuration","text":"After a successful Helm deployment in a hybrid deployment, you can add a load balancer or an external IP to the DNS registry. Run the following command to get a load balancer external IP: kubectl get services -n <your-namespace> For Amazon EKS, you must add the external IP to route53.","title":"(Optional) External DNS configuration"},{"location":"platform/kubernetes/deployment/helm_install_commands/#optional-automated-host-extraction","text":"As described in the Configure networking topic, there are instances wherein you do not know the resulting external IP or FQDN for your deployment and you left the host value empty. In that case, run a Helm upgrade command, and it automatically polls the Ambassador Ingress and extract the found IP or FQDN from there. The Helm Chart logic goes ahead and populates all application configuration with the correct settings. An example is provided below. You may use the following Helm upgrade command to trigger the automated host extraction: # Helm upgrade command helm upgrade -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"(Optional) Automated host extraction"},{"location":"platform/kubernetes/deployment/helm_install_commands/#additional-reference","text":"Step by Step Guide: How to deploy HCL DX Container Update CF197 and higher to Microsoft Azure Kubernetes Service (AKS) using Helm","title":"Additional reference"},{"location":"platform/kubernetes/deployment/helm_uninstall/","text":"Uninstall To remove your HCL Digital Experience 9.5 deployment from your Cluster deployed using Helm, it is recommended that you use Helm uninstall. Uninstall command To run the uninstall, use the following command as shown in this example: # Helm uninstall command helm uninstall your-release-name -n my-namespace where my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to and your-release-name is the Helm release name you selected during installation. After a successful deployment, Helm responds with the following message: release \"your-release-name\" uninstalled","title":"Uninstall"},{"location":"platform/kubernetes/deployment/helm_uninstall/#uninstall","text":"To remove your HCL Digital Experience 9.5 deployment from your Cluster deployed using Helm, it is recommended that you use Helm uninstall.","title":"Uninstall"},{"location":"platform/kubernetes/deployment/helm_uninstall/#uninstall-command","text":"To run the uninstall, use the following command as shown in this example: # Helm uninstall command helm uninstall your-release-name -n my-namespace where my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to and your-release-name is the Helm release name you selected during installation. After a successful deployment, Helm responds with the following message: release \"your-release-name\" uninstalled","title":"Uninstall command"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/","text":"Configure applications Core Supported LDAP configuration You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option Authoring/Rendering configuration You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true Configuration Wizard configuration You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true OpenLDAP configuration If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\" Remote Search configuration You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true","title":"Configure applications"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#configure-applications","text":"","title":"Configure applications"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#core","text":"","title":"Core"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#supported-ldap-configuration","text":"You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option","title":"Supported LDAP configuration"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#authoringrendering-configuration","text":"You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true","title":"Authoring/Rendering configuration"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#configuration-wizard-configuration","text":"You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true","title":"Configuration Wizard configuration"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#openldap-configuration","text":"If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\"","title":"OpenLDAP configuration"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_apps/#remote-search-configuration","text":"You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true","title":"Remote Search configuration"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_credentials/","text":"Configure credentials HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment. Adjusting default credentials You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\"","title":"Configure credentials"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_credentials/#configure-credentials","text":"HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment.","title":"Configure credentials"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_credentials/#adjusting-default-credentials","text":"You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\"","title":"Adjusting default credentials"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_scaling/","text":"Configure scaling The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note You are not able to use more than one (1) Core Pod until you have performed a database transfer. Configuring pod count Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3 Configuring HorizontalPodAutoscalers The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications. mands](helm_install_commands.md)** for the next steps.","title":"Configure scaling"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_scaling/#configure-scaling","text":"The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note You are not able to use more than one (1) Core Pod until you have performed a database transfer.","title":"Configure scaling"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_scaling/#configuring-pod-count","text":"Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3","title":"Configuring pod count"},{"location":"platform/kubernetes/deployment/preparation/optional_configure_scaling/#configuring-horizontalpodautoscalers","text":"The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications. mands](helm_install_commands.md)** for the next steps.","title":"Configuring HorizontalPodAutoscalers"},{"location":"platform/kubernetes/deployment/preparation/optional_core_sidecar_log/","text":"Configure Core sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\"","title":"Configure Core sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/optional_core_sidecar_log/#configure-core-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log.","title":"Configure Core sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/optional_core_sidecar_log/#configure-custom-sidecar-containers","text":"Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\"","title":"Configure custom sidecar containers"},{"location":"platform/kubernetes/deployment/preparation/optional_disable_apps/","text":"Select DX applications to deploy HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed. Disabling or enabling specific applications You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one.","title":"Select DX applications to deploy"},{"location":"platform/kubernetes/deployment/preparation/optional_disable_apps/#select-dx-applications-to-deploy","text":"HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed.","title":"Select DX applications to deploy"},{"location":"platform/kubernetes/deployment/preparation/optional_disable_apps/#disabling-or-enabling-specific-applications","text":"You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one.","title":"Disabling or enabling specific applications"},{"location":"platform/kubernetes/deployment/preparation/optional_experimental_features/","text":"Experimental features Incubator section in the values.yaml file The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Important No features within the incubator section are recommended to be used in the production environments.","title":"Experimental features"},{"location":"platform/kubernetes/deployment/preparation/optional_experimental_features/#experimental-features","text":"","title":"Experimental features"},{"location":"platform/kubernetes/deployment/preparation/optional_experimental_features/#incubator-section-in-the-valuesyaml-file","text":"The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Important No features within the incubator section are recommended to be used in the production environments.","title":"Incubator section in the values.yaml file"},{"location":"platform/kubernetes/deployment/preparation/optional_imagepullsecrets/","text":"Using ImagePullSecrets To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to. Configure deployment to use ImagePullSecrets In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images.","title":"Using ImagePullSecrets"},{"location":"platform/kubernetes/deployment/preparation/optional_imagepullsecrets/#using-imagepullsecrets","text":"To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to.","title":"Using ImagePullSecrets"},{"location":"platform/kubernetes/deployment/preparation/optional_imagepullsecrets/#configure-deployment-to-use-imagepullsecrets","text":"In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images.","title":"Configure deployment to use ImagePullSecrets"},{"location":"platform/kubernetes/deployment/preparation/optional_internal_networking/","text":"Configure to deploy on the internal network This section contains the procedure to deploy DX on the internal network. How to deploy DX on the internal network To deploy DX on the internal network (with no public access), we need to add the platform-specific annotations for the ambassador service. Update your custom values.yaml file with the annotation specific to your cloud provider. Refer to the list of annotations . annotations: service: # Annotations for ambassador service. ambassador: [] Example for GKE: annotations: service: # Annotations for ambassador service. ambassador: - key: cloud.google.com/load-balancer-type value : \"Internal\" How to update the existing deployment Follow the steps to update an existing deployment from an external network to an internal network or vice-versa: Note Update to the network type results change in IP address and requires updates to your DNS services. Disable ambassador in your custom values.yaml file and then do helm update. Example: # Controls which application is deployed and configured applications: -- -- # Deploys the Ambassador Ingress and Redis ambassador: false After updating your custom values.yaml file, run helm update command. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n dxns . -f ./cloud-deploy-values.yaml After the update is completed, enable the ambassador and add annotations specific to your cloud provider in custom values.yaml file. # Controls which application is deployed and configured applications: -- -- # Deploys the Ambassador Ingress and Redis ambassador: true -- -- # Annotations for different DX Resources. # Type: Array of objects # Sample values for core: # core: # - key: KEY1 # value: VALUE1 # - key: KEY2 # value: VALUE2 annotations: service: # Annotations for ambassador service. ambassador: - key: cloud.google.com/load-balancer-type value : \"Internal\" Note To switch your existing deployment from an internal network to a public network, remove the annotation from the ambassador service. After updating values.yaml with annotations, run helm update command. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n dxns . -f ./cloud-deploy-values.yaml Do a helm update with your existing custom values.yaml file to make sure all the updates are present in the deployment. Run the helm update command with the updated values.yaml file. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n external-lb . -f ./cloud-deploy-values.yaml","title":"Configure to deploy on the internal network"},{"location":"platform/kubernetes/deployment/preparation/optional_internal_networking/#configure-to-deploy-on-the-internal-network","text":"This section contains the procedure to deploy DX on the internal network.","title":"Configure to deploy on the internal network"},{"location":"platform/kubernetes/deployment/preparation/optional_internal_networking/#how-to-deploy-dx-on-the-internal-network","text":"To deploy DX on the internal network (with no public access), we need to add the platform-specific annotations for the ambassador service. Update your custom values.yaml file with the annotation specific to your cloud provider. Refer to the list of annotations . annotations: service: # Annotations for ambassador service. ambassador: [] Example for GKE: annotations: service: # Annotations for ambassador service. ambassador: - key: cloud.google.com/load-balancer-type value : \"Internal\"","title":"How to deploy DX on the internal network"},{"location":"platform/kubernetes/deployment/preparation/optional_internal_networking/#how-to-update-the-existing-deployment","text":"Follow the steps to update an existing deployment from an external network to an internal network or vice-versa: Note Update to the network type results change in IP address and requires updates to your DNS services. Disable ambassador in your custom values.yaml file and then do helm update. Example: # Controls which application is deployed and configured applications: -- -- # Deploys the Ambassador Ingress and Redis ambassador: false After updating your custom values.yaml file, run helm update command. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n dxns . -f ./cloud-deploy-values.yaml After the update is completed, enable the ambassador and add annotations specific to your cloud provider in custom values.yaml file. # Controls which application is deployed and configured applications: -- -- # Deploys the Ambassador Ingress and Redis ambassador: true -- -- # Annotations for different DX Resources. # Type: Array of objects # Sample values for core: # core: # - key: KEY1 # value: VALUE1 # - key: KEY2 # value: VALUE2 annotations: service: # Annotations for ambassador service. ambassador: - key: cloud.google.com/load-balancer-type value : \"Internal\" Note To switch your existing deployment from an internal network to a public network, remove the annotation from the ambassador service. After updating values.yaml with annotations, run helm update command. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n dxns . -f ./cloud-deploy-values.yaml Do a helm update with your existing custom values.yaml file to make sure all the updates are present in the deployment. Run the helm update command with the updated values.yaml file. helm upgrade dx-deployment -n <your namespace> . -f ./<your customized `values.yaml` file> Example: helm upgrade dx-deployment -n external-lb . -f ./cloud-deploy-values.yaml","title":"How to update the existing deployment"},{"location":"platform/kubernetes/deployment/preparation/optional_labels_annotations/","text":"Configure Labels and Annotations This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: release helm.sh/chart app.kubernetes.io/version app.kubernetes.io/managed-by app.kubernetes.io/name app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Configure environment variables for DX resources This section explains the configuration of environment variables for different DX resources. Environment variables To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note Additional environment values are not mandatory for a deployment. Sample environment variables for core pods To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2","title":"Configure Labels and Annotations"},{"location":"platform/kubernetes/deployment/preparation/optional_labels_annotations/#configure-labels-and-annotations","text":"This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: release helm.sh/chart app.kubernetes.io/version app.kubernetes.io/managed-by app.kubernetes.io/name app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2","title":"Configure Labels and Annotations"},{"location":"platform/kubernetes/deployment/preparation/optional_labels_annotations/#configure-environment-variables-for-dx-resources","text":"This section explains the configuration of environment variables for different DX resources.","title":"Configure environment variables for DX resources"},{"location":"platform/kubernetes/deployment/preparation/optional_labels_annotations/#environment-variables","text":"To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note Additional environment values are not mandatory for a deployment.","title":"Environment variables"},{"location":"platform/kubernetes/deployment/preparation/optional_labels_annotations/#sample-environment-variables-for-core-pods","text":"To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2","title":"Sample environment variables for core pods"},{"location":"platform/kubernetes/deployment/preparation/optional_nodeselectors/","text":"Using NodeSelectors This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Prepare cluster nodes You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: # Edit Node kubectl edit node k8s-node-4 OpenShift Client: # Edit Node kubectl edit node k8s-node-4 The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: # Execute lookup via kubectl kubectl get node k8s-node-4 --show-labels # Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready <none> 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress # Execute lookup via kubectl oc get node k8s-node-4 --show-labels # Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready <none> 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress Configure nodes You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none>","title":"Using NodeSelectors"},{"location":"platform/kubernetes/deployment/preparation/optional_nodeselectors/#using-nodeselectors","text":"This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node.","title":"Using NodeSelectors"},{"location":"platform/kubernetes/deployment/preparation/optional_nodeselectors/#prepare-cluster-nodes","text":"You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: # Edit Node kubectl edit node k8s-node-4 OpenShift Client: # Edit Node kubectl edit node k8s-node-4 The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: # Execute lookup via kubectl kubectl get node k8s-node-4 --show-labels # Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready <none> 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress # Execute lookup via kubectl oc get node k8s-node-4 --show-labels # Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready <none> 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress","title":"Prepare cluster nodes"},{"location":"platform/kubernetes/deployment/preparation/optional_nodeselectors/#configure-nodes","text":"You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none>","title":"Configure nodes"},{"location":"platform/kubernetes/deployment/preparation/optional_rs_sidecar_log/","text":"Configure Remote Search sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. ``` logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\" ```","title":"Configure Remote Search sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/optional_rs_sidecar_log/#configure-remote-search-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. ``` logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\" ```","title":"Configure Remote Search sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/overview/","text":"Overview This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 using Helm. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Read this first Helm architecture for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 and later deployment using Helm. Mandatory tasks The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm. Prepare a namespace Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster. Prepare the Helm deployment configuration file Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Load container images This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. Configure persistent volume claims HCL Digital Experience requires Persistent Volumes to be present in order to have all applications running. This topic explains to you how you can configure the deployment to use the Persistent Volumes in your cluster. Configure networking Depending on your deployment, your requirements for networking may differ from the default. This topic shows you what needs to be configured to get HCL Digital Experience up and running inside your Kubernetes cluster and be accessible. Configure internal network Add the platform-specific annotations for the ambassador service to deploy DX on the internal network (network with no public access). Configure ingress certificate The Ambassador Ingress requires a SSL certificate to use, this topic shows how to configure that for the HCL Digital Experience Kubernetes deployment. Optional These tasks may be relevant for you if you have specific requirements for the deployment. Using ImagePullSecrets In your deployment it might be necessary to access a container image registry that requires credentials to pull images from. This topic explains to you how you configure the DX deployment to work under such conditions. Using NodeSelectors You may want to have certain applications of the HCL Digital Experience Kubernetes deployment to be running on certain Nodes inside your cluster. This topic will explain how to achieve that for each application. Choose deployed applications This topic shows you how to disable and enable specific applications from HCL Digital Experience in your Kubernetes deployment. Configure applications There are application specific configurations that you may want to adjust, e.g. if Core should behave as a rendering or authoring environment. This topic explains to you how that can be configured. Configure scaling This topic explains how you can configure Pod count and automated scaling for applications that support it. Configure credentials This topic shows you how to adjust credentials that are used for the deployment. Configure Core sidecar logging This topic describes how to expose logs that are written to files by the DX Core application. Configure Remote Search sidecar logging This topic describes how you can expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. Configure Labels and Annotations This topic documents the configuration of labels and annotations for different DX resources. Try experimental features This topic describes the incubator section in the Helm Charts.","title":"Overview"},{"location":"platform/kubernetes/deployment/preparation/overview/#overview","text":"This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 using Helm. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Read this first Helm architecture for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 and later deployment using Helm.","title":"Overview"},{"location":"platform/kubernetes/deployment/preparation/overview/#mandatory-tasks","text":"The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm.","title":"Mandatory tasks"},{"location":"platform/kubernetes/deployment/preparation/overview/#prepare-a-namespace","text":"Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster.","title":"Prepare a namespace"},{"location":"platform/kubernetes/deployment/preparation/overview/#prepare-the-helm-deployment-configuration-file","text":"Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient.","title":"Prepare the Helm deployment configuration file"},{"location":"platform/kubernetes/deployment/preparation/overview/#load-container-images","text":"This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods.","title":"Load container images"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-persistent-volume-claims","text":"HCL Digital Experience requires Persistent Volumes to be present in order to have all applications running. This topic explains to you how you can configure the deployment to use the Persistent Volumes in your cluster.","title":"Configure persistent volume claims"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-networking","text":"Depending on your deployment, your requirements for networking may differ from the default. This topic shows you what needs to be configured to get HCL Digital Experience up and running inside your Kubernetes cluster and be accessible.","title":"Configure networking"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-internal-network","text":"Add the platform-specific annotations for the ambassador service to deploy DX on the internal network (network with no public access).","title":"Configure internal network"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-ingress-certificate","text":"The Ambassador Ingress requires a SSL certificate to use, this topic shows how to configure that for the HCL Digital Experience Kubernetes deployment.","title":"Configure ingress certificate"},{"location":"platform/kubernetes/deployment/preparation/overview/#optional","text":"These tasks may be relevant for you if you have specific requirements for the deployment.","title":"Optional"},{"location":"platform/kubernetes/deployment/preparation/overview/#using-imagepullsecrets","text":"In your deployment it might be necessary to access a container image registry that requires credentials to pull images from. This topic explains to you how you configure the DX deployment to work under such conditions.","title":"Using ImagePullSecrets"},{"location":"platform/kubernetes/deployment/preparation/overview/#using-nodeselectors","text":"You may want to have certain applications of the HCL Digital Experience Kubernetes deployment to be running on certain Nodes inside your cluster. This topic will explain how to achieve that for each application.","title":"Using NodeSelectors"},{"location":"platform/kubernetes/deployment/preparation/overview/#choose-deployed-applications","text":"This topic shows you how to disable and enable specific applications from HCL Digital Experience in your Kubernetes deployment.","title":"Choose deployed applications"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-applications","text":"There are application specific configurations that you may want to adjust, e.g. if Core should behave as a rendering or authoring environment. This topic explains to you how that can be configured.","title":"Configure applications"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-scaling","text":"This topic explains how you can configure Pod count and automated scaling for applications that support it.","title":"Configure scaling"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-credentials","text":"This topic shows you how to adjust credentials that are used for the deployment.","title":"Configure credentials"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-core-sidecar-logging","text":"This topic describes how to expose logs that are written to files by the DX Core application.","title":"Configure Core sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-remote-search-sidecar-logging","text":"This topic describes how you can expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application.","title":"Configure Remote Search sidecar logging"},{"location":"platform/kubernetes/deployment/preparation/overview/#configure-labels-and-annotations","text":"This topic documents the configuration of labels and annotations for different DX resources.","title":"Configure Labels and Annotations"},{"location":"platform/kubernetes/deployment/preparation/overview/#try-experimental-features","text":"This topic describes the incubator section in the Helm Charts.","title":"Try experimental features"},{"location":"platform/kubernetes/deployment/preparation/prepare_configuration/","text":"Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Warning Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. The configuration flow Helm provides multiple ways to define values that can be processed to run an installation. Processing involves a three-step approach, that is ordered sequentially within a hierarchy. Helm Chart values.yaml Every Helm Chart contains a values.yaml file. It defines all configurable parameters that a Helm Chart accepts and the default values that are used during an installation. If you do not provide any other configuration during an installation, Helm extracts all deployment information from the values.yaml file inside the Helm Chart. All parameters that were not overwritten using any other configuration methods return to their default values from the values.yaml file inside the Helm Chart. Custom value files Helm provides you with a way to maintain your own custom values files. You can specify a custom values file you want to use when running an installation. This custom values file only needs to contain the parameters that you want to overwrite with your preferred settings. Note There is no need to have the same complete set of parameters inside your custom values file, as there are available by default in the Helm Chart values.yaml . As outlined previously in this section, everything that is not defined in your custom values file are applied using the defaults from values.yaml inside the Helm Charts. Please be aware that the parameters you can configure using your custom values file need to exactly align with those provided by the Helm Charts own values.yaml. You cannot configure anything that is not exposed in the values.yaml definition. Override parameters It is possible to define values using a --set parameter in the Helm CLI during the installation of a Helm Chart. Since there are many values that can be configured in the HCL Digital Experience deployment, we do not recommend this technique, since it makes installation commands very large and confusing. The default HCL DX 9.5 Container values.yaml file HCL DX 9.5 Helm Chart provides a default values.yaml, which contains all possible configuration parameters. To access this file, you may use the following command when you have the HCL DX 9.5 CF196 or later Helm Chart tar.gz file on hand: # Command to extract values.ymal from Helm Chart helm show values hcl-dx-deployment.tar.gz > values.yaml The file contains all configurable parameters and their default values. You may use this file as a blueprint to create your own custom-values.yaml . You may also just rename the extracted values.yaml to custom-values.yaml . Note Having a complete copy of the default values.yaml is not necessary and may bloat your configuration file with values that are already present in the DX Helm Chart. A custom configuration file Helm allows you to provide a custom configuration file during the installation or upgrade process. That file only overwrites settings that are defined within it. For parts of the configuration that are not defined in your custom configuration file, Helm returns to the default values in the values.yaml file inside the DX Helm Chart. This allows you to create a file that only overwrites settings that are required, keeping the overall size of your configuration file small and the maintainability high. This Help Center documentation refers to the custom configuration file as custom-values.yaml . You may name your custom configuration file as preferred.","title":"Prepare configuration"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/","text":"Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Full Kubernetes or OpenShift deployment If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured. Core host In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly. Configure Cross Origin Resource Sharing (CORS) The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured. Hybrid host Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details. Configure Ingress certificate To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading. Generate self-signed certificate It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment. Use certificate Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience Configure secret in deployment You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note Verify you have entered the correct name. Configure minimum TLS version for Ingress From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps.","title":"Configure networking"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#configure-networking","text":"This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster.","title":"Configure networking"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#full-kubernetes-or-openshift-deployment","text":"If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured.","title":"Full Kubernetes or OpenShift deployment"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#core-host","text":"In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly.","title":"Core host"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#configure-cross-origin-resource-sharing-cors","text":"The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured.","title":"Configure Cross Origin Resource Sharing (CORS)"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#hybrid-host","text":"Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details.","title":"Hybrid host"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#configure-ingress-certificate","text":"To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading.","title":"Configure Ingress certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#generate-self-signed-certificate","text":"It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment.","title":"Generate self-signed certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#use-certificate","text":"Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience","title":"Use certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#configure-secret-in-deployment","text":"You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note Verify you have entered the correct name.","title":"Configure secret in deployment"},{"location":"platform/kubernetes/deployment/preparation/prepare_configure_networking/#configure-minimum-tls-version-for-ingress","text":"From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps.","title":"Configure minimum TLS version for Ingress"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/","text":"Configure ingress certificate To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading. Generate self-signed certificate It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment. Use certificate Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience Configure secret in deployment You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note Verify you have entered the correct name. Configure minimum TLS version for Ingress From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\"","title":"Configure ingress certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/#configure-ingress-certificate","text":"To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading.","title":"Configure ingress certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/#generate-self-signed-certificate","text":"It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment.","title":"Generate self-signed certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/#use-certificate","text":"Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience","title":"Use certificate"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/#configure-secret-in-deployment","text":"You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note Verify you have entered the correct name.","title":"Configure secret in deployment"},{"location":"platform/kubernetes/deployment/preparation/prepare_ingress_certificate/#configure-minimum-tls-version-for-ingress","text":"From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\"","title":"Configure minimum TLS version for Ingress"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/","text":"Load images This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same. Extract HCL Digital Experience 9.5 package. The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. # Unzip of HCL Digital Experience 9.5 CFxxx package unzip hcl-dx-kubernetes-v95-CF196.zip The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt # Notices file dx-dx-ambassador-image-154.tar.gz # Image for the Ambassador Ingress hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for the Core Operator (not needed for Helm deployments) hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip # Cloud deployment scripts incl. dxctl (not needed for Helm deployments) hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Content Composer hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for Core hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for the Digital Asset Management Operator (not needed for Helm deployments) hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Digital Asset Management hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip # Sample UI for Experience API hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Image Processor hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz # Image for OpenLDAP hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Digital Asset Management Persistence hcl-dx-redis-image-X.X.X.tar.gz # Image for Ambassador Ingress Redis hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for Remote Search hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Ring API hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz # Image for Runtime Controller hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz # Helm Charts Load images locally To load the individual image files, you may use the following command: # Command to load container image into local repository # docker load < image-file-name.tar.gz docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: # Command to load all images at once # Since HCL Digital Experience images are all containing the word \"images\", # we can filter for fitting tar.gz files ls -f | grep image | xargs -L 1 docker load -i This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: # List all images docker images # Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB Re-tag images If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Warning Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: # Re-tag an existing loaded image # docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION # Example command for DX Core: docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: # Command to prefix all HCL Digital Experience container images # export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" # First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header # Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' The output may be verified by using the following command: # List all images docker images # Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB Push to repository You may use the following command to push the container images to your repository: # Push the new tagged images # docker push NEW_IMAGE_TAG:VERSION # Example command for core: docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 If you want to push all your locally processed images, you may use the following command: # Command to push all HCL Digital Experience images to a remote repository # export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" # Push the images, first we filter for the ones necessary # Second we execute a docker push for each image docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster. Adjust deployment configuration After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. # Fill in the values fitting to your configuration # Ensure to use the correct image version tags images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\" Additional Tasks If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Using ImagePullSecrets topic for instructions on how to configure this.","title":"Load images"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#load-images","text":"This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same.","title":"Load images"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#extract-hcl-digital-experience-95-package","text":"The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. # Unzip of HCL Digital Experience 9.5 CFxxx package unzip hcl-dx-kubernetes-v95-CF196.zip The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt # Notices file dx-dx-ambassador-image-154.tar.gz # Image for the Ambassador Ingress hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for the Core Operator (not needed for Helm deployments) hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip # Cloud deployment scripts incl. dxctl (not needed for Helm deployments) hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Content Composer hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for Core hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for the Digital Asset Management Operator (not needed for Helm deployments) hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Digital Asset Management hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip # Sample UI for Experience API hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Image Processor hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz # Image for OpenLDAP hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Digital Asset Management Persistence hcl-dx-redis-image-X.X.X.tar.gz # Image for Ambassador Ingress Redis hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz # Image for Remote Search hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz # Image for Ring API hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz # Image for Runtime Controller hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz # Helm Charts","title":"Extract HCL Digital Experience 9.5 package."},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#load-images-locally","text":"To load the individual image files, you may use the following command: # Command to load container image into local repository # docker load < image-file-name.tar.gz docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: # Command to load all images at once # Since HCL Digital Experience images are all containing the word \"images\", # we can filter for fitting tar.gz files ls -f | grep image | xargs -L 1 docker load -i This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: # List all images docker images # Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB","title":"Load images locally"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#re-tag-images","text":"If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Warning Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: # Re-tag an existing loaded image # docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION # Example command for DX Core: docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: # Command to prefix all HCL Digital Experience container images # export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" # First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header # Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' The output may be verified by using the following command: # List all images docker images # Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB","title":"Re-tag images"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#push-to-repository","text":"You may use the following command to push the container images to your repository: # Push the new tagged images # docker push NEW_IMAGE_TAG:VERSION # Example command for core: docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 If you want to push all your locally processed images, you may use the following command: # Command to push all HCL Digital Experience images to a remote repository # export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" # Push the images, first we filter for the ones necessary # Second we execute a docker push for each image docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster.","title":"Push to repository"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#adjust-deployment-configuration","text":"After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. # Fill in the values fitting to your configuration # Ensure to use the correct image version tags images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\"","title":"Adjust deployment configuration"},{"location":"platform/kubernetes/deployment/preparation/prepare_load_images/#additional-tasks","text":"If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Using ImagePullSecrets topic for instructions on how to configure this.","title":"Additional Tasks"},{"location":"platform/kubernetes/deployment/preparation/prepare_namespace/","text":"You need to create a namespace in your Kubernetes cluster that contains all the resources related to your HCL DX 9.5 Container deployment. It is recommended that this is created before deployment as you may need to add an ImagePullSecret or configure the TLS certificate for the Ambassador Ingress before deployment. Identify a name for your namespace and create it using the following syntax: On Kubernetes platforms Kubectl # Command to create a namespace using kubectl # This example creates a namespace called \"my-namespace\" kubectl create ns my-namespace OpenShift For OpenShift, you must create a namespace with specific settings. Use the following namespace definition and save it as namespace.yaml. You must replace my-namespace in the template with the name of the namespace you are using. apiVersion: v1 kind: Namespace metadata: name: my-namespace annotations: openshift.io/sa.scc.mcs: \"s0:c24,c4\" openshift.io/sa.scc.supplemental-groups: \"1001/10000\" openshift.io/sa.scc.uid-range: \"1000/10000\" OpenShift client # Command to create namespace from template file oc apply -f namespace.yaml","title":"Prepare namespace"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/","text":"PersistentVolumeClaims To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information. Persistent Volume Types Important Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count. Configuration parameters To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to. Sample PVC configurations The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" Sample Persistent Volume definitions Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps.","title":"PersistentVolumeClaims"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/#persistentvolumeclaims","text":"To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information.","title":"PersistentVolumeClaims"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/#persistent-volume-types","text":"Important Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count.","title":"Persistent Volume Types"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/#configuration-parameters","text":"To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to.","title":"Configuration parameters"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/#sample-pvc-configurations","text":"The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\"","title":"Sample PVC configurations"},{"location":"platform/kubernetes/deployment/preparation/prepare_persistent_volume_claims/#sample-persistent-volume-definitions","text":"Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps.","title":"Sample Persistent Volume definitions"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/","text":"HAProxy configuration By default, HAProxy is deployed with a LoadBalancer type service to handle the incoming traffic as well as the SSL offloading for HCL Digital Experience. In addition, the Helm deployment offers adjustability for HAProxy and its services to allow for more flexible deployment and use of custom Ingress Controllers . Note When migrating from Ambassador, the default configuration of HAProxy matches the default configuration of Ambassador as close as possible. Any changes made to scaling , resources , horizontalPodAutoScaler , annotations or labels must be transferred to the appropriate configurations for HAProxy manually. Networking configuration The networking parameters used to configure the HAProxy services are located in networking.haproxy in the values.yaml file. Parameter Description Default value ssl Enable or disable SSL offloading in HAProxy. Depending on this setting, HAProxy handles either HTTP or HTTPS traffic. true serviceType Defines the Kubernetes ServiceType of the HAProxy service. Supported ServiceType includes LoadBalancer , ClusterIP and NodePort LoadBalancer servicePort This value is used to select the port exposed by the HAProxy service. Defaults to port 443 if ssl is set to true , otherwise, port 80 is used. null serviceNodePort This value is used to select the node port exposed by the HAProxy service. Defaults to a port selected by Kubernetes if no value is set. null Note If ssl is set to true , HAProxy will use the certificate that is supplied as a secret in networking.tlsCertSecret . networking : # Networking configurations specific to HAProxy haproxy : # Configuration to enable/disable ssl offloading in HAProxy ssl : true # Configuration to set the service type for the HAProxy service. Supported values are \"ClusterIP\", \"LoadBalancer\", and \"NodePort\" serviceType : \"LoadBalancer\" # Configuration to set the port exposed by the HAProxy Service. If this is not set, port 80 is used if SSL offloading is disabled and port 443 if SSL offloading is enabled. servicePort : # Only applies for the \"NodePort\" serviceType. Configuration to set the NodePort exposed by the HAProxy service. If this is not set, a port is automatically selected by Kubernetes serviceNodePort : This configuration is helpful for those who want to use a custom Ingress Controller to expose the service in a compatible way. Even then, HAProxy will still be active. The Ingress Controller will handle the incoming traffic and then route them to the HAProxy service. HAProxy with and without Ambassador In Container Update CF203, Ambassador is still included along with the new HAProxy service so that you can get familiar with the HAProxy service, use the following configuration options to test both the services side-by-side, and prepare to eventually disable Ambassador and migrate to the HAProxy service. Note that the Ambassador service will be removed in a future version. Both Ambassador and HAProxy can be enabled or disabled in the applications section of the Helm values. Depending on the combination of settings, HAProxy is deployed by itself or in a side-by-side mode with Ambassador. The side-by-side mode is mainly used for migration from Ambassador to HAProxy service. Refer to the Migrate from Ambassador to HAProxy page for migration information. # Controls which application is deployed and configured applications : # Deploys the Ambassador Ingress and Redis ambassador : true # Deploys HAProxy haproxy : true The matrix below shows the deployment options for combinations of HAProxy and Ambassador: Deployment type ambassador haproxy Result fresh deployment ( helm install ) not available true (default) HAProxy update of existing deployment ( helm upgrade ) true (default) true (default) Ambassador and HAProxy (side-by-side mode) update of existing deployment ( helm upgrade ) false true (default) HAProxy Scaling Three replicas of the HAProxy Pod are created by default, and this value can be adjusted in the custom-values.yaml file for the Helm deployment. scaling : # The default number of replicas per application replicas : haproxy : 3 # Change this value as per the number of replicas required for HAProxy. Resources HAProxy is set to use the following resource allocation values by default. The values can be adjusted as required in the custom-values.yaml file for the Helm deployment. resources : # HAProxy resource allocation haproxy : requests : cpu : \"1000m\" memory : \"1G\" limits : cpu : \"2000m\" memory : \"4G\"","title":"HAProxy configuration"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/#haproxy-configuration","text":"By default, HAProxy is deployed with a LoadBalancer type service to handle the incoming traffic as well as the SSL offloading for HCL Digital Experience. In addition, the Helm deployment offers adjustability for HAProxy and its services to allow for more flexible deployment and use of custom Ingress Controllers . Note When migrating from Ambassador, the default configuration of HAProxy matches the default configuration of Ambassador as close as possible. Any changes made to scaling , resources , horizontalPodAutoScaler , annotations or labels must be transferred to the appropriate configurations for HAProxy manually.","title":"HAProxy configuration"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/#networking-configuration","text":"The networking parameters used to configure the HAProxy services are located in networking.haproxy in the values.yaml file. Parameter Description Default value ssl Enable or disable SSL offloading in HAProxy. Depending on this setting, HAProxy handles either HTTP or HTTPS traffic. true serviceType Defines the Kubernetes ServiceType of the HAProxy service. Supported ServiceType includes LoadBalancer , ClusterIP and NodePort LoadBalancer servicePort This value is used to select the port exposed by the HAProxy service. Defaults to port 443 if ssl is set to true , otherwise, port 80 is used. null serviceNodePort This value is used to select the node port exposed by the HAProxy service. Defaults to a port selected by Kubernetes if no value is set. null Note If ssl is set to true , HAProxy will use the certificate that is supplied as a secret in networking.tlsCertSecret . networking : # Networking configurations specific to HAProxy haproxy : # Configuration to enable/disable ssl offloading in HAProxy ssl : true # Configuration to set the service type for the HAProxy service. Supported values are \"ClusterIP\", \"LoadBalancer\", and \"NodePort\" serviceType : \"LoadBalancer\" # Configuration to set the port exposed by the HAProxy Service. If this is not set, port 80 is used if SSL offloading is disabled and port 443 if SSL offloading is enabled. servicePort : # Only applies for the \"NodePort\" serviceType. Configuration to set the NodePort exposed by the HAProxy service. If this is not set, a port is automatically selected by Kubernetes serviceNodePort : This configuration is helpful for those who want to use a custom Ingress Controller to expose the service in a compatible way. Even then, HAProxy will still be active. The Ingress Controller will handle the incoming traffic and then route them to the HAProxy service.","title":"Networking configuration"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/#haproxy-with-and-without-ambassador","text":"In Container Update CF203, Ambassador is still included along with the new HAProxy service so that you can get familiar with the HAProxy service, use the following configuration options to test both the services side-by-side, and prepare to eventually disable Ambassador and migrate to the HAProxy service. Note that the Ambassador service will be removed in a future version. Both Ambassador and HAProxy can be enabled or disabled in the applications section of the Helm values. Depending on the combination of settings, HAProxy is deployed by itself or in a side-by-side mode with Ambassador. The side-by-side mode is mainly used for migration from Ambassador to HAProxy service. Refer to the Migrate from Ambassador to HAProxy page for migration information. # Controls which application is deployed and configured applications : # Deploys the Ambassador Ingress and Redis ambassador : true # Deploys HAProxy haproxy : true The matrix below shows the deployment options for combinations of HAProxy and Ambassador: Deployment type ambassador haproxy Result fresh deployment ( helm install ) not available true (default) HAProxy update of existing deployment ( helm upgrade ) true (default) true (default) Ambassador and HAProxy (side-by-side mode) update of existing deployment ( helm upgrade ) false true (default) HAProxy","title":"HAProxy with and without Ambassador"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/#scaling","text":"Three replicas of the HAProxy Pod are created by default, and this value can be adjusted in the custom-values.yaml file for the Helm deployment. scaling : # The default number of replicas per application replicas : haproxy : 3 # Change this value as per the number of replicas required for HAProxy.","title":"Scaling"},{"location":"platform/kubernetes/haproxy-migration/haproxy-configuration/#resources","text":"HAProxy is set to use the following resource allocation values by default. The values can be adjusted as required in the custom-values.yaml file for the Helm deployment. resources : # HAProxy resource allocation haproxy : requests : cpu : \"1000m\" memory : \"1G\" limits : cpu : \"2000m\" memory : \"4G\"","title":"Resources"},{"location":"platform/kubernetes/haproxy-migration/haproxy-fresh-installation/","text":"Fresh Installations using HAProxy In fresh DX deployments from CF203 and later, HAProxy is deployed by default, in place of Ambassador. Although HAProxy is introduced in CF203, you will still have Ambassador CustomResourceDefinitions shipped with it to enable you to smoothly transition from Ambassador to HAProxy. However, for fresh deployments, the installation of Ambassador should be skipped by adding the --skip-crds flag to the helm install command. Example: helm install -n <namespace> -f <custom-values.yaml> <release-name> <path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz> --skip-crds","title":"Fresh Installations using HAProxy"},{"location":"platform/kubernetes/haproxy-migration/haproxy-fresh-installation/#fresh-installations-using-haproxy","text":"In fresh DX deployments from CF203 and later, HAProxy is deployed by default, in place of Ambassador. Although HAProxy is introduced in CF203, you will still have Ambassador CustomResourceDefinitions shipped with it to enable you to smoothly transition from Ambassador to HAProxy. However, for fresh deployments, the installation of Ambassador should be skipped by adding the --skip-crds flag to the helm install command. Example: helm install -n <namespace> -f <custom-values.yaml> <release-name> <path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz> --skip-crds","title":"Fresh Installations using HAProxy"},{"location":"platform/kubernetes/haproxy-migration/haproxy-introduction/","text":"Overview Important Ambassador is deprecated in CF203 and is replaced by HAProxy. Since Ambassador will be removed in the subsequent versions, you are expected to migrate to HAProxy in CF203 in preparation for the upcoming releases. Up to version CF202, the deployment of HCL Digital Experience included an ingress controller \"Ambassador\" as part of the DX Kubernetes namespace. Ambassador required a set of CustomResourceDefinitions , which needed to be deployed as a cluster-wide Kubernetes resource. DX Ambassador before CF203 To prevent incompatibilities with newer Ambassador versions and allow more flexible use of other Ingress Controllers , starting with CF203, the Ambassador ingress controller is deprecated and will be removed in the subsequent releases. With CF203, HAProxy is introduced as a new component that replaces Ambassador. It acts as a single-entry point into the DX namespace and handles all internal routing. It can either be configured to expose a LoadBalancer service to directly accept incoming traffic into the cluster or act only as an internal Kubernetes service to accept traffic from a Kubernetes Ingress Controller at the cluster level. As a result, HAProxy takes over all functionalities that the Ambassador ingress controller provided for DX beforehand. HAProxy reverse proxy with optional external Ingress","title":"Overview"},{"location":"platform/kubernetes/haproxy-migration/haproxy-introduction/#overview","text":"Important Ambassador is deprecated in CF203 and is replaced by HAProxy. Since Ambassador will be removed in the subsequent versions, you are expected to migrate to HAProxy in CF203 in preparation for the upcoming releases. Up to version CF202, the deployment of HCL Digital Experience included an ingress controller \"Ambassador\" as part of the DX Kubernetes namespace. Ambassador required a set of CustomResourceDefinitions , which needed to be deployed as a cluster-wide Kubernetes resource. DX Ambassador before CF203 To prevent incompatibilities with newer Ambassador versions and allow more flexible use of other Ingress Controllers , starting with CF203, the Ambassador ingress controller is deprecated and will be removed in the subsequent releases. With CF203, HAProxy is introduced as a new component that replaces Ambassador. It acts as a single-entry point into the DX namespace and handles all internal routing. It can either be configured to expose a LoadBalancer service to directly accept incoming traffic into the cluster or act only as an internal Kubernetes service to accept traffic from a Kubernetes Ingress Controller at the cluster level. As a result, HAProxy takes over all functionalities that the Ambassador ingress controller provided for DX beforehand. HAProxy reverse proxy with optional external Ingress","title":"Overview"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/","text":"Migrate from Ambassador to HAProxy Important This page only applies for updates of existing DX deployments and if an immediate switch to HAProxy together with disabling Ambassador is not possible or has to be tested first. For all other cases please refer to the Fresh Installations without Ambassador page or the configuration options in HAProxy with and without Ambassador to disable Ambassador before the upgrade. The preferred migration route is a blue green deployment where one instance can directly be deployed with Ambassador disabled and HAProxy enabled and tested. The following instructions are meant to offer an alternative way to test Ambassador and HAProxy side-by-side if a blue green deployment is not available. This documentation explains the method to test HAProxy side by side with Ambassador to confirm the setup is working before switching completely. In this case Ambassador keeps running uninterrupted, and HAProxy is running on its own port (configurable, 31001 by default). Ambassador and HAProxy side-by-side deployment In the Helm chart custom values file, you can see an option for HAProxy similar to the other applications to enable/disable it. Please refer to the HAProxy configuration page for details. # Controls which application is deployed and configured applications : # Deploys the Ambassador Ingress and Redis ambassador : true # Deploy HAProxy haproxy : true For the side-by-side mode of Ambassador and HAProxy change the value of both to true and upgrade the Helm deployment. helm upgrade -n <namespace> -f <custom-values.yaml> <release-name> <path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz> After deploying, the services and pods of both applications (Ambassador and HAProxy) should be up and running in the namespace. Make sure that Ambassador is now working as an active request handler so its service type would be a default LoadBalancer and HAProxy is running as a passive instance so that its service would be a ClusterIP service. To verify this, run below command. kubectl -n <namespace> get all --selector 'app in (<release-name>-haproxy, <release-name>-ambassador)' This will print as an output for all the instances of Ambassador and HAProxy currently running into the namespace. The output will list out all the running pods as well as services only for Ambassador and HAProxy, which should be similar as shown below. NAME READY STATUS RESTARTS AGE pod/dx-deployment-ambassador-7689f9fd45-sn9nb 1/1 Running 0 2m7s pod/dx-deployment-haproxy-6b464bd866-p777b 1/1 Running 0 8m32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dx-deployment-ambassador LoadBalancer 10.110.149.188 <pending> 80:32633/TCP,443:31627/TCP,31001:32113/TCP,1636:31232/TCP,9043:30319/TCP 2m7s service/dx-deployment-haproxy ClusterIP 10.101.191.20 <none> 443/TCP,9043/TCP,1636/TCP 8m32s NAME DESIRED CURRENT READY AGE replicaset.apps/dx-deployment-ambassador-7689f9fd45 1 1 1 2m7s replicaset.apps/dx-deployment-haproxy-6b464bd866 1 1 1 8m32s HAProxy Testing (deployment verification) Notes: This does not work for hybrid deployments. For testing in hybrid environments please skip directly to Test Applications on HAProxy Port . To test and verify that HAProxy is deployed without any issue into the cluster, follow the below steps. HAProxy will communicate via a dedicated port, so whenever a request is made through that port, that request first goes to the Ambassador and is then forwarded to HAProxy. This dedicated port can be configured from the values of the Helm chart. The CORS configuration has to be temporarily extended with entries including the port. incubator : networking : haproxy : # The port on which Ambassador redirects traffic to HAProxy to test it in a side-by-side mode before switching to HAProxy ambassadorPassthroughPort : 31001 addon : contentComposer : corsOrigin : https://<hostname>:31001 designStudio : corsOrigin : https://<hostname>:31001 digitalAssetManagement : corsOrigin : https://<hostname>:31001 imageProcessor : corsOrigin : https://<hostname>:31001 ringApi : corsOrigin : https://<hostname>:31001 WebSphere Configuration Setting (for HAProxy dedicated configurable port) The virtual host needs to be configured for the HAProxy port, into the WebSphere server for side-by-side mode. This is used to identify the request made from the external host with that dedicated port for HAProxy. Here, for the example, the default port 31001 is configured as a dedicated port for the HAProxy. Refer to the screenshots below to configure the dedicated port in the virtual host of the WebSphere server. Open WebSphere console and navigate to Virtual Hosts, then click on default_host (or your preferred host) Click on Host Aliases to open list of ports Click on New button to add new port Enter the port in the text field, then click Apply to apply the changes Click on Save After adding port to the virtual host, it can be viewed in the list For the changes to take effect, the WebSphere_Portal server must be restarted. To do so, recycle the Core Pods using the following command. Make sure to replace <release-name> with the Helm release name chosen during deployment and to insert the correct <namespace> : kubectl -n <namespace> rollout restart statefulset.apps/<release-name>-core After the above changes are made, you can append the defined dedicated port into the request URL to test DX using HAProxy. The request URL should look like below (here, 31001 is used for an example). The normal request to access the portal: https://<host-name>/wps/myportal Custom request with port 31001 : https://<host-name>:31001/wps/myportal If DX is rendering as expected using the dedicated port, HAProxy does serve the requests as expected. Test Applications on HAProxy Port Navigate to the Application menu. The Pages present in the menu are using Ambassador by default. The goal is to create a new, temporary page which points to HAProxy port for testing. To do so navigate to Administration Open Administration within the Application Menu Within that, navigate to Pages . Traverse through Content Root and Practitioner Studio to view the Application menu contents. Open Pages within the Administration Page Open Content Root Open Practitioner Studio within Content root Here you need to find and copy the page which you want to test with HAProxy. Using the New Page option a new page within the Application Menu can be created. The properties set for the page which needs to be recreated with HAProxy can be copied from the existing one and entered within the new page. Duplicate the page which needs to be tested with HAProxy The properties are defined here and can be modified to specify the HAProxy changes. If any Page properties or other options are specified, you can copy those as well to the new HAProxy specific page. Properties of the page which needs to be tested with HAProxy Newly created HAProxy Page Once the details are copied, the new page can be seen within the pages section. Next the contents to be displayed within the page needs to be defined. For this a Portlet is added and used to display the contents of the page and then save it. Search for the Portlet to be added Once the Portlet is added, the shared property values need to be updated with the HAProxy port number. Open Edit Shared Setting of the new Portlet added The application ID remains the same ( medialibrary in this example) while the URL is updated such that it calls the HAProxy port Adding HAProxy port to the URL After adding the content, both the original Ambassador page is running on its default URL, while the new page is also available in its respective port URL using HAProxy. Ambassador Page: Newly created HAProxy Page: Enabling HAProxy port in Web Content (Image) Another functionality to test if Ambassador as well as HAProxy are running is to use the Image Picker . Traverse through the Web Content to open and edit or create a page. Sample Articles within Web Content View the source code and add an image tag. Edit the source code and add Image Tags In the pop up opened, we can decide which source we want to include the image from, in this scenario it would be selected withing the DAM source. Select the source as DAM and Insert Image On clicking Select Image another pop up is displayed. This will always point to the default application (without port specified) and cannot be configured. The image is selected and added to the source code Insert Image from DAM In the source code the image is added via the img tag, since this was selected from the DAM pointing to Ambassador the port specified is the default port. For testing we can add another image tag an update the port value to the new HAProxy port. Image fetched from default DAM and DAM of HAProxy port After saving the changes, both Ambassador as well the HAProxy fetched images should be visible. Disable Ambassador After HAProxy is verified and deployed into the cluster, Ambassador can be disabled such that only HAProxy is running and handles the requests directly. After disabling Ambassador, the ambassadorPassthroughPort ( 31001 ) won't be available anymore. To do so update the value file with Ambassador application flag to false and upgrade the helm chart. applications : # Setting ambassador flag to false ambassador : false # Setting HAProxy flag to true haproxy : true After disabling Ambassador, the Ambassador pods and services are removed and only HAProxy is up and running. All the requests will be handled by HAProxy directly. To verify that Ambassador pods and services are terminated from the namespace, run the following command. kubectl -n <namespace> get all --selector 'app in (<release-name>-haproxy, <release-name>-ambassador)' Make sure that this command shows only the HAProxy instances, and not the Ambassador instances (pods as well as services) because they are already terminated from the namespace. The output should look as follows. NAME READY STATUS RESTARTS AGE pod/dx-deployment-haproxy-6b464bd866-p777b 1/1 Running 0 39m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dx-deployment-haproxy LoadBalancer 10.101.191.20 10.134.209.53 443:30318/TCP,9043:30521/TCP,1636:31777/TCP 39m NAME DESIRED CURRENT READY AGE replicaset.apps/dx-deployment-haproxy-6b464bd866 1 1 1 39m","title":"Migrate from Ambassador to HAProxy"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#migrate-from-ambassador-to-haproxy","text":"Important This page only applies for updates of existing DX deployments and if an immediate switch to HAProxy together with disabling Ambassador is not possible or has to be tested first. For all other cases please refer to the Fresh Installations without Ambassador page or the configuration options in HAProxy with and without Ambassador to disable Ambassador before the upgrade. The preferred migration route is a blue green deployment where one instance can directly be deployed with Ambassador disabled and HAProxy enabled and tested. The following instructions are meant to offer an alternative way to test Ambassador and HAProxy side-by-side if a blue green deployment is not available. This documentation explains the method to test HAProxy side by side with Ambassador to confirm the setup is working before switching completely. In this case Ambassador keeps running uninterrupted, and HAProxy is running on its own port (configurable, 31001 by default).","title":"Migrate from Ambassador to HAProxy"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#ambassador-and-haproxy-side-by-side-deployment","text":"In the Helm chart custom values file, you can see an option for HAProxy similar to the other applications to enable/disable it. Please refer to the HAProxy configuration page for details. # Controls which application is deployed and configured applications : # Deploys the Ambassador Ingress and Redis ambassador : true # Deploy HAProxy haproxy : true For the side-by-side mode of Ambassador and HAProxy change the value of both to true and upgrade the Helm deployment. helm upgrade -n <namespace> -f <custom-values.yaml> <release-name> <path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz> After deploying, the services and pods of both applications (Ambassador and HAProxy) should be up and running in the namespace. Make sure that Ambassador is now working as an active request handler so its service type would be a default LoadBalancer and HAProxy is running as a passive instance so that its service would be a ClusterIP service. To verify this, run below command. kubectl -n <namespace> get all --selector 'app in (<release-name>-haproxy, <release-name>-ambassador)' This will print as an output for all the instances of Ambassador and HAProxy currently running into the namespace. The output will list out all the running pods as well as services only for Ambassador and HAProxy, which should be similar as shown below. NAME READY STATUS RESTARTS AGE pod/dx-deployment-ambassador-7689f9fd45-sn9nb 1/1 Running 0 2m7s pod/dx-deployment-haproxy-6b464bd866-p777b 1/1 Running 0 8m32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dx-deployment-ambassador LoadBalancer 10.110.149.188 <pending> 80:32633/TCP,443:31627/TCP,31001:32113/TCP,1636:31232/TCP,9043:30319/TCP 2m7s service/dx-deployment-haproxy ClusterIP 10.101.191.20 <none> 443/TCP,9043/TCP,1636/TCP 8m32s NAME DESIRED CURRENT READY AGE replicaset.apps/dx-deployment-ambassador-7689f9fd45 1 1 1 2m7s replicaset.apps/dx-deployment-haproxy-6b464bd866 1 1 1 8m32s","title":"Ambassador and HAProxy side-by-side deployment"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#haproxy-testing-deployment-verification","text":"Notes: This does not work for hybrid deployments. For testing in hybrid environments please skip directly to Test Applications on HAProxy Port . To test and verify that HAProxy is deployed without any issue into the cluster, follow the below steps. HAProxy will communicate via a dedicated port, so whenever a request is made through that port, that request first goes to the Ambassador and is then forwarded to HAProxy. This dedicated port can be configured from the values of the Helm chart. The CORS configuration has to be temporarily extended with entries including the port. incubator : networking : haproxy : # The port on which Ambassador redirects traffic to HAProxy to test it in a side-by-side mode before switching to HAProxy ambassadorPassthroughPort : 31001 addon : contentComposer : corsOrigin : https://<hostname>:31001 designStudio : corsOrigin : https://<hostname>:31001 digitalAssetManagement : corsOrigin : https://<hostname>:31001 imageProcessor : corsOrigin : https://<hostname>:31001 ringApi : corsOrigin : https://<hostname>:31001","title":"HAProxy Testing (deployment verification)"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#websphere-configuration-setting-for-haproxy-dedicated-configurable-port","text":"The virtual host needs to be configured for the HAProxy port, into the WebSphere server for side-by-side mode. This is used to identify the request made from the external host with that dedicated port for HAProxy. Here, for the example, the default port 31001 is configured as a dedicated port for the HAProxy. Refer to the screenshots below to configure the dedicated port in the virtual host of the WebSphere server. Open WebSphere console and navigate to Virtual Hosts, then click on default_host (or your preferred host) Click on Host Aliases to open list of ports Click on New button to add new port Enter the port in the text field, then click Apply to apply the changes Click on Save After adding port to the virtual host, it can be viewed in the list For the changes to take effect, the WebSphere_Portal server must be restarted. To do so, recycle the Core Pods using the following command. Make sure to replace <release-name> with the Helm release name chosen during deployment and to insert the correct <namespace> : kubectl -n <namespace> rollout restart statefulset.apps/<release-name>-core After the above changes are made, you can append the defined dedicated port into the request URL to test DX using HAProxy. The request URL should look like below (here, 31001 is used for an example). The normal request to access the portal: https://<host-name>/wps/myportal Custom request with port 31001 : https://<host-name>:31001/wps/myportal If DX is rendering as expected using the dedicated port, HAProxy does serve the requests as expected.","title":"WebSphere Configuration Setting (for HAProxy dedicated configurable port)"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#test-applications-on-haproxy-port","text":"Navigate to the Application menu. The Pages present in the menu are using Ambassador by default. The goal is to create a new, temporary page which points to HAProxy port for testing. To do so navigate to Administration Open Administration within the Application Menu Within that, navigate to Pages . Traverse through Content Root and Practitioner Studio to view the Application menu contents. Open Pages within the Administration Page Open Content Root Open Practitioner Studio within Content root Here you need to find and copy the page which you want to test with HAProxy. Using the New Page option a new page within the Application Menu can be created. The properties set for the page which needs to be recreated with HAProxy can be copied from the existing one and entered within the new page. Duplicate the page which needs to be tested with HAProxy The properties are defined here and can be modified to specify the HAProxy changes. If any Page properties or other options are specified, you can copy those as well to the new HAProxy specific page. Properties of the page which needs to be tested with HAProxy Newly created HAProxy Page Once the details are copied, the new page can be seen within the pages section. Next the contents to be displayed within the page needs to be defined. For this a Portlet is added and used to display the contents of the page and then save it. Search for the Portlet to be added Once the Portlet is added, the shared property values need to be updated with the HAProxy port number. Open Edit Shared Setting of the new Portlet added The application ID remains the same ( medialibrary in this example) while the URL is updated such that it calls the HAProxy port Adding HAProxy port to the URL After adding the content, both the original Ambassador page is running on its default URL, while the new page is also available in its respective port URL using HAProxy. Ambassador Page: Newly created HAProxy Page:","title":"Test Applications on HAProxy Port"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#enabling-haproxy-port-in-web-content-image","text":"Another functionality to test if Ambassador as well as HAProxy are running is to use the Image Picker . Traverse through the Web Content to open and edit or create a page. Sample Articles within Web Content View the source code and add an image tag. Edit the source code and add Image Tags In the pop up opened, we can decide which source we want to include the image from, in this scenario it would be selected withing the DAM source. Select the source as DAM and Insert Image On clicking Select Image another pop up is displayed. This will always point to the default application (without port specified) and cannot be configured. The image is selected and added to the source code Insert Image from DAM In the source code the image is added via the img tag, since this was selected from the DAM pointing to Ambassador the port specified is the default port. For testing we can add another image tag an update the port value to the new HAProxy port. Image fetched from default DAM and DAM of HAProxy port After saving the changes, both Ambassador as well the HAProxy fetched images should be visible.","title":"Enabling HAProxy port in Web Content (Image)"},{"location":"platform/kubernetes/haproxy-migration/haproxy-migration/#disable-ambassador","text":"After HAProxy is verified and deployed into the cluster, Ambassador can be disabled such that only HAProxy is running and handles the requests directly. After disabling Ambassador, the ambassadorPassthroughPort ( 31001 ) won't be available anymore. To do so update the value file with Ambassador application flag to false and upgrade the helm chart. applications : # Setting ambassador flag to false ambassador : false # Setting HAProxy flag to true haproxy : true After disabling Ambassador, the Ambassador pods and services are removed and only HAProxy is up and running. All the requests will be handled by HAProxy directly. To verify that Ambassador pods and services are terminated from the namespace, run the following command. kubectl -n <namespace> get all --selector 'app in (<release-name>-haproxy, <release-name>-ambassador)' Make sure that this command shows only the HAProxy instances, and not the Ambassador instances (pods as well as services) because they are already terminated from the namespace. The output should look as follows. NAME READY STATUS RESTARTS AGE pod/dx-deployment-haproxy-6b464bd866-p777b 1/1 Running 0 39m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dx-deployment-haproxy LoadBalancer 10.101.191.20 10.134.209.53 443:30318/TCP,9043:30521/TCP,1636:31777/TCP 39m NAME DESIRED CURRENT READY AGE replicaset.apps/dx-deployment-haproxy-6b464bd866 1 1 1 39m","title":"Disable Ambassador"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/","text":"Remove Ambassador CRDs Notes: This section applies only to the existing HCL Digital Experience deployments. For fresh deployments, refer to the Fresh Installations using HAProxy topic. Even after the migration to HAProxy is completed and the Ambassador is not deployed anymore, the Ambassador CustomResourceDefinitions will still be present in your cluster. Helm is not uninstalling them automatically because they are considered cluster-wide resources. The following steps can be used to check if the CRDs are still in use in the cluster, and give guidance on how to remove them manually if they are found unused. The following 19 Ambassador CRDs were deployed as part of DX deployment: filters.getambassador.io filterpolicies.getambassador.io authservices.getambassador.io consulresolvers.getambassador.io devportals.getambassador.io hosts.getambassador.io kubernetesendpointresolvers.getambassador.io kubernetesserviceresolvers.getambassador.io logservices.getambassador.io mappings.getambassador.io modules.getambassador.io ratelimitservices.getambassador.io tcpmappings.getambassador.io tlscontexts.getambassador.io tracingservices.getambassador.io projects.getambassador.io projectcontrollers.getambassador.io projectrevisions.getambassador.io ratelimits.getambassador.io Check the clusters CustomResourceDefinitions To check if the definitions are on the cluster, run the following command: kubectl get crd --selector app.kubernetes.io/name = ambassador This should return the 19 CRDs listed previously. If more CRDs are returned, it indicates that another Ambassador instance is deployed in your cluster. This needs to be taken into consideration when removing the CRDs Check the clusters CustomResources To see the actual resources that are using the CRDs above, run the following command: kubectl get ambassador-crds --all-namespaces If HCL Digital Experience is migrated to HAProxy and Ambassador is disabled, then the previous command should return the following message: No resources found Remove the CustomResourceDefinitions If no resources are using CRDs , it is safe to remove the resources from the cluster by running the following command: kubectl delete crd \\ filters.getambassador.io \\ filterpolicies.getambassador.io \\ authservices.getambassador.io \\ consulresolvers.getambassador.io \\ devportals.getambassador.io \\ hosts.getambassador.io \\ kubernetesendpointresolvers.getambassador.io \\ kubernetesserviceresolvers.getambassador.io \\ logservices.getambassador.io \\ mappings.getambassador.io \\ modules.getambassador.io \\ ratelimitservices.getambassador.io \\ tcpmappings.getambassador.io \\ tlscontexts.getambassador.io \\ tracingservices.getambassador.io \\ projects.getambassador.io \\ projectcontrollers.getambassador.io \\ projectrevisions.getambassador.io \\ ratelimits.getambassador.io Restore CustomResourceDefinitions as fallback In case you want the deleted CRDs back, you can restore them from the HCL Digital Experience Helm chart. To restore CRDs, unpack the CRDs and apply them by running the following command: tar zxvf hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz hcl-dx-deployment/crds kubectl apply -f ./hcl-dx-deployment/crds","title":"Remove Ambassador CRDs"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/#remove-ambassador-crds","text":"Notes: This section applies only to the existing HCL Digital Experience deployments. For fresh deployments, refer to the Fresh Installations using HAProxy topic. Even after the migration to HAProxy is completed and the Ambassador is not deployed anymore, the Ambassador CustomResourceDefinitions will still be present in your cluster. Helm is not uninstalling them automatically because they are considered cluster-wide resources. The following steps can be used to check if the CRDs are still in use in the cluster, and give guidance on how to remove them manually if they are found unused. The following 19 Ambassador CRDs were deployed as part of DX deployment: filters.getambassador.io filterpolicies.getambassador.io authservices.getambassador.io consulresolvers.getambassador.io devportals.getambassador.io hosts.getambassador.io kubernetesendpointresolvers.getambassador.io kubernetesserviceresolvers.getambassador.io logservices.getambassador.io mappings.getambassador.io modules.getambassador.io ratelimitservices.getambassador.io tcpmappings.getambassador.io tlscontexts.getambassador.io tracingservices.getambassador.io projects.getambassador.io projectcontrollers.getambassador.io projectrevisions.getambassador.io ratelimits.getambassador.io","title":"Remove Ambassador CRDs"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/#check-the-clusters-customresourcedefinitions","text":"To check if the definitions are on the cluster, run the following command: kubectl get crd --selector app.kubernetes.io/name = ambassador This should return the 19 CRDs listed previously. If more CRDs are returned, it indicates that another Ambassador instance is deployed in your cluster. This needs to be taken into consideration when removing the CRDs","title":"Check the clusters CustomResourceDefinitions"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/#check-the-clusters-customresources","text":"To see the actual resources that are using the CRDs above, run the following command: kubectl get ambassador-crds --all-namespaces If HCL Digital Experience is migrated to HAProxy and Ambassador is disabled, then the previous command should return the following message: No resources found","title":"Check the clusters CustomResources"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/#remove-the-customresourcedefinitions","text":"If no resources are using CRDs , it is safe to remove the resources from the cluster by running the following command: kubectl delete crd \\ filters.getambassador.io \\ filterpolicies.getambassador.io \\ authservices.getambassador.io \\ consulresolvers.getambassador.io \\ devportals.getambassador.io \\ hosts.getambassador.io \\ kubernetesendpointresolvers.getambassador.io \\ kubernetesserviceresolvers.getambassador.io \\ logservices.getambassador.io \\ mappings.getambassador.io \\ modules.getambassador.io \\ ratelimitservices.getambassador.io \\ tcpmappings.getambassador.io \\ tlscontexts.getambassador.io \\ tracingservices.getambassador.io \\ projects.getambassador.io \\ projectcontrollers.getambassador.io \\ projectrevisions.getambassador.io \\ ratelimits.getambassador.io","title":"Remove the CustomResourceDefinitions"},{"location":"platform/kubernetes/haproxy-migration/haproxy-remove-ambassador/#restore-customresourcedefinitions-as-fallback","text":"In case you want the deleted CRDs back, you can restore them from the HCL Digital Experience Helm chart. To restore CRDs, unpack the CRDs and apply them by running the following command: tar zxvf hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz hcl-dx-deployment/crds kubectl apply -f ./hcl-dx-deployment/crds","title":"Restore  CustomResourceDefinitions as fallback"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/","text":"Back up and restore DAM This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed. Backup Prerequisites Verify that core , persistence-node and digital-asset-management pods are up and in Running state. kubectl -n <namespace> get pods Example kubectl -n dxns get pods You may see more than one persistence-node pods running: dx-deployment-core-0 3/3 Running 0 3h49m dx-deployment-persistence-node-0 2/2 Running 0 3h49m dx-deployment-persistence-node-1 2/2 Running 0 3h48m dx-deployment-persistence-node-2 2/2 Running 0 3h48m dx-deployment-digital-asset-management-0 1/1 Running 0 3h48m Backup Core references for DAM Export DAM collection references from Core: kubectl -n <namespace> exec pod/<release-name>-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user <wpsadmin-user> -password <wpsadmin-password> -url http://localhost:10039/wps/config -in /opt/HCL/PortalServer/doc/xml-samples/ExportAllDAMCollections.xml -out /path/to/export/to/damExport.xml\" Example kubectl -n dxns exec pod/dx-deployment-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user wpsadmin -password wpsadmin -url http://localhost:10039/wps/config -in /opt/HCL/PortalServer/doc/xml-samples/ExportAllDAMCollections.xml -out /tmp/damExport.xml\" Download the collection dump to the local system: kubectl cp -c core <namespace>/<release-name>-core-0:/path/to/export/to/damExport.xml <target-file> Example kubectl cp -c core dxns/dx-deployment-core-0:/tmp/damExport.xml /tmp/damExport.xml Backup Persistence Determine the primary persistence-node using the following command: kubectl -n <namespace> exec pod/<release-name>-persistence-node-<running-node-index> -c persistence-node -- repmgr cluster show --compact --terse 2>/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs For <running-node-index> , select the index of any persistence-node that is in the Running state. In most cases, node 0 can be used. Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- repmgr cluster show --compact --terse 2 >/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs This command returns the name of the primary persistence-node . Please use this node for the following steps when <primary-node-name> is referenced. Example output: dx-deployment-persistence-node-0 Dump the current database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"pg_dump dxmediadb > /path/to/export/to/dxmediadb.dmp\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"pg_dump dxmediadb > /tmp/dxmediadb.dmp\" Download the database dump to the local system: kubectl cp -c persistence-node <namespace>/<primary-node-name>:/path/to/export/to/dxmediadb.dmp <target-file> Example kubectl cp -c persistence-node dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Backup DAM binaries Compress the DAM binaries located in the /opt/app/upload directory: kubectl -n <namespace> exec pod/<dam-pod-name> -- /bin/bash -c \"tar -cvpzf /path/to/backupml.tar.gz --one-file-system --directory /opt/app/upload .\" Example kubectl -n dxns exec pod/dx-deployment-digital-asset-management-0 -- /bin/bash -c \"tar -cvpzf /tmp/backupml.tar.gz --one-file-system --directory /opt/app/upload .\" Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<dam-pod-name>:<source-file> <target-file> Example kubectl cp dxns/dx-deployment-digital-asset-management-0:/tmp/backupml.tar.gz /tmp/backupml.tar.gz Restore Prerequisites Verify that core , persistence-node and digital-asset-management pods are up and in Running state. kubectl -n <namespace> get pods Example kubectl -n dxns get pods You may see more than one persistence-node pods running: dx-deployment-core-0 3/3 Running 0 3h49m dx-deployment-persistence-node-0 2/2 Running 0 3h49m dx-deployment-persistence-node-1 2/2 Running 0 3h48m dx-deployment-persistence-node-2 2/2 Running 0 3h48m dx-deployment-digital-asset-management-0 1/1 Running 0 3h48m Restore Core references for DAM Upload the collection dump to the core pod: kubectl cp -c core <source-file> <namespace>/<release-name>-core-0:/path/to/damExport.xml Example kubectl cp -c core /tmp/damExport.xml dxns/dx-deployment-core-0:/tmp/damExport.xml Import DAM collection references to Core kubectl -n <namespace> exec pod/<release-name>-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user <wpsadmin-user> -password <wpsadmin-password> -url http://localhost:10039/wps/config -in /path/to/damExport.xml\" Example kubectl -n dxns exec pod/dx-deployment-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user wpsadmin -password wpsadmin -url http://localhost:10039/wps/config -in /tmp/damExport.xml\" Restore DAM binary Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<dam-pod-name>:<target-file> Example kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-digital-asset-management-0:/tmp/backupml.tar.gz Restore the DAM binaries: kubectl -n <namespace> exec pod/<dam-pod-name> -- /bin/bash -c \"tar -mpxf /path/to/backupml.tar.gz --directory /opt/app/upload\" Example kubectl -n dxns exec pod/dx-deployment-digital-asset-management-0 -- /bin/bash -c \"tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload\" Restore Persistence Determine the primary persistence-node using: kubectl -n <namespace> exec pod/<release-name>-persistence-node-<running-node-index> -c persistence-node -- repmgr cluster show --compact --terse 2>/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs For <running-node-index> select the index of any persistence-node that is in the Running state. In most cases, node 0 can be used. Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- repmgr cluster show --compact --terse 2 >/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs This command returns the name of the primary persistence-node . Please use this node for the following steps when <primary-node-name> is referenced. Example output: dx-deployment-persistence-node-0 Copy the database dump file to the primary persistence-node pod: kubectl cp -c persistence-node <target-file> <namespace>/<primary-node-name>:<target-file> Example kubectl cp -c persistence-node /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Run the following commands in order: Set the database connection limit to 0 for dxmediadb : kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- dropdb dxmediadb Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- dropdb dxmediadb Note If you are getting the following error, run the commands from this step again until they are completed without errors. dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"createdb -O dxuser dxmediadb\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"createdb -O dxuser dxmediadb\" Restore the database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"psql dxmediadb < /tmp/dxmediadb.dmp\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"psql dxmediadb < /tmp/dxmediadb.dmp\" Restore the database connection limit: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Additional step to restore your database After the steps above are completed it can take some time for the Persistence connections pool as well as DAM to restart and get back to a Running state. If DAM does not recover on its own, use the following workaround: Delete the DAM pod to restart it: kubectl delete pod <dam-pod-name> -n <namespace> Example kubectl delete pod dx-deployment-digital-asset-management-0 -n dxns","title":"Back up and restore DAM"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#back-up-and-restore-dam","text":"This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed.","title":"Back up and restore DAM"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#backup","text":"","title":"Backup"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#prerequisites","text":"Verify that core , persistence-node and digital-asset-management pods are up and in Running state. kubectl -n <namespace> get pods Example kubectl -n dxns get pods You may see more than one persistence-node pods running: dx-deployment-core-0 3/3 Running 0 3h49m dx-deployment-persistence-node-0 2/2 Running 0 3h49m dx-deployment-persistence-node-1 2/2 Running 0 3h48m dx-deployment-persistence-node-2 2/2 Running 0 3h48m dx-deployment-digital-asset-management-0 1/1 Running 0 3h48m","title":"Prerequisites"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#backup-core-references-for-dam","text":"Export DAM collection references from Core: kubectl -n <namespace> exec pod/<release-name>-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user <wpsadmin-user> -password <wpsadmin-password> -url http://localhost:10039/wps/config -in /opt/HCL/PortalServer/doc/xml-samples/ExportAllDAMCollections.xml -out /path/to/export/to/damExport.xml\" Example kubectl -n dxns exec pod/dx-deployment-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user wpsadmin -password wpsadmin -url http://localhost:10039/wps/config -in /opt/HCL/PortalServer/doc/xml-samples/ExportAllDAMCollections.xml -out /tmp/damExport.xml\" Download the collection dump to the local system: kubectl cp -c core <namespace>/<release-name>-core-0:/path/to/export/to/damExport.xml <target-file> Example kubectl cp -c core dxns/dx-deployment-core-0:/tmp/damExport.xml /tmp/damExport.xml","title":"Backup Core references for DAM"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#backup-persistence","text":"Determine the primary persistence-node using the following command: kubectl -n <namespace> exec pod/<release-name>-persistence-node-<running-node-index> -c persistence-node -- repmgr cluster show --compact --terse 2>/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs For <running-node-index> , select the index of any persistence-node that is in the Running state. In most cases, node 0 can be used. Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- repmgr cluster show --compact --terse 2 >/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs This command returns the name of the primary persistence-node . Please use this node for the following steps when <primary-node-name> is referenced. Example output: dx-deployment-persistence-node-0 Dump the current database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"pg_dump dxmediadb > /path/to/export/to/dxmediadb.dmp\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"pg_dump dxmediadb > /tmp/dxmediadb.dmp\" Download the database dump to the local system: kubectl cp -c persistence-node <namespace>/<primary-node-name>:/path/to/export/to/dxmediadb.dmp <target-file> Example kubectl cp -c persistence-node dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp","title":"Backup Persistence"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#backup-dam-binaries","text":"Compress the DAM binaries located in the /opt/app/upload directory: kubectl -n <namespace> exec pod/<dam-pod-name> -- /bin/bash -c \"tar -cvpzf /path/to/backupml.tar.gz --one-file-system --directory /opt/app/upload .\" Example kubectl -n dxns exec pod/dx-deployment-digital-asset-management-0 -- /bin/bash -c \"tar -cvpzf /tmp/backupml.tar.gz --one-file-system --directory /opt/app/upload .\" Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<dam-pod-name>:<source-file> <target-file> Example kubectl cp dxns/dx-deployment-digital-asset-management-0:/tmp/backupml.tar.gz /tmp/backupml.tar.gz","title":"Backup DAM binaries"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#restore","text":"","title":"Restore"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#prerequisites_1","text":"Verify that core , persistence-node and digital-asset-management pods are up and in Running state. kubectl -n <namespace> get pods Example kubectl -n dxns get pods You may see more than one persistence-node pods running: dx-deployment-core-0 3/3 Running 0 3h49m dx-deployment-persistence-node-0 2/2 Running 0 3h49m dx-deployment-persistence-node-1 2/2 Running 0 3h48m dx-deployment-persistence-node-2 2/2 Running 0 3h48m dx-deployment-digital-asset-management-0 1/1 Running 0 3h48m","title":"Prerequisites"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#restore-core-references-for-dam","text":"Upload the collection dump to the core pod: kubectl cp -c core <source-file> <namespace>/<release-name>-core-0:/path/to/damExport.xml Example kubectl cp -c core /tmp/damExport.xml dxns/dx-deployment-core-0:/tmp/damExport.xml Import DAM collection references to Core kubectl -n <namespace> exec pod/<release-name>-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user <wpsadmin-user> -password <wpsadmin-password> -url http://localhost:10039/wps/config -in /path/to/damExport.xml\" Example kubectl -n dxns exec pod/dx-deployment-core-0 -c core -- /bin/bash -c \"/opt/HCL/PortalServer/bin/xmlaccess.sh -user wpsadmin -password wpsadmin -url http://localhost:10039/wps/config -in /tmp/damExport.xml\"","title":"Restore Core references for DAM"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#restore-dam-binary","text":"Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<dam-pod-name>:<target-file> Example kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-digital-asset-management-0:/tmp/backupml.tar.gz Restore the DAM binaries: kubectl -n <namespace> exec pod/<dam-pod-name> -- /bin/bash -c \"tar -mpxf /path/to/backupml.tar.gz --directory /opt/app/upload\" Example kubectl -n dxns exec pod/dx-deployment-digital-asset-management-0 -- /bin/bash -c \"tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload\"","title":"Restore DAM binary"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#restore-persistence","text":"Determine the primary persistence-node using: kubectl -n <namespace> exec pod/<release-name>-persistence-node-<running-node-index> -c persistence-node -- repmgr cluster show --compact --terse 2>/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs For <running-node-index> select the index of any persistence-node that is in the Running state. In most cases, node 0 can be used. Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- repmgr cluster show --compact --terse 2 >/dev/null | grep \"primary\" | awk '{split($0,a,\"|\"); print a[2]}' | xargs This command returns the name of the primary persistence-node . Please use this node for the following steps when <primary-node-name> is referenced. Example output: dx-deployment-persistence-node-0 Copy the database dump file to the primary persistence-node pod: kubectl cp -c persistence-node <target-file> <namespace>/<primary-node-name>:<target-file> Example kubectl cp -c persistence-node /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Run the following commands in order: Set the database connection limit to 0 for dxmediadb : kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- dropdb dxmediadb Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- dropdb dxmediadb Note If you are getting the following error, run the commands from this step again until they are completed without errors. dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"createdb -O dxuser dxmediadb\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"createdb -O dxuser dxmediadb\" Restore the database: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- /bin/bash -c \"psql dxmediadb < /tmp/dxmediadb.dmp\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- /bin/bash -c \"psql dxmediadb < /tmp/dxmediadb.dmp\" Restore the database connection limit: kubectl -n <namespace> exec pod/<primary-node-name> -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Example kubectl -n dxns exec pod/dx-deployment-persistence-node-0 -c persistence-node -- psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\"","title":"Restore Persistence"},{"location":"platform/kubernetes/operations/helm_dam_backup_restore/#additional-step-to-restore-your-database","text":"After the steps above are completed it can take some time for the Persistence connections pool as well as DAM to restart and get back to a Running state. If DAM does not recover on its own, use the following workaround: Delete the DAM pod to restart it: kubectl delete pod <dam-pod-name> -n <namespace> Example kubectl delete pod dx-deployment-digital-asset-management-0 -n dxns","title":"Additional step to restore your database"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/","text":"Configure Remote Search This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments. Note Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. Introduction Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL. Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages. EJBs and host names HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs. Defining serverindex.xml on the Remote Search server When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned. Remote Search services configuration The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Configure WCM Authoring Portlet search function Note Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Additional Routing Configuration for supported Kubernetes platforms To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console","title":"Configure Remote Search"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#configure-remote-search","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments. Note Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms.","title":"Configure Remote Search"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#introduction","text":"Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL.","title":"Introduction"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#deploying-remote-search-in-hcl-digital-experience-95-openshift-and-kubernetes-platforms","text":"Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages.","title":"Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#ejbs-and-host-names","text":"HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs.","title":"EJBs and host names"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#defining-serverindexxml-on-the-remote-search-server","text":"When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned.","title":"Defining serverindex.xml on the Remote Search server"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#remote-search-services-configuration","text":"The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1","title":"Remote Search services configuration"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#jcr-content-source-configuration","text":"Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm","title":"JCR Content Source Configuration"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#configure-wcm-authoring-portlet-search-function","text":"Note Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path.","title":"Configure WCM Authoring Portlet search function"},{"location":"platform/kubernetes/operations/kubernetes_remote_search/#additional-routing-configuration-for-supported-kubernetes-platforms","text":"To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console","title":"Additional Routing Configuration for supported Kubernetes platforms"},{"location":"platform/kubernetes/operations/run_core_config_engine/","text":"Running DX Core configuration tasks This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments. Running Core Config Engine tasks In an HCL Digital Experience 9.5 Container deployment using Helm , some DX 9.5 Core configuration tasks (such as change of context root) are now performed using the Helm upgrade route. However, others (such as database migration) are performed using a Config Engine task, as they would be in an on-premise or hybrid environment. Steps to run a Config Engine task: Open a shell on a DX Core pod. The following example instructions uses pod \u20180\u2019 as it should always be available: kubectl exec -it -n < namespace > dx-deployment-core-0 -c core -- /bin/bash Create the semaphore file. On the Core pod, use the following command to create a file which tells the Kubernetes probes that a Configuration task is in progress: touch /opt/app/configInProgress Run the Config Engine command. Follow the instructions for the particular configuration task that you need to perform. See DB Transfer Config Engine task for an example. Remove the semaphore file. On the Core pod, use the following command to delete the file which tells the Kubernetes probes that a configuration task is in progress: rm -f /opt/app/configInProgress You can now close the shell on the Core pod: exit (Optional) Restart other Core pods. If you have multiple Core pods running, and if the configuration task you just performed requires a server restart, you should now restart all the Core pods other than the one on which you ran the task. To do this, run the following command for each other Core pod. For example, if you have Core pods dx-deployment-core-0 , dx-deployment-core-1 , and dx-deployment-core-2 and performed the configuration task on pod 0, then run the following command below for pods dx-deployment-core-1 and dx-deployment-core-2 : kubectl delete pod -n < namespace > < pod-name > Note To reduce the impact on availability, it is recommended that you wait for a pod to be ready again before running the command for the next pod.","title":"Running DX Core configuration tasks"},{"location":"platform/kubernetes/operations/run_core_config_engine/#running-dx-core-configuration-tasks","text":"This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments.","title":"Running DX Core configuration tasks"},{"location":"platform/kubernetes/operations/run_core_config_engine/#running-core-config-engine-tasks","text":"In an HCL Digital Experience 9.5 Container deployment using Helm , some DX 9.5 Core configuration tasks (such as change of context root) are now performed using the Helm upgrade route. However, others (such as database migration) are performed using a Config Engine task, as they would be in an on-premise or hybrid environment. Steps to run a Config Engine task: Open a shell on a DX Core pod. The following example instructions uses pod \u20180\u2019 as it should always be available: kubectl exec -it -n < namespace > dx-deployment-core-0 -c core -- /bin/bash Create the semaphore file. On the Core pod, use the following command to create a file which tells the Kubernetes probes that a Configuration task is in progress: touch /opt/app/configInProgress Run the Config Engine command. Follow the instructions for the particular configuration task that you need to perform. See DB Transfer Config Engine task for an example. Remove the semaphore file. On the Core pod, use the following command to delete the file which tells the Kubernetes probes that a configuration task is in progress: rm -f /opt/app/configInProgress You can now close the shell on the Core pod: exit (Optional) Restart other Core pods. If you have multiple Core pods running, and if the configuration task you just performed requires a server restart, you should now restart all the Core pods other than the one on which you ran the task. To do this, run the following command for each other Core pod. For example, if you have Core pods dx-deployment-core-0 , dx-deployment-core-1 , and dx-deployment-core-2 and performed the configuration task on pod 0, then run the following command below for pods dx-deployment-core-1 and dx-deployment-core-2 : kubectl delete pod -n < namespace > < pod-name > Note To reduce the impact on availability, it is recommended that you wait for a pod to be ready again before running the command for the next pod.","title":"Running Core Config Engine tasks"},{"location":"platform/kubernetes/operations/update_helm_deployment/","text":"Upgrade the Helm deployment to the latest version This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. This section assumes that you prepared your cluster and your custom-values.yaml file, using guidance provided in the Planning your HCL DX 9.5 container deployment using Helm topic, and then installed your deployment using the instructions in the Install topic. Overview of Helm Configuration Updates Once an HCL Digital Experience Kubernetes 9.5 deployment is installed, it is possible to update its configuration directly using the standard Kubernetes or OpenShift commands (for example, by updating values in the various config maps). However, this is NOT the recommended approach. Some of the configuration parameters have interdependencies, as outlined in the Planning section . These require knowledgeable management to make changes that are compatible with interdependency requirements. For example, if you change the context root for DX Core you also need to change the readiness and liveness probes. The recommended approach for configuration changes is to update the custom-values.yaml file used to install the deployment, and then run a Helm upgrade. This has the added benefit that your custom-values.yaml file remains an up-to-date description of the configuration of your environment. Helm Upgrade configuration command After making the needed changes to your custom-values.yaml file, use the following command: # Helm upgrade command helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you have updated. The path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience Helm Chart that you extracted in the preparation steps.","title":"Upgrade the Helm deployment to the latest version"},{"location":"platform/kubernetes/operations/update_helm_deployment/#upgrade-the-helm-deployment-to-the-latest-version","text":"This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. This section assumes that you prepared your cluster and your custom-values.yaml file, using guidance provided in the Planning your HCL DX 9.5 container deployment using Helm topic, and then installed your deployment using the instructions in the Install topic. Overview of Helm Configuration Updates Once an HCL Digital Experience Kubernetes 9.5 deployment is installed, it is possible to update its configuration directly using the standard Kubernetes or OpenShift commands (for example, by updating values in the various config maps). However, this is NOT the recommended approach. Some of the configuration parameters have interdependencies, as outlined in the Planning section . These require knowledgeable management to make changes that are compatible with interdependency requirements. For example, if you change the context root for DX Core you also need to change the readiness and liveness probes. The recommended approach for configuration changes is to update the custom-values.yaml file used to install the deployment, and then run a Helm upgrade. This has the added benefit that your custom-values.yaml file remains an up-to-date description of the configuration of your environment. Helm Upgrade configuration command After making the needed changes to your custom-values.yaml file, use the following command: # Helm upgrade command helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you have updated. The path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience Helm Chart that you extracted in the preparation steps.","title":"Upgrade the Helm deployment to the latest version"},{"location":"platform/kubernetes/operations/monitoring/basic_monitor_helm_deployment/","text":"Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitoring Requirements Video: Understanding the Liveness and Readiness Probes for HCL DX 9.5 Container Helm Deployments To use the monitoring commands described below, the Kubernetes Metrics Server must be installed, configured and running. For information on how to do this, please see the Kubernetes documentation . Monitoring commands With the Metrics Server installed, standard kubectl top commands can be used to monitor Digital Experience 9.5 components or the nodes on which they are installed. Example To get memory and CPU usage details for the pods in your DX deployment: ``` kubectl top pod -n your-namespace -l release=your-release-name ``` In the above example `your-namespace` is the namespace in which your HCL Digital Experience 9.5 deployment is installed and `your-release-name` is the Helm release name you used when installing. To get memory and CPU usage details for the current Kubernetes node: ``` kubectl top node ```","title":"Basic monitoring"},{"location":"platform/kubernetes/operations/monitoring/basic_monitor_helm_deployment/#basic-monitoring","text":"This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm.","title":"Basic monitoring"},{"location":"platform/kubernetes/operations/monitoring/basic_monitor_helm_deployment/#monitoring","text":"Requirements Video: Understanding the Liveness and Readiness Probes for HCL DX 9.5 Container Helm Deployments To use the monitoring commands described below, the Kubernetes Metrics Server must be installed, configured and running. For information on how to do this, please see the Kubernetes documentation . Monitoring commands With the Metrics Server installed, standard kubectl top commands can be used to monitor Digital Experience 9.5 components or the nodes on which they are installed. Example To get memory and CPU usage details for the pods in your DX deployment: ``` kubectl top pod -n your-namespace -l release=your-release-name ``` In the above example `your-namespace` is the namespace in which your HCL Digital Experience 9.5 deployment is installed and `your-release-name` is the Helm release name you used when installing. To get memory and CPU usage details for the current Kubernetes node: ``` kubectl top node ```","title":"Monitoring"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/","text":"Monitor the deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments. Prometheus metrics and Grafana The Digital Experience 9.5 Helm deployment supports monitoring the deployment activity with advanced metrics and visualization, by exposing standards-based Prometheus -compatible metrics. Prometheus metrics components can scrape the metrics of most of the DX 9.5 container applications. The collected data is queried from Prometheus and are visualized in operations dashboard solutions, such as Grafana . The following information can advise administrators which Digital Experience 9.5 applications can use these tools with some usage examples. Digital Experience 9.5 applications and Prometheus metrics The following Digital Experience 9.5 applications expose metrics that can be tracked with Prometheus metrics. Application Port Route Core 10038 /metrics Remote Search 9060 /metrics Content Composer 3000 /probe/metrics Design Studio 3000 /probe/metrics Digital Asset Management 3000 /probe/metrics Image Processor 3000 /probe/metrics Ring API 3000 /probe/metrics DAM Persistence 9187 /metrics Ambassador 8877 /metrics HAProxy 8404 /metrics Important HCL Digital Experience 9.5 does not include a deployment of Prometheus or Grafana . The metrics are enabled by default for the DX 9.5 Helm chart . This exposes Prometheus-compatible metrics, which can be consumed by any common Prometheus installation. HCL DX 9.5 metrics are compatible with the following deployment and discovery types of Prometheus in Kubernetes environments: Prometheus - Discovers metrics by evaluating the annotation of the services Prometheus Operator - Discovers metrics using the ServiceMonitor custom resources Administrators can configure the HCL DX 9.5 metrics depending on their specific Prometheus deployment, as outlined in the following sections. Configure Prometheus metrics Metrics for the Digital Experience 9.5 applications in the DX 9.5 Helm chart are enabled by default, with prometheusDiscoveryType set to annotations . The metrics are configured independently for each DX 9.5 application. The parameter to disable metrics is included in the example configurations. Parameter Description Default value metrics.<application>.scrape Determines if the metrics of this application are scraped by Prometheus. true metrics.<application>.prometheusDiscoveryType Determines how Prometheus discovers the metrics of a service. Accepts \"annotation\" and \"serviceMonitor\" . The \"serviceMonitor\" setting requires that the ServiceMonitor CRD (which comes with the Prometheus Operator), is installed in the cluster. \"annotation\" Example: Default configuration : Metrics are enabled for Core with the appropriate annotation for Prometheus: metrics : core : scrape : true prometheusDiscoveryType : \"annotation\" Create a ServiceMonitor for Prometheus Operator: metrics : core : scrape : true prometheusDiscoveryType : \"serviceMonitor\" Disable the metrics for Core: metrics : core : scrape : false Grafana dashboards The exposed DX 9.5 applications metrics are compatible with a set of existing Grafana operations dashboards that are available from the Grafana dashboard page, as well as a set of Grafana-supported custom dashboards provided in JSON format. See following examples, which can be imported directly into Grafana. Publicly available operations dashboards You can directly download or import the following dashboards from the Grafana community page using the IDs or links. ID Dashboard Applications 14151 WebSphere Application Server PMI metrics dashboard Core, Remote Search 11159 NodeJS application dashboard Content Composer, Design Studio, Digital Asset Management, Image Processor, Experience API 9628 PostgreSQL database DAM Persistence 10850 Ambassador dashboard Ambassador 12693 HAProxy dashboard HAProxy HCL Digital Experience custom dashboards The following dashboards are provided by HCL Software for use with HCL Digital Experience 9.5 deployments. These examples expose custom metrics for DX applications or provide enhanced features for existing dashboards. They are available in the public HCL Software GitHub repository . Dashboard Application(s) dam_dashboard.json Digital Asset Management References to Prometheus and Grafana installations Important The resources outline here are optional deployment examples. HCL Software does not provide direct support for any issues related to the Prometheus metrics or the Grafana visualization tools. To leverage the full potential of the Digital Experience 9.5 applications metrics, an existing Prometheus and Grafana deployment can be used. Following is a list of additional metrics tracking and visualization services (non-exhaustive) that you can consider when developing solutions according to your deployment needs: kube-prometheus-stack Helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus adapter for Kubernetes metrics APIs kube-state-metrics Grafana Note The kube-prometheus-stack Helm chart is based on the kube-prometheus repository, and comes with a set of tools to monitor the Kubernetes cluster, as well as pre-installed Grafana dashboards for visualization. prometheus and grafana are provided as independent Helm charts.","title":"Monitor the deployment using metrics"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#monitor-the-deployment-using-metrics","text":"This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments.","title":"Monitor the deployment using metrics"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#prometheus-metrics-and-grafana","text":"The Digital Experience 9.5 Helm deployment supports monitoring the deployment activity with advanced metrics and visualization, by exposing standards-based Prometheus -compatible metrics. Prometheus metrics components can scrape the metrics of most of the DX 9.5 container applications. The collected data is queried from Prometheus and are visualized in operations dashboard solutions, such as Grafana . The following information can advise administrators which Digital Experience 9.5 applications can use these tools with some usage examples.","title":"Prometheus metrics and Grafana"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#digital-experience-95-applications-and-prometheus-metrics","text":"The following Digital Experience 9.5 applications expose metrics that can be tracked with Prometheus metrics. Application Port Route Core 10038 /metrics Remote Search 9060 /metrics Content Composer 3000 /probe/metrics Design Studio 3000 /probe/metrics Digital Asset Management 3000 /probe/metrics Image Processor 3000 /probe/metrics Ring API 3000 /probe/metrics DAM Persistence 9187 /metrics Ambassador 8877 /metrics HAProxy 8404 /metrics Important HCL Digital Experience 9.5 does not include a deployment of Prometheus or Grafana . The metrics are enabled by default for the DX 9.5 Helm chart . This exposes Prometheus-compatible metrics, which can be consumed by any common Prometheus installation. HCL DX 9.5 metrics are compatible with the following deployment and discovery types of Prometheus in Kubernetes environments: Prometheus - Discovers metrics by evaluating the annotation of the services Prometheus Operator - Discovers metrics using the ServiceMonitor custom resources Administrators can configure the HCL DX 9.5 metrics depending on their specific Prometheus deployment, as outlined in the following sections.","title":"Digital Experience 9.5 applications and Prometheus metrics"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#configure-prometheus-metrics","text":"Metrics for the Digital Experience 9.5 applications in the DX 9.5 Helm chart are enabled by default, with prometheusDiscoveryType set to annotations . The metrics are configured independently for each DX 9.5 application. The parameter to disable metrics is included in the example configurations. Parameter Description Default value metrics.<application>.scrape Determines if the metrics of this application are scraped by Prometheus. true metrics.<application>.prometheusDiscoveryType Determines how Prometheus discovers the metrics of a service. Accepts \"annotation\" and \"serviceMonitor\" . The \"serviceMonitor\" setting requires that the ServiceMonitor CRD (which comes with the Prometheus Operator), is installed in the cluster. \"annotation\" Example: Default configuration : Metrics are enabled for Core with the appropriate annotation for Prometheus: metrics : core : scrape : true prometheusDiscoveryType : \"annotation\" Create a ServiceMonitor for Prometheus Operator: metrics : core : scrape : true prometheusDiscoveryType : \"serviceMonitor\" Disable the metrics for Core: metrics : core : scrape : false","title":"Configure Prometheus metrics"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#grafana-dashboards","text":"The exposed DX 9.5 applications metrics are compatible with a set of existing Grafana operations dashboards that are available from the Grafana dashboard page, as well as a set of Grafana-supported custom dashboards provided in JSON format. See following examples, which can be imported directly into Grafana. Publicly available operations dashboards You can directly download or import the following dashboards from the Grafana community page using the IDs or links. ID Dashboard Applications 14151 WebSphere Application Server PMI metrics dashboard Core, Remote Search 11159 NodeJS application dashboard Content Composer, Design Studio, Digital Asset Management, Image Processor, Experience API 9628 PostgreSQL database DAM Persistence 10850 Ambassador dashboard Ambassador 12693 HAProxy dashboard HAProxy","title":"Grafana dashboards"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#hcl-digital-experience-custom-dashboards","text":"The following dashboards are provided by HCL Software for use with HCL Digital Experience 9.5 deployments. These examples expose custom metrics for DX applications or provide enhanced features for existing dashboards. They are available in the public HCL Software GitHub repository . Dashboard Application(s) dam_dashboard.json Digital Asset Management","title":"HCL Digital Experience custom dashboards"},{"location":"platform/kubernetes/operations/monitoring/monitor_helm_deployment_metrics/#references-to-prometheus-and-grafana-installations","text":"Important The resources outline here are optional deployment examples. HCL Software does not provide direct support for any issues related to the Prometheus metrics or the Grafana visualization tools. To leverage the full potential of the Digital Experience 9.5 applications metrics, an existing Prometheus and Grafana deployment can be used. Following is a list of additional metrics tracking and visualization services (non-exhaustive) that you can consider when developing solutions according to your deployment needs: kube-prometheus-stack Helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus adapter for Kubernetes metrics APIs kube-state-metrics Grafana Note The kube-prometheus-stack Helm chart is based on the kube-prometheus repository, and comes with a set of tools to monitor the Kubernetes cluster, as well as pre-installed Grafana dashboards for visualization. prometheus and grafana are provided as independent Helm charts.","title":"References to Prometheus and Grafana installations"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/","text":"Configure and access logs This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. HCL Digital Experience logs are important for maintaining and troubleshooting both environments and custom applications. These logs frequently form part of the essential information requested by HCL Support to diagnose issues. In a Helm-based deployment of DX, logs are exposed as Kubernetes container logs which give a consistent mechanism for retrieving the logs of different components, as well as making them consumable by cluster-level logging solutions in Kubernetes. Configure logging In CF200, a new mechanism is introduced for configuring log settings at runtime (without pod restarts) in Helm-based DX deployments. Log levels and trace strings are set in your custom-values.yaml file and applied using a helm upgrade command. Under the covers, this sets values in a new <release-name>-global config map which are monitored by the various running DX containers. When the containers detect a change to the values pertinent to themselves, they update their log configurations accordingly (without restarting). At that point, the new log behavior is immediately reflected in their Kubernetes logs. Note OpenLDAP, Ambassador, and Redis are not yet configurable using this feature. Setting the log configuration for a DX application You can set a desired log configuration for a DX application by specifying an appropriate log string in your Helm custom-values.yaml file. Place the log string in the level property for the specified application. These properties are found in the logging subsection of the incubator section. For example, to set the configuration for Content Composer, use the following property: incubator: logging: # Content Composer specific logging configuration contentComposer: level: \"api:server-v1:*=info\" You can see the string format in the following section. Once the property is set, run the helm upgrade command. Log configuration string format Log configuration strings (the values set in the level properties of the custom-values.yaml) use the following common format, where multiple trace settings for the same application are separated by commas: <component>:<pattern>=<log-level>,<component>:<pattern>=<log-level> component - represents a subsystem of the application and must be from a limited list per application (see the following examples). pattern - describes the specific component area to log (for example, a Java package). log-level - defines the granularity at which logging is enabled (see later for permitted levels). The exact format of pattern depends on the configured application. The appropriate values are provided by HCL Support, if you are asked to enable tracing as part of a case. Some examples of log configuration strings for different DX applications are given as follows: DX Core example : wp_profile:com.hcl.App=info,wp_profile:com.hcl.util.Data=finest Digital Asset Management example : api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug Supported application and component names Following are the supported application and component names, where the application names are the subsections under logging in the custom-values.yaml: Application Component names core wp_profile , cw_profile contentComposer api designStudio api digitalAssetManagement api , worker imageProcessor api persistenceConnectionPool pgpool persistenceNode psql , repmgr remoteSearch prs_profile ringApi api runtimeController controller Supported log levels For most applications, three log levels are supported: debug , info , and error . Core and Remote Search, where all existing WebSphere Application Server trace levels are supported, such as all or finest . Accessing Kubernetes container logs Container logs for DX applications can be accessed individually or collectively, as described in the following subsections. Logs for DX Core and Remote Search are accessed differently from other applications, as those pods have multiple containers to provide access to additional logs. Accessing DX Core logs To access a Core application log, use the command: kubectl logs -n <namespace> <core-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-core-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Core log file. Note The additional logging enabled for Core goes to trace.log. To configure trace.log for sidecar logging, see Configure Core sidecar logging . By default, two sidecar containers are launched with Core: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Core sidecar log containers, please see Configure Core sidecar logging . Accessing Remote Search logs To access a Remote Search application log, use the command: kubectl logs -n <namespace> <remote-search-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Remote Search log file. Note The additional logging enabled for Remote Search goes to trace.log. To configure trace.log for sidecar logging, see Configure Remote Search sidecar logging . By default, two sidecar containers are launched with Remote Search: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Remote Search sidecar log containers, please see Configure Remote Search sidecar logging . Accessing logs for other applications Applications other than Core and Remote Search do not have logging sidecar containers and only provide a single log per pod, which can typically be obtained using the command: kubectl logs -n <namespace> <pod-name> (omitting a container name), for example: kubectl logs -n dxns dx-deployment-digital-asset-management-0 This is not the case for Persistence Node pods, which have non-logging sidecar containers (for metrics gathering). For these pods, you must append the main container name ( persistence-node ) when accessing the log, for example: kubectl logs -n dxns dx-deployment-persistence-node-0 persistence-node Accessing all application logs simultaneously All application logs from DX pods in a deployment can be combined into a single output using the command: kubectl logs -n <namespace> -l release=<release-name> --tail=-1 --all-containers where: namespace - is the namespace in which your HCL Digital Experience deployment is installed. release-name - is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command. Default log output The log output for a DX deployment is set to a non-verbose configuration by default. Application name Default log settings Core *=info Content Composer api:server-v1:*=info Design Studio api:server-v1:*=info Digital Asset Management api:server-v1:*=info,worker:server-v1:*=info Image Processor api:server-v1:*=info Persistence Connection Pool pgpool:=info Persistence Node psql:=info,repmgr:=info Remote Search *=info Ring API api:server-v1:*=info Runtime Controller controller:.*=INFO,controller:com.hcl.dx.*=INFO All applications send their log output directly to stdout and stderr of the corresponding container that they are running in. Besides that, the following applications also write their log output into a file that is available in the file system of the containers: Application name Log location Core /opt/HCL/wp_profile/logs/WebSphere_Portal and /opt/HCL/AppServer/profiles/cw_profile/logs/server1 Remote Search /opt/HCL/AppServer/profiles/prs_profile/logs/server1 Persistence Node /var/lib/pgsql/11/data/log and /var/lib/pgsql/11/data/dx/repmgr/log Note The Core and Remote Search have the following default settings for their log output files and that needs to be considered when sizing their persistent volumes: Output file Size per file Files kept SystemOut.log 5MB 3 SystemErr.log 5MB 3 trace.log 20MB 3 The amount of logs that are stored per container in running the Pods depends on the configuration of your Kubernetes Cluster. Refer to the documentation of your cloud provider for further information. Note that for all applications that do not write their logs separately to a file, the only source of historical log data is the Kubernetes logging. We encourage the customers to process the logging of their Kubernetes Cluster in a separate logging solution of their choice.","title":"Configure and access logs"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#configure-and-access-logs","text":"This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. HCL Digital Experience logs are important for maintaining and troubleshooting both environments and custom applications. These logs frequently form part of the essential information requested by HCL Support to diagnose issues. In a Helm-based deployment of DX, logs are exposed as Kubernetes container logs which give a consistent mechanism for retrieving the logs of different components, as well as making them consumable by cluster-level logging solutions in Kubernetes.","title":"Configure and access logs"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#configure-logging","text":"In CF200, a new mechanism is introduced for configuring log settings at runtime (without pod restarts) in Helm-based DX deployments. Log levels and trace strings are set in your custom-values.yaml file and applied using a helm upgrade command. Under the covers, this sets values in a new <release-name>-global config map which are monitored by the various running DX containers. When the containers detect a change to the values pertinent to themselves, they update their log configurations accordingly (without restarting). At that point, the new log behavior is immediately reflected in their Kubernetes logs. Note OpenLDAP, Ambassador, and Redis are not yet configurable using this feature.","title":"Configure logging"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#setting-the-log-configuration-for-a-dx-application","text":"You can set a desired log configuration for a DX application by specifying an appropriate log string in your Helm custom-values.yaml file. Place the log string in the level property for the specified application. These properties are found in the logging subsection of the incubator section. For example, to set the configuration for Content Composer, use the following property: incubator: logging: # Content Composer specific logging configuration contentComposer: level: \"api:server-v1:*=info\" You can see the string format in the following section. Once the property is set, run the helm upgrade command.","title":"Setting the log configuration for a DX application"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#log-configuration-string-format","text":"Log configuration strings (the values set in the level properties of the custom-values.yaml) use the following common format, where multiple trace settings for the same application are separated by commas: <component>:<pattern>=<log-level>,<component>:<pattern>=<log-level> component - represents a subsystem of the application and must be from a limited list per application (see the following examples). pattern - describes the specific component area to log (for example, a Java package). log-level - defines the granularity at which logging is enabled (see later for permitted levels). The exact format of pattern depends on the configured application. The appropriate values are provided by HCL Support, if you are asked to enable tracing as part of a case. Some examples of log configuration strings for different DX applications are given as follows: DX Core example : wp_profile:com.hcl.App=info,wp_profile:com.hcl.util.Data=finest Digital Asset Management example : api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug","title":"Log configuration string format"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#supported-application-and-component-names","text":"Following are the supported application and component names, where the application names are the subsections under logging in the custom-values.yaml: Application Component names core wp_profile , cw_profile contentComposer api designStudio api digitalAssetManagement api , worker imageProcessor api persistenceConnectionPool pgpool persistenceNode psql , repmgr remoteSearch prs_profile ringApi api runtimeController controller","title":"Supported application and component names"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#supported-log-levels","text":"For most applications, three log levels are supported: debug , info , and error . Core and Remote Search, where all existing WebSphere Application Server trace levels are supported, such as all or finest .","title":"Supported log levels"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#accessing-kubernetes-container-logs","text":"Container logs for DX applications can be accessed individually or collectively, as described in the following subsections. Logs for DX Core and Remote Search are accessed differently from other applications, as those pods have multiple containers to provide access to additional logs.","title":"Accessing Kubernetes container logs"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#accessing-dx-core-logs","text":"To access a Core application log, use the command: kubectl logs -n <namespace> <core-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-core-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Core log file. Note The additional logging enabled for Core goes to trace.log. To configure trace.log for sidecar logging, see Configure Core sidecar logging . By default, two sidecar containers are launched with Core: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Core sidecar log containers, please see Configure Core sidecar logging .","title":"Accessing DX Core logs"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#accessing-remote-search-logs","text":"To access a Remote Search application log, use the command: kubectl logs -n <namespace> <remote-search-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Remote Search log file. Note The additional logging enabled for Remote Search goes to trace.log. To configure trace.log for sidecar logging, see Configure Remote Search sidecar logging . By default, two sidecar containers are launched with Remote Search: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Remote Search sidecar log containers, please see Configure Remote Search sidecar logging .","title":"Accessing Remote Search logs"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#accessing-logs-for-other-applications","text":"Applications other than Core and Remote Search do not have logging sidecar containers and only provide a single log per pod, which can typically be obtained using the command: kubectl logs -n <namespace> <pod-name> (omitting a container name), for example: kubectl logs -n dxns dx-deployment-digital-asset-management-0 This is not the case for Persistence Node pods, which have non-logging sidecar containers (for metrics gathering). For these pods, you must append the main container name ( persistence-node ) when accessing the log, for example: kubectl logs -n dxns dx-deployment-persistence-node-0 persistence-node","title":"Accessing logs for other applications"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#accessing-all-application-logs-simultaneously","text":"All application logs from DX pods in a deployment can be combined into a single output using the command: kubectl logs -n <namespace> -l release=<release-name> --tail=-1 --all-containers where: namespace - is the namespace in which your HCL Digital Experience deployment is installed. release-name - is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command.","title":"Accessing all application logs simultaneously"},{"location":"platform/kubernetes/operations/troubleshooting/configure_access_helm_logs/#default-log-output","text":"The log output for a DX deployment is set to a non-verbose configuration by default. Application name Default log settings Core *=info Content Composer api:server-v1:*=info Design Studio api:server-v1:*=info Digital Asset Management api:server-v1:*=info,worker:server-v1:*=info Image Processor api:server-v1:*=info Persistence Connection Pool pgpool:=info Persistence Node psql:=info,repmgr:=info Remote Search *=info Ring API api:server-v1:*=info Runtime Controller controller:.*=INFO,controller:com.hcl.dx.*=INFO All applications send their log output directly to stdout and stderr of the corresponding container that they are running in. Besides that, the following applications also write their log output into a file that is available in the file system of the containers: Application name Log location Core /opt/HCL/wp_profile/logs/WebSphere_Portal and /opt/HCL/AppServer/profiles/cw_profile/logs/server1 Remote Search /opt/HCL/AppServer/profiles/prs_profile/logs/server1 Persistence Node /var/lib/pgsql/11/data/log and /var/lib/pgsql/11/data/dx/repmgr/log Note The Core and Remote Search have the following default settings for their log output files and that needs to be considered when sizing their persistent volumes: Output file Size per file Files kept SystemOut.log 5MB 3 SystemErr.log 5MB 3 trace.log 20MB 3 The amount of logs that are stored per container in running the Pods depends on the configuration of your Kubernetes Cluster. Refer to the documentation of your cloud provider for further information. Note that for all applications that do not write their logs separately to a file, the only source of historical log data is the Kubernetes logging. We encourage the customers to process the logging of their Kubernetes Cluster in a separate logging solution of their choice.","title":"Default log output"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/","text":"Troubleshooting your Helm deployment This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm. Logs Access to the HCL Digital Experience 9.5 component logs is important for maintaining and troubleshooting both your container platform environments and your custom applications. It is also essential for supplying information that may be requested by HCL Software Support . Most component logs can be accessed directly on the Kubernetes or OpenShift platforms via the kubectl logs or OpenShift log access commands. The DX 9.5 Core component has additional important log files that are stored on persistent volumes, and need to be retrieved in a different manner. Accessing DX 9.5 container logs on Kubernetes or OpenShift All container logs from DX 9.5 pods in a deployment with Helm can be combined into a single output using the commands: kubectl logs -n your-namespace -l release=your-release-name --tail=-1 In the example above, the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command. Retrieving additional DX Core logs Useful additional DX Core logs are stored on persistent volumes. To retrieve these, repeat the command below for each DX Core pod: kubectl cp -n your-namespace pod-name:opt/HCL/wp_profile/logs/WebSphere_Portal/ . In the example above the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and pod-name is the particular DX Core pod from which you wish to retrieve logs. Events Pod events can give useful information for troubleshooting, such as why certain pods are not running. To get the events for a pod, you can use the following command: kubectl describe pod -n your-namespace pod-name In the above example, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and pod-name is the particular pod that you wish to examine. PersistentVolumeClaims (PVCs) One of the more common reasons for a pod not starting that can be identified via the pod events (see above) is that it has unbound persistent volume claims. To understand why the claims have not been fulfilled, it is useful to list both the current persistent volume claims and the current persistent volumes and to compare the two. When comparing, it is useful to check mismatches in storage class, access modes and capacity between available volumes and unfulfilled claims. The commands to list these resources are: kubectl get pvc -n your-namespace kubectl get pv In the above example, your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed. Retrieving the deployment configuration In addition to logs, HCL Support may also request configuration information about your deployment. This is can be obtained using kubectl describe commands for different classes of objects and a selector to get all for your deployment. The most likely object types are given in the examples below: kubectl describe pods -n your-namespace -l release=your-release-name kubectl describe deployments -n your-namespace -l release=your-release-name kubectl describe statefulsets -n your-namespace -l release=your-release-name kubectl describe secrets -n your-namespace -l release=your-release-name kubectl describe services -n your-namespace -l release=your-release-name kubectl describe mappings -n your-namespace -l release=your-release-name kubectl describe configmaps -n your-namespace -l release=your-release-name In the above examples, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file by appending > some-file-name to any command. Configure and access logs in Helm This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitor the Digital Experience deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments.","title":"Troubleshooting your Helm deployment"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#troubleshooting-your-helm-deployment","text":"This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm.","title":"Troubleshooting your Helm deployment"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#logs","text":"Access to the HCL Digital Experience 9.5 component logs is important for maintaining and troubleshooting both your container platform environments and your custom applications. It is also essential for supplying information that may be requested by HCL Software Support . Most component logs can be accessed directly on the Kubernetes or OpenShift platforms via the kubectl logs or OpenShift log access commands. The DX 9.5 Core component has additional important log files that are stored on persistent volumes, and need to be retrieved in a different manner.","title":"Logs"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#accessing-dx-95-container-logs-on-kubernetes-or-openshift","text":"All container logs from DX 9.5 pods in a deployment with Helm can be combined into a single output using the commands: kubectl logs -n your-namespace -l release=your-release-name --tail=-1 In the example above, the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command.","title":"Accessing DX 9.5 container logs on Kubernetes or OpenShift"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#retrieving-additional-dx-core-logs","text":"Useful additional DX Core logs are stored on persistent volumes. To retrieve these, repeat the command below for each DX Core pod: kubectl cp -n your-namespace pod-name:opt/HCL/wp_profile/logs/WebSphere_Portal/ . In the example above the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and pod-name is the particular DX Core pod from which you wish to retrieve logs.","title":"Retrieving additional DX Core logs"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#events","text":"Pod events can give useful information for troubleshooting, such as why certain pods are not running. To get the events for a pod, you can use the following command: kubectl describe pod -n your-namespace pod-name In the above example, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and pod-name is the particular pod that you wish to examine.","title":"Events"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#persistentvolumeclaims-pvcs","text":"One of the more common reasons for a pod not starting that can be identified via the pod events (see above) is that it has unbound persistent volume claims. To understand why the claims have not been fulfilled, it is useful to list both the current persistent volume claims and the current persistent volumes and to compare the two. When comparing, it is useful to check mismatches in storage class, access modes and capacity between available volumes and unfulfilled claims. The commands to list these resources are: kubectl get pvc -n your-namespace kubectl get pv In the above example, your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed.","title":"PersistentVolumeClaims (PVCs)"},{"location":"platform/kubernetes/operations/troubleshooting/helm_troubleshooting/#retrieving-the-deployment-configuration","text":"In addition to logs, HCL Support may also request configuration information about your deployment. This is can be obtained using kubectl describe commands for different classes of objects and a selector to get all for your deployment. The most likely object types are given in the examples below: kubectl describe pods -n your-namespace -l release=your-release-name kubectl describe deployments -n your-namespace -l release=your-release-name kubectl describe statefulsets -n your-namespace -l release=your-release-name kubectl describe secrets -n your-namespace -l release=your-release-name kubectl describe services -n your-namespace -l release=your-release-name kubectl describe mappings -n your-namespace -l release=your-release-name kubectl describe configmaps -n your-namespace -l release=your-release-name In the above examples, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file by appending > some-file-name to any command. Configure and access logs in Helm This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitor the Digital Experience deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments.","title":"Retrieving the deployment configuration"},{"location":"platform/kubernetes/operator-based/customizing_container_deployment/","text":"Customizing the container deployment This section describes how to customize your HCL Digital Experience 9.5 container deployment. About this task Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation. Before you begin Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note All modifications should be made to the custom resource instance and not the individual parts of the deployment. Procedure Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Customizing the container deployment"},{"location":"platform/kubernetes/operator-based/customizing_container_deployment/#customizing-the-container-deployment","text":"This section describes how to customize your HCL Digital Experience 9.5 container deployment.","title":"Customizing the container deployment"},{"location":"platform/kubernetes/operator-based/customizing_container_deployment/#about-this-task","text":"Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation.","title":"About this task"},{"location":"platform/kubernetes/operator-based/customizing_container_deployment/#before-you-begin","text":"Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note All modifications should be made to the custom resource instance and not the individual parts of the deployment.","title":"Before you begin"},{"location":"platform/kubernetes/operator-based/customizing_container_deployment/#procedure","text":"Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Procedure"},{"location":"platform/kubernetes/operator-based/deploy_container_platforms/","text":"Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments HCL Digital Experience 9.5 Container Deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Container administration 9.5 The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. Troubleshooting cloud container Containers This section lists the basics of troubleshooting the containerized image or your deployment.","title":"Operator-based deployment"},{"location":"platform/kubernetes/operator-based/deploy_container_platforms/#operator-based-deployment","text":"This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments HCL Digital Experience 9.5 Container Deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Container administration 9.5 The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. Troubleshooting cloud container Containers This section lists the basics of troubleshooting the containerized image or your deployment.","title":"Operator-based deployment"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/","text":"dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments About this task Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note Experience API must be enabled to deploy Content Composer and Digital Asset Management. Prerequisites The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl . Creating a deployment Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information. Updating a deployment Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command. Deleting a deployment There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Main usage Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help dxctl help Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false","title":"dxctl"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#dxctl","text":"Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments","title":"dxctl"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#about-this-task","text":"Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note Experience API must be enabled to deploy Content Composer and Digital Asset Management.","title":"About this task"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#prerequisites","text":"The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl .","title":"Prerequisites"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#creating-a-deployment","text":"Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information.","title":"Creating a deployment"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#updating-a-deployment","text":"Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command.","title":"Updating a deployment"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#deleting-a-deployment","text":"There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Deleting a deployment"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#main-usage","text":"Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help","title":"Main usage"},{"location":"platform/kubernetes/operator-based/dxtools_dxctl/#dxctl-help","text":"Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false","title":"dxctl help"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration/","text":"Migrate DAM persistence and binaries This section shows the guidance to back up and restore your DAM persistence and binaries. Follow this guidance to create a backup of the DAM persistence and binaries from your Operator deployment, and restore them to a Helm-based deployment. Back up from an Operator-based deployment You must start Digital Asset Management in maintenance mode, to ensure that no actions are performed during the migration. You can set DAM to maintenance mode by changing the ConfigMap of the Operator deployment: kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, add the following entry and save the changes: data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod restarts automatically. Please wait until the pod restarts before proceeding. Verify that persistence (read-write) and DAM pods are up and running, and are in maintenance mode. Use the following command to see the current status of the pods: kubectl -n <namespace> get pods If more than one DAM or persistence pod is running, scale down the pods to only one of each type. Adjust the dxctl property file: dam.minreplicas:1 dam.maxreplicas:1 persist.minreplicas:1 persist.maxreplicas:1 And apply it using the dxctl tool. ./dxctl -\u2013update -p deployment.properties The changes are applied and any replicas are terminated. Use kubectl to check the progress. Verify that DAM is in maintenance mode by running the following command: kubectl -n <namespace> logs <pod-name> Example: kubectl -n dxns logs dx-deployment-dam-0 If your output looks similar to the following, maintenance mode is enabled and you can continue: Maintenance mode is: true Listening for SIGTERM Maintenance mode is enabled. This mode solely starts the pod without any processes within it. Connect to the persistence pod. The following command opens a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database using pg_dump : pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the shell in the persistence pod exit Download the dumped database from the persistence pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Connect to the DAM pod. The following command opens a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located in /opt/app/upload: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Exit the shell in the DAM pod: exit Download the compressed DAM binaries from the DAM pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your back up to the Helm-based deployment Important Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have [extracted the Kubernetes DX configuration]((https://help.hcltechsw.com/digital-experience/9.5/containerization/helm_extract_operator_properties.html) from the Operator-based deployment to a valid custom-values.yaml file is done. You must enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except digitalAssetManagement and the persistence : applications: core: false runtimeController: false contentComposer: false designStudio: false digitalAssetManagement: true imageProcessor: false openLdap: false persistence: true remoteSearch: false ringApi: false ambassador: false Scale down persistence to a single node: scaling: replicas: persistenceNode: 1 You can now start the Helm deployment. If you're running the migration of DAM first: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If core migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment The following outcomes are expected: The DAM and persistence pods are running and kept alive. The DAM application is not running. Upload the backup database. Use the following command to transfer the backup database to the remote persistence pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns-helm/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to the persistence (read-write) pod. Use the following command to open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns-helm -- /bin/bash You must drop the DAM database, if it exists. Disconnect all connections that use the database and drop any existing databases of the Helm deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the shell in the persistence pod: exit Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Exit the shell in the DAM pod: exit Disable the migration mode and the deployment. Before you can start the final upgrade of the Helm deployment, some adjustments are needed: Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. You can now upgrade the Helm deployment. helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment","title":"Migrate DAM persistence and binaries"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration/#migrate-dam-persistence-and-binaries","text":"This section shows the guidance to back up and restore your DAM persistence and binaries. Follow this guidance to create a backup of the DAM persistence and binaries from your Operator deployment, and restore them to a Helm-based deployment.","title":"Migrate DAM persistence and binaries"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration/#back-up-from-an-operator-based-deployment","text":"You must start Digital Asset Management in maintenance mode, to ensure that no actions are performed during the migration. You can set DAM to maintenance mode by changing the ConfigMap of the Operator deployment: kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, add the following entry and save the changes: data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod restarts automatically. Please wait until the pod restarts before proceeding. Verify that persistence (read-write) and DAM pods are up and running, and are in maintenance mode. Use the following command to see the current status of the pods: kubectl -n <namespace> get pods If more than one DAM or persistence pod is running, scale down the pods to only one of each type. Adjust the dxctl property file: dam.minreplicas:1 dam.maxreplicas:1 persist.minreplicas:1 persist.maxreplicas:1 And apply it using the dxctl tool. ./dxctl -\u2013update -p deployment.properties The changes are applied and any replicas are terminated. Use kubectl to check the progress. Verify that DAM is in maintenance mode by running the following command: kubectl -n <namespace> logs <pod-name> Example: kubectl -n dxns logs dx-deployment-dam-0 If your output looks similar to the following, maintenance mode is enabled and you can continue: Maintenance mode is: true Listening for SIGTERM Maintenance mode is enabled. This mode solely starts the pod without any processes within it. Connect to the persistence pod. The following command opens a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database using pg_dump : pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the shell in the persistence pod exit Download the dumped database from the persistence pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Connect to the DAM pod. The following command opens a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located in /opt/app/upload: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Exit the shell in the DAM pod: exit Download the compressed DAM binaries from the DAM pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz","title":"Back up from an Operator-based deployment"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration/#restore-your-back-up-to-the-helm-based-deployment","text":"Important Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have [extracted the Kubernetes DX configuration]((https://help.hcltechsw.com/digital-experience/9.5/containerization/helm_extract_operator_properties.html) from the Operator-based deployment to a valid custom-values.yaml file is done. You must enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except digitalAssetManagement and the persistence : applications: core: false runtimeController: false contentComposer: false designStudio: false digitalAssetManagement: true imageProcessor: false openLdap: false persistence: true remoteSearch: false ringApi: false ambassador: false Scale down persistence to a single node: scaling: replicas: persistenceNode: 1 You can now start the Helm deployment. If you're running the migration of DAM first: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If core migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment The following outcomes are expected: The DAM and persistence pods are running and kept alive. The DAM application is not running. Upload the backup database. Use the following command to transfer the backup database to the remote persistence pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns-helm/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to the persistence (read-write) pod. Use the following command to open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns-helm -- /bin/bash You must drop the DAM database, if it exists. Disconnect all connections that use the database and drop any existing databases of the Helm deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the shell in the persistence pod: exit Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Exit the shell in the DAM pod: exit Disable the migration mode and the deployment. Before you can start the final upgrade of the Helm deployment, some adjustments are needed: Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. You can now upgrade the Helm deployment. helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment","title":"Restore your back up to the Helm-based deployment"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/","text":"Migrate to new DAM DB This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB. 1. Upgrade your existing Helm-based deployments to CF200 To perform the DAM DB migration you must first upgrade your existing DX deployment to CF200. Important During upgrading to CF200, the Helm-based upgrade procedure detects the old deprecated DAM DB and notifies you with the following warning. The message indicates that the deployment is using an old DAM DB system that is deprecated, hence you must migrate to the new DAM DB. If you do not migrate to a new DAM DB, you might lose data during future DX updates. Warning Installation of HCL DX 95 CF200 done. This deployment is using an old DAM Database system and is deprecated. You must migrate this to the new DAM Database. If you receive this message, you must upgrade your DAM Database using the following steps; otherwise you can continue with the upgrade procedure. 2. Back up the existing DAM DB Back up of your existing DAM Database. Ensure that the DX pods are in running state before proceeding with the backup procedure. Connect to the Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns -- /bin/bash Dump the current database using the pg_dump command: pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit Download the dumped database to local system by using the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp 3. Migrate to new DB Enable the DAM DB migration mode to migrate your existing DAM DB to the new DB. Change the following configuration in your custom values.yaml file. Enable DAM Database Migration mode: # Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: true Scale down persistence nodes to 1: scaling: # The default amount of replicas per application replicas: persistenceNode: 1 Perform an upgrade with the new configuration: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n <namespace> The upgrade will turn off the deprecated old Database system and deploy the new DAM Database system. 4. Restore DB from Old DB Backup Restore the data from the old database to the new database. Upload Old DB backup to persistence pod: You can now transfer the backup database to the remote Persistence container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Drop the DAM database if it exists: Disconnect all connections that use the database and drop any existing databases of the deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit 5. Disable DAM Database Migration mode and Scale the persistence nodes to 3 Once the restore procedure is completed, you can disable the DAM DB migration mode. You must also scale the persistence node to 3 for scalability and availability. Disable DAM Database Migration mode: # Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: false Scale up persistence nodes to 3: scaling: # The default amount of replicas per application replicas: persistenceNode: 3 Perform a helm upgrade with the updated values.yaml file: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n < namespace > On successful migration, you will receive the following message. Success message: Installation of HCL DX 95 CF200 done. See HCL Digital Experience product documentation website , for further information. You might have to wait for a few minutes until all the persistence pods and DAM pods are back to the running state.","title":"Migrate to new DAM DB"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#migrate-to-new-dam-db","text":"This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB.","title":"Migrate to new DAM DB"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#1-upgrade-your-existing-helm-based-deployments-to-cf200","text":"To perform the DAM DB migration you must first upgrade your existing DX deployment to CF200. Important During upgrading to CF200, the Helm-based upgrade procedure detects the old deprecated DAM DB and notifies you with the following warning. The message indicates that the deployment is using an old DAM DB system that is deprecated, hence you must migrate to the new DAM DB. If you do not migrate to a new DAM DB, you might lose data during future DX updates. Warning Installation of HCL DX 95 CF200 done. This deployment is using an old DAM Database system and is deprecated. You must migrate this to the new DAM Database. If you receive this message, you must upgrade your DAM Database using the following steps; otherwise you can continue with the upgrade procedure.","title":"1. Upgrade your existing Helm-based deployments to CF200"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#2-back-up-the-existing-dam-db","text":"Back up of your existing DAM Database. Ensure that the DX pods are in running state before proceeding with the backup procedure. Connect to the Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns -- /bin/bash Dump the current database using the pg_dump command: pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit Download the dumped database to local system by using the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp","title":"2. Back up the existing DAM DB"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#3-migrate-to-new-db","text":"Enable the DAM DB migration mode to migrate your existing DAM DB to the new DB. Change the following configuration in your custom values.yaml file. Enable DAM Database Migration mode: # Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: true Scale down persistence nodes to 1: scaling: # The default amount of replicas per application replicas: persistenceNode: 1 Perform an upgrade with the new configuration: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n <namespace> The upgrade will turn off the deprecated old Database system and deploy the new DAM Database system.","title":"3. Migrate to new DB"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#4-restore-db-from-old-db-backup","text":"Restore the data from the old database to the new database. Upload Old DB backup to persistence pod: You can now transfer the backup database to the remote Persistence container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Drop the DAM database if it exists: Disconnect all connections that use the database and drop any existing databases of the deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit","title":"4. Restore DB from Old DB Backup"},{"location":"platform/kubernetes/operator-migration/helm_dam_migration_newDB/#5-disable-dam-database-migration-mode-and-scale-the-persistence-nodes-to-3","text":"Once the restore procedure is completed, you can disable the DAM DB migration mode. You must also scale the persistence node to 3 for scalability and availability. Disable DAM Database Migration mode: # Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: false Scale up persistence nodes to 3: scaling: # The default amount of replicas per application replicas: persistenceNode: 3 Perform a helm upgrade with the updated values.yaml file: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n < namespace > On successful migration, you will receive the following message. Success message: Installation of HCL DX 95 CF200 done. See HCL Digital Experience product documentation website , for further information. You might have to wait for a few minutes until all the persistence pods and DAM pods are back to the running state.","title":"5. Disable DAM Database Migration mode and Scale the persistence nodes to 3"},{"location":"platform/kubernetes/operator-migration/helm_fallback_migration_Operator_deployment/","text":"Revert operator migration This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Follow this guidance to create a backup to support the capability to restore the DX 9.5 Core and Digital Asset Management Operator deployment. Restore Core Operator deployment Connect to the Core pod. The following command opens a shell in the running core container: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Start the server. Navigate to the profile bin folder and run the startServer command. cd /opt/HCL/wp_profile/bin/./startServer.sh WebSphere_Portal Exit the core container. Close the shell in the core container. exit Reset the scaling of the Core pods. Reset the replication settings for Core to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dx.minreplicas:<min_number_of_replicas> dx.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and the core pods are started. Use the kubectl get pods command to check the progress. If the pods are not started after a few minutes, force the change to be applied using the following command: kubectl delete statefulset -n <namespace> dx-deployment Restore DAM Operator deployment Disable maintenance mode for DAM. Digital Asset Management must be started without maintenance mode to restore the running status. We achieve this by changing the ConfigMap of the Operator deployment. kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, remove maintenance_mode:true from the dx.deploy.dam.features entry and save the changes. If maintenance_mode:true is the only entry for this key, dx.deploy.dam.features can be removed completely. data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod will restart automatically after some seconds. Please wait until the pod is restarted before proceeding. Reset the scaling of the DAM and Persistence pods. Reset the replication settings for DAM and Persistence to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dam.minreplicas:<min_number_of_replicas> dam.maxreplicas:<max_number_of_replicas> persist.minreplicas:<min_number_of_replicas> persist.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties","title":"Revert operator migration"},{"location":"platform/kubernetes/operator-migration/helm_fallback_migration_Operator_deployment/#revert-operator-migration","text":"This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Follow this guidance to create a backup to support the capability to restore the DX 9.5 Core and Digital Asset Management Operator deployment.","title":"Revert operator migration"},{"location":"platform/kubernetes/operator-migration/helm_fallback_migration_Operator_deployment/#restore-core-operator-deployment","text":"Connect to the Core pod. The following command opens a shell in the running core container: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Start the server. Navigate to the profile bin folder and run the startServer command. cd /opt/HCL/wp_profile/bin/./startServer.sh WebSphere_Portal Exit the core container. Close the shell in the core container. exit Reset the scaling of the Core pods. Reset the replication settings for Core to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dx.minreplicas:<min_number_of_replicas> dx.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and the core pods are started. Use the kubectl get pods command to check the progress. If the pods are not started after a few minutes, force the change to be applied using the following command: kubectl delete statefulset -n <namespace> dx-deployment","title":"Restore Core Operator deployment"},{"location":"platform/kubernetes/operator-migration/helm_fallback_migration_Operator_deployment/#restore-dam-operator-deployment","text":"Disable maintenance mode for DAM. Digital Asset Management must be started without maintenance mode to restore the running status. We achieve this by changing the ConfigMap of the Operator deployment. kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, remove maintenance_mode:true from the dx.deploy.dam.features entry and save the changes. If maintenance_mode:true is the only entry for this key, dx.deploy.dam.features can be removed completely. data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod will restart automatically after some seconds. Please wait until the pod is restarted before proceeding. Reset the scaling of the DAM and Persistence pods. Reset the replication settings for DAM and Persistence to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dam.minreplicas:<min_number_of_replicas> dam.maxreplicas:<max_number_of_replicas> persist.minreplicas:<min_number_of_replicas> persist.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties","title":"Restore DAM Operator deployment"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/","text":"Migrate the Core profile This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment. Back up from an Operator-based deployment Ensure that only one Core pod is running. Check how many pods are running with the following command: kubectl -n <namespace> get pods If more than one pod is running, scale down the Core pods so only one (1) pod is running. On the Operator deployment, adjust the dxctl property file: dx.maxreplicas:1 dx.minreplicas:1 then apply the changes in the updated property file using the dxctl tool: ./dxctl -\u2013update -p deployment.properties Once the changes are applied, any replicas will be terminated. You may check the progress by running the kubectl command. Connect to the Core pod The following command opens a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash 1. **Stop the Core application before a backup from the wp\\_profile is created.** Navigate to the profile bin folder and run the `stopServer` command: ``` cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> ``` !!! note While the server is stopped, the liveness probe returns a failure result to Kubernetes. Once the maximum allowed number of failures is reached, Kubernetes restarts the pod, closing any `kubectl exec` session and brings the Core pod back online. However, with the default liveness probe settings in `full-deployment.properties`, this process takes approximately two \\(2\\) hours to occur. If you need to adjust your liveness probe settings to allow time to perform the profile backup \\(for instance, because you have reduced them considerably from the default in your deployment\\), make the changes in your `full-deployment.properties` file, and then apply the changes using `dxctl` as described in Step 1. 2. **Compress the whole Core profile folder /opt/HCL/wp\\_profile** by running the following command: ``` cd /opt/HCL tar -cvpzf core_prof_95_CF199.tar.gz --exclude=/core_prof_95_CF199.tar.gz --one-file-system wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ``` Download the backup Core profile Download the backup Core profile from the Core pod to your local system** by running the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> For example: ``` kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF199.tar.gz /tmp/core_prof_95_CF199.tar.gz ``` (Optional) Make the old environment unavailable. Kubernetes eventually brings the Core pod back online by restarting it (see the note on liveness probes in Step 2.a). If you want to keep the Operator-based deployment unavailable to users after backing up the profile, you can do the following steps: Adjust the dxctl property file : dx.maxreplicas: 0 dx.minreplicas: 0 dx.replicas: 0 Apply it using the dxctl tool : ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and all the additional Core pods are terminated. You can use kubectl get pods to check the progress. If the additional Core pods are not terminated after a few minutes, use the following command to force the changes to be applied: kubectl delete statefulset -n <namespace> dx-deployment If you want to enable the Operator-based deployment Core pod again, set the values of the dxctl property file to values greater than zero and apply the changes using the dxctl tool. Restore your back up to the Helm-based deployment Important Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure correct state Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file. Enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except runtimeController and the core : applications: core: true runtimeController: true contentComposer: false designStudio: false digitalAssetManagement: false imageProcessor: false openLdap: false persistence: false remoteSearch: false ringApi: false ambassador: false Start the Helm deployment. If you are running the migration of the Core first, run the following command: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If DAM migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> For example: helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment Expected Outcome The following outcomes are expected: The core pod is running and kept alive. The core application is not running. No default profile is created automatically. The folder /opt/HCL/wp_profile is empty. Upload the backup profile Use the following command to transfer the backup profile to the remote Core pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> For example: kubectl cp /tmp/core_prof_95_CF199.tar.gz dxns-helm/dx-deployment-core-0:/tmp/core_prof_95_CF199.tar.gz Connect to the Core pod Use the following command to pine a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-core-0 -n dxns-helm -- /bin/bash 1. **Extract the profile.** Move the core\\_prof\\_95\\_CF199.tar.gz from the /temp folder to the profile folder /opt/HCL/profiles before extracting it: ``` tar -xf /tmp/core_prof_95_CF199.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF199 rm /tmp/core_prof_95_CF199.tar.gz ``` 2. **Create a symbolink \\(symlink\\)** by running the following command: ``` rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF199 /opt/HCL/wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ``` Disable the migration mode and the deployment. Important If Digital Asset Management was used in the Operator-based deployment, you must follow the DAM migration instructions before disabling migration mode. Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. Upgrade the Helm deployment using the following command: helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment Reconfigure applications to use relative paths. Coming from old Operator deployments, it can appear that the applications (Digital Asset Management, Content Composer, and Design Studio) are still configured to use absolute URLs for their rendering. If you use any of these applications, it is highly recommended that you reconfigure them to use relative paths. Important Reconfigure relative paths for applications that are active in your deployment. Otherwise, the command will fail. Replace the placeholders for NAMESPACE , YOUR_WAS_ADMIN_USER , and YOUR_WAS_ADMIN_PASSWORD with the corresponding values of your deployment. For Content Composer: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-headless-content-pages -Dcc.static.ui.url=/dx/ui/content/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' For Digital Asset Management: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-media-library-pages -Ddam.static.ui.url=/dx/ui/dam/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' For Design Studio: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-content-sites-pages -Dcontentsites.ui.url=/dx/ui/site-manager/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' Create Ingress Secret Create the secret with your TLS certificate for the Ambassador Ingress in your Helm-based deployment. To make your migrated Helm deployment accessible, you need to configure the TLS certificate that is used by the Ambassador Ingress. See Use certificate for more information. (Optional) Update Secrets If you have changed the WAS/Portal Administrator user, update the corresponding secrets ( dx-deployment-was and dx-deployment-wps ) in the Helm-based deployment. (Optional) Configure Remote Search. Skip this step if you have not used Remote Search in your Operator deployment, or if you have no plans to use Remote Search within your Helm-based deployment. Configure Remote Search and re-index your data again. See instructions on how to configure Remote Search within a Kubernetes environment .","title":"Migrate the Core profile"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#migrate-the-core-profile","text":"This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment.","title":"Migrate the Core profile"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#back-up-from-an-operator-based-deployment","text":"","title":"Back up from an Operator-based deployment"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#ensure-that-only-one-core-pod-is-running","text":"Check how many pods are running with the following command: kubectl -n <namespace> get pods If more than one pod is running, scale down the Core pods so only one (1) pod is running. On the Operator deployment, adjust the dxctl property file: dx.maxreplicas:1 dx.minreplicas:1 then apply the changes in the updated property file using the dxctl tool: ./dxctl -\u2013update -p deployment.properties Once the changes are applied, any replicas will be terminated. You may check the progress by running the kubectl command.","title":"Ensure that only one Core pod is running."},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#connect-to-the-core-pod","text":"The following command opens a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash 1. **Stop the Core application before a backup from the wp\\_profile is created.** Navigate to the profile bin folder and run the `stopServer` command: ``` cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> ``` !!! note While the server is stopped, the liveness probe returns a failure result to Kubernetes. Once the maximum allowed number of failures is reached, Kubernetes restarts the pod, closing any `kubectl exec` session and brings the Core pod back online. However, with the default liveness probe settings in `full-deployment.properties`, this process takes approximately two \\(2\\) hours to occur. If you need to adjust your liveness probe settings to allow time to perform the profile backup \\(for instance, because you have reduced them considerably from the default in your deployment\\), make the changes in your `full-deployment.properties` file, and then apply the changes using `dxctl` as described in Step 1. 2. **Compress the whole Core profile folder /opt/HCL/wp\\_profile** by running the following command: ``` cd /opt/HCL tar -cvpzf core_prof_95_CF199.tar.gz --exclude=/core_prof_95_CF199.tar.gz --one-file-system wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ```","title":"Connect to the Core pod"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#download-the-backup-core-profile","text":"Download the backup Core profile from the Core pod to your local system** by running the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> For example: ``` kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF199.tar.gz /tmp/core_prof_95_CF199.tar.gz ```","title":"Download the backup Core profile"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#optional-make-the-old-environment-unavailable","text":"Kubernetes eventually brings the Core pod back online by restarting it (see the note on liveness probes in Step 2.a). If you want to keep the Operator-based deployment unavailable to users after backing up the profile, you can do the following steps: Adjust the dxctl property file : dx.maxreplicas: 0 dx.minreplicas: 0 dx.replicas: 0 Apply it using the dxctl tool : ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and all the additional Core pods are terminated. You can use kubectl get pods to check the progress. If the additional Core pods are not terminated after a few minutes, use the following command to force the changes to be applied: kubectl delete statefulset -n <namespace> dx-deployment If you want to enable the Operator-based deployment Core pod again, set the values of the dxctl property file to values greater than zero and apply the changes using the dxctl tool.","title":"(Optional) Make the old environment unavailable."},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#restore-your-back-up-to-the-helm-based-deployment","text":"Important Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment.","title":"Restore your back up to the Helm-based deployment"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#ensure-correct-state","text":"Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file. Enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except runtimeController and the core : applications: core: true runtimeController: true contentComposer: false designStudio: false digitalAssetManagement: false imageProcessor: false openLdap: false persistence: false remoteSearch: false ringApi: false ambassador: false Start the Helm deployment. If you are running the migration of the Core first, run the following command: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If DAM migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> For example: helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment Expected Outcome The following outcomes are expected: The core pod is running and kept alive. The core application is not running. No default profile is created automatically. The folder /opt/HCL/wp_profile is empty.","title":"Ensure correct state"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#upload-the-backup-profile","text":"Use the following command to transfer the backup profile to the remote Core pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> For example: kubectl cp /tmp/core_prof_95_CF199.tar.gz dxns-helm/dx-deployment-core-0:/tmp/core_prof_95_CF199.tar.gz","title":"Upload the backup profile"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#connect-to-the-core-pod_1","text":"Use the following command to pine a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-core-0 -n dxns-helm -- /bin/bash 1. **Extract the profile.** Move the core\\_prof\\_95\\_CF199.tar.gz from the /temp folder to the profile folder /opt/HCL/profiles before extracting it: ``` tar -xf /tmp/core_prof_95_CF199.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF199 rm /tmp/core_prof_95_CF199.tar.gz ``` 2. **Create a symbolink \\(symlink\\)** by running the following command: ``` rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF199 /opt/HCL/wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ```","title":"Connect to the Core pod"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#disable-the-migration-mode-and-the-deployment","text":"Important If Digital Asset Management was used in the Operator-based deployment, you must follow the DAM migration instructions before disabling migration mode. Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. Upgrade the Helm deployment using the following command: helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment","title":"Disable the migration mode and the deployment."},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#reconfigure-applications-to-use-relative-paths","text":"Coming from old Operator deployments, it can appear that the applications (Digital Asset Management, Content Composer, and Design Studio) are still configured to use absolute URLs for their rendering. If you use any of these applications, it is highly recommended that you reconfigure them to use relative paths. Important Reconfigure relative paths for applications that are active in your deployment. Otherwise, the command will fail. Replace the placeholders for NAMESPACE , YOUR_WAS_ADMIN_USER , and YOUR_WAS_ADMIN_PASSWORD with the corresponding values of your deployment. For Content Composer: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-headless-content-pages -Dcc.static.ui.url=/dx/ui/content/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' For Digital Asset Management: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-media-library-pages -Ddam.static.ui.url=/dx/ui/dam/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' For Design Studio: kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-content-sites-pages -Dcontentsites.ui.url=/dx/ui/site-manager/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>'","title":"Reconfigure applications to use relative paths."},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#create-ingress-secret","text":"Create the secret with your TLS certificate for the Ambassador Ingress in your Helm-based deployment. To make your migrated Helm deployment accessible, you need to configure the TLS certificate that is used by the Ambassador Ingress. See Use certificate for more information.","title":"Create Ingress Secret"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#optional-update-secrets","text":"If you have changed the WAS/Portal Administrator user, update the corresponding secrets ( dx-deployment-was and dx-deployment-wps ) in the Helm-based deployment.","title":"(Optional) Update Secrets"},{"location":"platform/kubernetes/operator-migration/helm_operator_core_migration/#optional-configure-remote-search","text":"Skip this step if you have not used Remote Search in your Operator deployment, or if you have no plans to use Remote Search within your Helm-based deployment. Configure Remote Search and re-index your data again. See instructions on how to configure Remote Search within a Kubernetes environment .","title":"(Optional) Configure Remote Search."},{"location":"platform/kubernetes/operator-migration/operator_backup_and_recovery_procedures/","text":"Backup and recovery procedures Containerization This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments. Digital Experience 9.5 Container Deployment Architecture Learn about the HCL DX 9.5 Container deployment architecture to get a better understanding of the backup and recovery options. Note This topology is also available in the Install the HCL Digital Experience 9.5 components topic. Instructions to back up the Digital Experience 9.5 Container components The following sections describe how the administrators can create and manage backups and recovery of DX 9.5 Container components such as wp_profile, persistence layer - database, and the media in Digital Asset Management. 1. wp-profile backup Backup the file systems in the Digital Experience 9.5 container profile ( Persistent volume claim wp_profile ). Refer to the Backup and Restore topic and component backup guidance for more information. Note The HCL Digital Asset Management \\(DAM\\) component uploads folder and the DAM persistent mount for the primary instance dx-deployment-persistence-0 in the statefulset. <!-- [dx-deployment-persistence](https://console-openshift-console.apps.hcl-dxdev.hcl-dx-dev.net/k8s/ns/master-tests/statefulsets/dx-deployment-persistence). --> To create a backup of the profile Persistent volume claim wp_profile , it is recommended that: The DX has only one instance. The DX 9.5 container instance is stopped using the HCL Portal and HCL Web Content Manager command as follows: kubectl exec --stdin --tty <DX_POD_NAME> -n <NAMESPACE> -- /bin/bash cd /opt/HCL/wp_profile/bin/ /stopServer.sh WebSphere_Portal -username <USERNAME> -password <PASSWORD> The entire /opt/HCL/wp_profile directory is backed up. In the command line interface, run the following command to back up the Digital Experience 9.5 Persistent volume claim wp_profile : Before running the tar command, ensure that the backup file system that you are using has ~50% free profile. cd /opt/HCL/wp_profile tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile/* After the tar backup command is completed, it is recommended that a copy of the backup.tar.gz file is created and placed to alternate long term storage. Recover from wp_profile pervasive volume backup You can extract the backup of the wp_profile volume to recover any files you need to restore. If the version of the backup matches the current fixpack level, you can use the extracted files to populate the original pervasive volume. The procedure to do this depends on how the backup was created. Any changes that occurred after the backup was created will not be recovered. The portal database must be restored to the backup that was created when the backup of wp_profile was created. 2. Persistence layer - database backup Run the following command to back up the container components that are managed through the DX Persistence layer: pg_dump name_of_database > name_of_backup_file To back up the system components on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file After the backup command is completed, it is recommended that a copy of the resulting file is created and placed to an alternate long term storage. See the Backup and restore DAM image topic for more information. 3. Digital Asset Management media backup Use the following commands to back up the Digital Asset Management media uploads volume: A command similar to the backup command outlined in Step 1 to create a backup of wp_profile can be used to back up the two /opt/app/upload and /etc/config Digital Asset Management mount points. Refer to the following examples: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload tar -C/ -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system etc/config/* See the Backup and restore DAM image topic for more information. Alternatively, the Kubernetes documentation pages present additional options to backup and clone persistent volumes. Volume SnapShots: (1.17 [beta] and later) CSI Volume Cloning Note If either of the methods described in [Step 2] or [Step 3] is used, it is important to understand fuzzy backups with the wp_profile. A fuzzy backup is a copy of data files or directories that were operating in one state when the backup started, but in a different state by the time the backup completed. In case a volume snapshot or Container Storage Interface (CSI) volume cloning approach is used with the wp_profile , it is important that the snapshot is taken with the Digital Experience instance in shutdown state to ensure that recovery is performed. HCL Digital Experience has successfully tested the volume snapshot and CSI volume cloning methods with HCL Digital Experience 9.5 container deployments. It is recommended that customers perform the additional testing if they are using options [Step 2] and [Step 3] to manage the wp_profile backup.","title":"Backup and recovery procedures Containerization"},{"location":"platform/kubernetes/operator-migration/operator_backup_and_recovery_procedures/#backup-and-recovery-procedures-containerization","text":"This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments.","title":"Backup and recovery procedures Containerization"},{"location":"platform/kubernetes/operator-migration/operator_backup_and_recovery_procedures/#digital-experience-95-container-deployment-architecture","text":"Learn about the HCL DX 9.5 Container deployment architecture to get a better understanding of the backup and recovery options. Note This topology is also available in the Install the HCL Digital Experience 9.5 components topic.","title":"Digital Experience 9.5 Container Deployment Architecture"},{"location":"platform/kubernetes/operator-migration/operator_backup_and_recovery_procedures/#instructions-to-back-up-the-digital-experience-95-container-components","text":"The following sections describe how the administrators can create and manage backups and recovery of DX 9.5 Container components such as wp_profile, persistence layer - database, and the media in Digital Asset Management. 1. wp-profile backup Backup the file systems in the Digital Experience 9.5 container profile ( Persistent volume claim wp_profile ). Refer to the Backup and Restore topic and component backup guidance for more information. Note The HCL Digital Asset Management \\(DAM\\) component uploads folder and the DAM persistent mount for the primary instance dx-deployment-persistence-0 in the statefulset. <!-- [dx-deployment-persistence](https://console-openshift-console.apps.hcl-dxdev.hcl-dx-dev.net/k8s/ns/master-tests/statefulsets/dx-deployment-persistence). --> To create a backup of the profile Persistent volume claim wp_profile , it is recommended that: The DX has only one instance. The DX 9.5 container instance is stopped using the HCL Portal and HCL Web Content Manager command as follows: kubectl exec --stdin --tty <DX_POD_NAME> -n <NAMESPACE> -- /bin/bash cd /opt/HCL/wp_profile/bin/ /stopServer.sh WebSphere_Portal -username <USERNAME> -password <PASSWORD> The entire /opt/HCL/wp_profile directory is backed up. In the command line interface, run the following command to back up the Digital Experience 9.5 Persistent volume claim wp_profile : Before running the tar command, ensure that the backup file system that you are using has ~50% free profile. cd /opt/HCL/wp_profile tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile/* After the tar backup command is completed, it is recommended that a copy of the backup.tar.gz file is created and placed to alternate long term storage. Recover from wp_profile pervasive volume backup You can extract the backup of the wp_profile volume to recover any files you need to restore. If the version of the backup matches the current fixpack level, you can use the extracted files to populate the original pervasive volume. The procedure to do this depends on how the backup was created. Any changes that occurred after the backup was created will not be recovered. The portal database must be restored to the backup that was created when the backup of wp_profile was created. 2. Persistence layer - database backup Run the following command to back up the container components that are managed through the DX Persistence layer: pg_dump name_of_database > name_of_backup_file To back up the system components on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file After the backup command is completed, it is recommended that a copy of the resulting file is created and placed to an alternate long term storage. See the Backup and restore DAM image topic for more information. 3. Digital Asset Management media backup Use the following commands to back up the Digital Asset Management media uploads volume: A command similar to the backup command outlined in Step 1 to create a backup of wp_profile can be used to back up the two /opt/app/upload and /etc/config Digital Asset Management mount points. Refer to the following examples: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload tar -C/ -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system etc/config/* See the Backup and restore DAM image topic for more information. Alternatively, the Kubernetes documentation pages present additional options to backup and clone persistent volumes. Volume SnapShots: (1.17 [beta] and later) CSI Volume Cloning Note If either of the methods described in [Step 2] or [Step 3] is used, it is important to understand fuzzy backups with the wp_profile. A fuzzy backup is a copy of data files or directories that were operating in one state when the backup started, but in a different state by the time the backup completed. In case a volume snapshot or Container Storage Interface (CSI) volume cloning approach is used with the wp_profile , it is important that the snapshot is taken with the Digital Experience instance in shutdown state to ensure that recovery is performed. HCL Digital Experience has successfully tested the volume snapshot and CSI volume cloning methods with HCL Digital Experience 9.5 container deployments. It is recommended that customers perform the additional testing if they are using options [Step 2] and [Step 3] to manage the wp_profile backup.","title":"Instructions to back up the Digital Experience 9.5 Container components"},{"location":"platform/kubernetes/operator-migration/operator_migration_preparation/","text":"Preparation This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. Warning The Operator-based to Helm-based migration is a strict side-by-side migration. This section outlines the needed steps to configure your new Helm-based deployment with your old Operator-based deployment configuration settings. Once you have extracted the needed data, and have shut down your Operator-based deployment, you can apply your exported data in your new Helm-based deployment. Note that you are not migrating your DX Core database as you will reuse the same database instance in your Helm-based deployment. Important Ensure that you go through the requirements in the HCL DX 9.5 limitations and requirements topic. Ensure that you have followed the preparation process in Planning your container deployment using Helm , and that you have already created your custom-values.yaml. As with any migration activity, we recommend that you make backups of the data of your current environment before proceeding. See Backup and recovery procedures for more information. In case of any errors after migration, you can fall back to your previous Operator-based environment. See Migration to restore Core and DAM Operator deployment for more information. You must have the properties file you used with dxctl in your old Operator deployment. If you do not have the properties file, refer to the dxctl topic to extract the properties file from your existing deployment using the getProperties function. Ensure to prepare any other needed infrastructure-related items (like persistent volumes, Kubernetes load balancer configuration, etc.) before proceeding with migration to Helm. Optionally, you can perform a test deployment to make sure that all prerequisites and requirements for the Helm deployment are met. Follow the installation steps and check if all the functionality of the default deployment is accessible. If you do not prefer to do an initial test, you can skip to start with the Core and DAM migration immediately. Before migrating to Helm, you must migrate the configuration of your Operator-based deployment first. Follow this guidance to prepare the property mappings for your HCL DX 9.5 Operator-based deployment. You can reuse the values from your deployment.properties file in your new custom-values.yaml. Property mappings This section lists the mapping of the dxctl deployment.properties file with the custom-values.yaml. Note You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you, or those that have been adjusted by you prior deploying the Operator with dxctl . dxctl deployment.properties custom-values.yaml Description dx.namespace Not applicable The namespace used for the deployment. This is handed directly to Helm through the command line interface. dx.name Not applicable The deployment name . This is handed directly to Helm through the command line interface. default.repository images.repository Defines the image repository for all container images. dx.pullpolicy images.pullPolicy Defines the image pull policy for all container images. <application>.image images.name.<application> Name of the container image. <application>.tag images.tag.<application> Name of the container tag. <application>.enabled applications.<application> Enables or disables specific applications. dx.pod.nodeselector nodeSelector.<application> NodeSelector used for pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance is tuned for authoring or not. composer.enabled applications.contentComposer Selects if Content Composer is deployed or not. dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management is deployed or not. persist.force-read Not applicable Read-only fallback enablement. This is always enabled in Helm. dxctl deployment.properties custom-values.yaml Description dx.volume volumes.core.profile.volumeName The name of the volume used for the DX core profile. dx.volume.size volumes.core.profile.requests.storage Size of the volume used for the DX core profile. dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for the DX core profile. dx.splitlogging: false Not applicable. Defines if the log directory uses a separate volume. This is always enabled in Helm. dx.logging.stgclass volumes.core.log.storageClassName StorageClass for the DX core logging volume. dx.logging.size volumes.core.log.requests.storage StorageClass for the DX core logging volume. dx.tranlogging Not applicable. Defines if the transaction log directory uses a separate volume. This is always enabled in Helm. dx.tranlogging.reclaim Not applicable. Reclaim policy for DX core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX core transaction log volume. dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX core transaction log volume. remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile. remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile. dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM. dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM. dxctl deployment.properties custom-values.yaml Description dx.path.contextroot networking.core.contextRoot Context root used for DX. dx.path.personalized networking.core.personalizedHome Personalized URL path for DX. dx.path.home networking.core.home Non-personalized URL path for DX. dx.deploy.host.override networking.core.host Host name to be used instead of the load balancer host name. dx.deploy.host.override.force Not applicable. Force the use of the override host. Obsolete in Helm. dx.config.cors / dam.config.cors networking.addon.<application>.corsOrigin CORS configuration for applications, can be configured per application in Helm. hybrid.enabled Not applicable. Defines if hybrid is enabled or not. Helm derives this from other networking and application settings. hybrid.url networking.core.host URL of the DX core instance in a hybrid deployment. hybrid.port networking.core.port Port of the DX core instance in a hybrid deployment. dxctl deployment.properties custom-values.yaml Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum number of pods when scaling is enabled. dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum number of pods when scaling is enabled. dx.replicas scaling.replicas.core Default number of pods when scaling is enabled. dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU target for autoscaling. dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum number of pods when scaling is enabled. api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum number of pods when scaling is enabled. api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU target for autoscaling. api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum number of pods when scaling is enabled. composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum number of pods when scaling is enabled. composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU target for autoscaling. composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum number of pods when scaling is enabled. dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum number of pods when scaling is enabled. dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU target for autoscaling. dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum number of pods when scaling is enabled. imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum number of pods when scaling is enabled. imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU target for autoscaling. imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dx.request.cpu resources.core.requests.cpu CPU request. dx.request.memory resources.core.requests.memory Memory request. dx.limit.cpu resources.core.limits.cpu CPU limit. dx.limit.memory resources.core.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description api.request.cpu resources.ringApi.requests.cpu CPU request. api.request.memory resources.ringApi.requests.memory Memory request. api.limit.cpu resources.ringApi.limits.cpu CPU limit. api.limit.memory resources.ringApi.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description composer.request.cpu resources.contentComposer.requests.cpu CPU request. composer.request.memory resources.contentComposer.requests.memory Memory request. composer.limit.cpu resources.contentComposer.limits.cpu CPU limit. composer.limit.memory resources.contentComposer.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU request. dam.request.memory resources.digitalAssetManagement.requests.memory Memory request. dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU limit. dam.limit.memory resources.digitalAssetManagement.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description persist.request.cpu resources.persistence.requests.cpu CPU request. persist.request.memory resources.persistence.requests.memory Memory request. persist.limit.cpu resources.persistence.limits.cpu CPU limit. persist.limit.memory resources.persistence.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU request. imgproc.request.memory resources.imageProcessor.requests.memory Memory request. imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU limit. imgproc.limit.memory resources.imageProcessor.limits.memory Memory limit.","title":"Preparation"},{"location":"platform/kubernetes/operator-migration/operator_migration_preparation/#preparation","text":"This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. Warning The Operator-based to Helm-based migration is a strict side-by-side migration. This section outlines the needed steps to configure your new Helm-based deployment with your old Operator-based deployment configuration settings. Once you have extracted the needed data, and have shut down your Operator-based deployment, you can apply your exported data in your new Helm-based deployment. Note that you are not migrating your DX Core database as you will reuse the same database instance in your Helm-based deployment. Important Ensure that you go through the requirements in the HCL DX 9.5 limitations and requirements topic. Ensure that you have followed the preparation process in Planning your container deployment using Helm , and that you have already created your custom-values.yaml. As with any migration activity, we recommend that you make backups of the data of your current environment before proceeding. See Backup and recovery procedures for more information. In case of any errors after migration, you can fall back to your previous Operator-based environment. See Migration to restore Core and DAM Operator deployment for more information. You must have the properties file you used with dxctl in your old Operator deployment. If you do not have the properties file, refer to the dxctl topic to extract the properties file from your existing deployment using the getProperties function. Ensure to prepare any other needed infrastructure-related items (like persistent volumes, Kubernetes load balancer configuration, etc.) before proceeding with migration to Helm. Optionally, you can perform a test deployment to make sure that all prerequisites and requirements for the Helm deployment are met. Follow the installation steps and check if all the functionality of the default deployment is accessible. If you do not prefer to do an initial test, you can skip to start with the Core and DAM migration immediately. Before migrating to Helm, you must migrate the configuration of your Operator-based deployment first. Follow this guidance to prepare the property mappings for your HCL DX 9.5 Operator-based deployment. You can reuse the values from your deployment.properties file in your new custom-values.yaml. Property mappings This section lists the mapping of the dxctl deployment.properties file with the custom-values.yaml. Note You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you, or those that have been adjusted by you prior deploying the Operator with dxctl . dxctl deployment.properties custom-values.yaml Description dx.namespace Not applicable The namespace used for the deployment. This is handed directly to Helm through the command line interface. dx.name Not applicable The deployment name . This is handed directly to Helm through the command line interface. default.repository images.repository Defines the image repository for all container images. dx.pullpolicy images.pullPolicy Defines the image pull policy for all container images. <application>.image images.name.<application> Name of the container image. <application>.tag images.tag.<application> Name of the container tag. <application>.enabled applications.<application> Enables or disables specific applications. dx.pod.nodeselector nodeSelector.<application> NodeSelector used for pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance is tuned for authoring or not. composer.enabled applications.contentComposer Selects if Content Composer is deployed or not. dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management is deployed or not. persist.force-read Not applicable Read-only fallback enablement. This is always enabled in Helm. dxctl deployment.properties custom-values.yaml Description dx.volume volumes.core.profile.volumeName The name of the volume used for the DX core profile. dx.volume.size volumes.core.profile.requests.storage Size of the volume used for the DX core profile. dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for the DX core profile. dx.splitlogging: false Not applicable. Defines if the log directory uses a separate volume. This is always enabled in Helm. dx.logging.stgclass volumes.core.log.storageClassName StorageClass for the DX core logging volume. dx.logging.size volumes.core.log.requests.storage StorageClass for the DX core logging volume. dx.tranlogging Not applicable. Defines if the transaction log directory uses a separate volume. This is always enabled in Helm. dx.tranlogging.reclaim Not applicable. Reclaim policy for DX core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX core transaction log volume. dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX core transaction log volume. remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile. remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile. dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM. dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM. dxctl deployment.properties custom-values.yaml Description dx.path.contextroot networking.core.contextRoot Context root used for DX. dx.path.personalized networking.core.personalizedHome Personalized URL path for DX. dx.path.home networking.core.home Non-personalized URL path for DX. dx.deploy.host.override networking.core.host Host name to be used instead of the load balancer host name. dx.deploy.host.override.force Not applicable. Force the use of the override host. Obsolete in Helm. dx.config.cors / dam.config.cors networking.addon.<application>.corsOrigin CORS configuration for applications, can be configured per application in Helm. hybrid.enabled Not applicable. Defines if hybrid is enabled or not. Helm derives this from other networking and application settings. hybrid.url networking.core.host URL of the DX core instance in a hybrid deployment. hybrid.port networking.core.port Port of the DX core instance in a hybrid deployment. dxctl deployment.properties custom-values.yaml Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum number of pods when scaling is enabled. dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum number of pods when scaling is enabled. dx.replicas scaling.replicas.core Default number of pods when scaling is enabled. dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU target for autoscaling. dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum number of pods when scaling is enabled. api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum number of pods when scaling is enabled. api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU target for autoscaling. api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum number of pods when scaling is enabled. composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum number of pods when scaling is enabled. composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU target for autoscaling. composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum number of pods when scaling is enabled. dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum number of pods when scaling is enabled. dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU target for autoscaling. dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum number of pods when scaling is enabled. imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum number of pods when scaling is enabled. imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU target for autoscaling. imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dx.request.cpu resources.core.requests.cpu CPU request. dx.request.memory resources.core.requests.memory Memory request. dx.limit.cpu resources.core.limits.cpu CPU limit. dx.limit.memory resources.core.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description api.request.cpu resources.ringApi.requests.cpu CPU request. api.request.memory resources.ringApi.requests.memory Memory request. api.limit.cpu resources.ringApi.limits.cpu CPU limit. api.limit.memory resources.ringApi.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description composer.request.cpu resources.contentComposer.requests.cpu CPU request. composer.request.memory resources.contentComposer.requests.memory Memory request. composer.limit.cpu resources.contentComposer.limits.cpu CPU limit. composer.limit.memory resources.contentComposer.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU request. dam.request.memory resources.digitalAssetManagement.requests.memory Memory request. dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU limit. dam.limit.memory resources.digitalAssetManagement.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description persist.request.cpu resources.persistence.requests.cpu CPU request. persist.request.memory resources.persistence.requests.memory Memory request. persist.limit.cpu resources.persistence.limits.cpu CPU limit. persist.limit.memory resources.persistence.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU request. imgproc.request.memory resources.imageProcessor.requests.memory Memory request. imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU limit. imgproc.limit.memory resources.imageProcessor.limits.memory Memory limit.","title":"Preparation"},{"location":"whatsnew/","text":"What's New The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. This site describes new features in each release. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes in each release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's New"},{"location":"whatsnew/#whats-new","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. This site describes new features in each release. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes in each release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's New"},{"location":"whatsnew/deprecated_features/","text":"Deprecated features Deprecation means that the features are supported at this time but can be removed in an upcoming CF. It is recommended that deprecated items be removed as soon as possible as they can cause unpredictable behavior. Links to more information on the replacement for those items listed for deprecation will be posted as they become available to provide help as you move away from these features. Recently Deprecated Features Category Deprecated Recommended Action Effective Date Deploying Ambassador Beginning with Container Update CF203, the Ambassador shipped as part of the DX Helm deployment is deprecated and will be removed in a subsequent HCL DX 9.5 Container Update release. For more information, see HAProxy replaces Ambassador May 2022 Deploying Operator-based deployment Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments. There will be no further updates or code fixes provided for the Operator-based deployments. For more information, see Helm-based deployment December 2021 Integrating Sametime Integration Integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Integrating Add to Sametime List Effective November 2021, integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Online Help Sametime Web 2.0 Contact List Effective November 2021, integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Developing Microsoft Exchange 2010 Calendar Effective November 2021, integration with Microsoft Exchange 2010 will be deprecated. Exchange 2010 is out of support for Microsoft. November 2021 Developing Microsoft Exchange 2010 Mail Effective November 2021, integration with Microsoft Exchange 2010 will be deprecated. Exchange 2010 is out of support for Microsoft. November 2021 Online Help Coach Effective November 2021, Coach portlet will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Online Help Dynamic Coach Effective November 2021, Dynamic Coach portlet will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Configuring Federated Documents Picker Effective November 2021, Federated Documents Picker feature will be deprecated. HCL Digital Asset Management may be use in replacement for this feature. November 2021 Online Help Frequent Users Effective November 2021, Frequent Users portlet will be deprecated. This is an obsolete portlet and no replacement is needed. November 2021 Setting up a website Syndicated Feed Portlet Effective November 2021, Syndicated Feed portlet will be deprecated. November 2021 Integrating TPIR Configuration Viewer Effective November 2021, TPIR Configuration Viewer will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Developing UX Screen Flow Manager - Dialog Stack Portlet Effective November 2021, Dialog Stack portlet will be deprecated. The feature was already removed on the later version of Portal. November 2021 Developing UX Screen Flow Manager - Dialog State Display Portlet Effective November 2021, Dialog State Display portlet will be deprecated. The feature was already removed on the later version of Portal. November 2021 Setting up a website Welcome to WebSphere Portal Effective November 2021, Welcome portlet will be deprecated. This is an obsolete portlet and no replacement is needed. November 2021 Integrating Watson Content Hub support Effective November 2021, integration with Watson Content Hub will be deprecated. HCL Digital Asset Management may be use in replacement for this feature. November 2021 Integrating Web Application Bridge - Microsoft SharePoint Effective November 2021, integration with Microsoft SharePoint will be deprecated. November 2021 Integrating SAP NetWeaver Effective November 2021, integration with SAP NetWeaver will be deprecated. NetWeaver is no longer supported by SAP. November 2021 Integrating Domino Effective November 2021, integration with Domino will be deprecated. The iFrame integration approach is already obsolete and not used. November 2021 Integrating iNotes portlet Effective November 2021, integration with Domino will be deprecated. The iFrame integration approach is already obsolete and not used. November 2021 Migrating IBM\u00ae\u202fJSF Bridge In WAS 9.0.5.2 and 8.5.5.17, IBM removed the JSF Bridge from WAS. To address this, an updated JSF Portlet Bridge is included in DX CF18 release. Customers installing HCL DX CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 should utilize the new JSF Portlet Bridge. The HCL JSF Portlet Bridge is built using the same class names as the IBM JSF Portlet Bridge provided in WAS 9.0.0.7 thru 9.0.5.1 and WAS 8.5.5.16 respectively. User applications should be able to continue to function without any modification. December 2020 Previously Deprecated Features Category Unsupported Recommended Action Setting up a site Running the \"Local Content Viewer\" portlet (WCM Rendering Portlet) with WSRP The Local Content View portlet has no architectural replacement. Instead, use WCM Rendering portlet locally. The feature allows customers to run the content viewer on a remote portal and integrate it into a local portal. You can run the content viewer only locally. - Documentation resource: Enabling remote rendering with WSRP and the Web Content Viewer Administering Frequent Users portlet - Documentation resource: Viewing frequent users Login filters can be used to record login activity as needed.- Documentation resource: New security APIs in HCL Portal Security OpenID authentication Legacy Portal OpenID TAI (com.ibm.portal.auth.tai.OpenidTAI provided by HCL Digital Experience) See Integrating with OpenID authentication For authentication in HCL Digital Experience with an external Identity Provider use SAML/OpenID Connect TAIs provided by IBM WebSphere Application Server or custom TAIs.- Documentation resource: Establishing SSO connections through SAML 2.0 tokens Administering XMLAccess xsds for older releases (HCL Digital Experience Portal 5.0 - 7.0) Use the XML configuration interface with XML input files specified according to the latest schema file:\u202fPortalConfig_8.5.0.xsd. - Documentation resource: Using the XML configuration command line client Developing Enabler, Builder, and Mashups components and API The\u202f Mashup Enabler and the Builder \u202fcomponents are deprecated. The\u202f Mashups Enabler \u202fand the\u202f Builder \u202fAPI are deprecated. - Documentation resource: Mashup integration post migration steps Personalization LikeMinds The\u202f LikeMinds \u202fservices and database domains are deprecated.- Documentation resource: Using the XML configuration commands line client Go to Active Site Analytics to learn how to configure a variety of analysis tools to support your Digital Experience platform requirements. - Documentation resource: Enabling Active Site Analytics for your Marketing Center Spot Personalization Feedback The\u202f Feedback \u202fdatabase domains are deprecated. - Documentation resource: Using the XML configuration commands line client Go to Active Site Analytics to learn how to configure a variety of analysis tools to support your Digital Experience platform requirements.- Documentation resource: Enabling Active Site Analytics for your Marketing Center Spot Security Active Credentials Active credentials are deprecated from the Credential Vault portlet. Passive credentials are still available. - Documentation resource: Credential Vault Administering Shared private pages The Sharing private pages service is deprecated. - Documentation resource: Sharing pages with other users Go to Managing Pages for the set of services available in HCL Digital Experience 8.5 and 9 releases. - Documentation resource: Managing pages Administering Parallel Portlet rendering The Parallel Portlet rendering feature is deprecated. See the\u202f HCL Digital Experience\u202fPerformance Tuning Guide \u202ffor recommendations to optimize digital experience application performance. Security Stand-alone LDAP user registry The stand-alone LDAP user registry configuration is deprecated. Instead, configure the federated LDAP user registry. If you upgraded from HCL Digital Experience 7.0 or 8.0 with a stand-alone LDAP user registry, you can continue to use your stand-alone LDAP user registry. However, run the\u202fwp-modify-federated-security\u202fto change to a federated LDAP user registry. - Documentation resource: Changing from a stand-alone repository to a federated repository Administering URL mappings URL mappings are deprecated. If you upgrade from HCL Digital Experience 8.0 to 8.5, you can continue to use your existing URL mappings, but creating new URL mappings is no longer supported. Use Vanity URLs or Friendly URLs Installation Full and Base installation options Before HCL Digital Experience 8.5, a customer chose either a full deployment with all the same pages and artifacts or a base deployment to customize their portal. Starting with HCL Digital Experience 8.5, the\u202f Configuration Wizard installs the full deployment. Customers can then remove pages to customize their portal. Then, they can package their customizations as a Portal Application Archive (PAA) file. Finally, customers can install their production server, run the\u202fempty-portal\u202ftask, and install the customization PAA file. Security LTPA version 1 token support The LTPA version 1 token is deprecated.\u202fWebSphere\u00ae\u202fApplication Server\u202f8.5.5. disables the LTPA version 1 token by default. If you are integrating with third-party applications that rely on LTPA version 1, update the application to support LTPA version 2. If you cannot update the application, you must manually re-enable LTPA version 1 support after you complete the migration. For information on updating your application to support LTPA version 2, see the documentation for the application Enabling or disabling single sign-on interoperability mode for the LTPA token for WebSphere\u00ae\u202fApplication Server\u202f8.5.5 and Enabling or disabling single sign-on interoperability mode for the LTPA token for WebSphere\u00ae\u202fApplication Server\u202f9.0.0","title":"Deprecated features"},{"location":"whatsnew/deprecated_features/#deprecated-features","text":"Deprecation means that the features are supported at this time but can be removed in an upcoming CF. It is recommended that deprecated items be removed as soon as possible as they can cause unpredictable behavior. Links to more information on the replacement for those items listed for deprecation will be posted as they become available to provide help as you move away from these features.","title":"Deprecated features"},{"location":"whatsnew/deprecated_features/#recently-deprecated-features","text":"Category Deprecated Recommended Action Effective Date Deploying Ambassador Beginning with Container Update CF203, the Ambassador shipped as part of the DX Helm deployment is deprecated and will be removed in a subsequent HCL DX 9.5 Container Update release. For more information, see HAProxy replaces Ambassador May 2022 Deploying Operator-based deployment Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments. There will be no further updates or code fixes provided for the Operator-based deployments. For more information, see Helm-based deployment December 2021 Integrating Sametime Integration Integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Integrating Add to Sametime List Effective November 2021, integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Online Help Sametime Web 2.0 Contact List Effective November 2021, integration with Sametime will be deprecated. The supported Sametime version 9.0 already reached EOS. November 2021 Developing Microsoft Exchange 2010 Calendar Effective November 2021, integration with Microsoft Exchange 2010 will be deprecated. Exchange 2010 is out of support for Microsoft. November 2021 Developing Microsoft Exchange 2010 Mail Effective November 2021, integration with Microsoft Exchange 2010 will be deprecated. Exchange 2010 is out of support for Microsoft. November 2021 Online Help Coach Effective November 2021, Coach portlet will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Online Help Dynamic Coach Effective November 2021, Dynamic Coach portlet will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Configuring Federated Documents Picker Effective November 2021, Federated Documents Picker feature will be deprecated. HCL Digital Asset Management may be use in replacement for this feature. November 2021 Online Help Frequent Users Effective November 2021, Frequent Users portlet will be deprecated. This is an obsolete portlet and no replacement is needed. November 2021 Setting up a website Syndicated Feed Portlet Effective November 2021, Syndicated Feed portlet will be deprecated. November 2021 Integrating TPIR Configuration Viewer Effective November 2021, TPIR Configuration Viewer will be deprecated. IBM no longer supports WebSphere Lombardi. November 2021 Developing UX Screen Flow Manager - Dialog Stack Portlet Effective November 2021, Dialog Stack portlet will be deprecated. The feature was already removed on the later version of Portal. November 2021 Developing UX Screen Flow Manager - Dialog State Display Portlet Effective November 2021, Dialog State Display portlet will be deprecated. The feature was already removed on the later version of Portal. November 2021 Setting up a website Welcome to WebSphere Portal Effective November 2021, Welcome portlet will be deprecated. This is an obsolete portlet and no replacement is needed. November 2021 Integrating Watson Content Hub support Effective November 2021, integration with Watson Content Hub will be deprecated. HCL Digital Asset Management may be use in replacement for this feature. November 2021 Integrating Web Application Bridge - Microsoft SharePoint Effective November 2021, integration with Microsoft SharePoint will be deprecated. November 2021 Integrating SAP NetWeaver Effective November 2021, integration with SAP NetWeaver will be deprecated. NetWeaver is no longer supported by SAP. November 2021 Integrating Domino Effective November 2021, integration with Domino will be deprecated. The iFrame integration approach is already obsolete and not used. November 2021 Integrating iNotes portlet Effective November 2021, integration with Domino will be deprecated. The iFrame integration approach is already obsolete and not used. November 2021 Migrating IBM\u00ae\u202fJSF Bridge In WAS 9.0.5.2 and 8.5.5.17, IBM removed the JSF Bridge from WAS. To address this, an updated JSF Portlet Bridge is included in DX CF18 release. Customers installing HCL DX CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 should utilize the new JSF Portlet Bridge. The HCL JSF Portlet Bridge is built using the same class names as the IBM JSF Portlet Bridge provided in WAS 9.0.0.7 thru 9.0.5.1 and WAS 8.5.5.16 respectively. User applications should be able to continue to function without any modification. December 2020","title":"Recently Deprecated Features"},{"location":"whatsnew/deprecated_features/#previously-deprecated-features","text":"Category Unsupported Recommended Action Setting up a site Running the \"Local Content Viewer\" portlet (WCM Rendering Portlet) with WSRP The Local Content View portlet has no architectural replacement. Instead, use WCM Rendering portlet locally. The feature allows customers to run the content viewer on a remote portal and integrate it into a local portal. You can run the content viewer only locally. - Documentation resource: Enabling remote rendering with WSRP and the Web Content Viewer Administering Frequent Users portlet - Documentation resource: Viewing frequent users Login filters can be used to record login activity as needed.- Documentation resource: New security APIs in HCL Portal Security OpenID authentication Legacy Portal OpenID TAI (com.ibm.portal.auth.tai.OpenidTAI provided by HCL Digital Experience) See Integrating with OpenID authentication For authentication in HCL Digital Experience with an external Identity Provider use SAML/OpenID Connect TAIs provided by IBM WebSphere Application Server or custom TAIs.- Documentation resource: Establishing SSO connections through SAML 2.0 tokens Administering XMLAccess xsds for older releases (HCL Digital Experience Portal 5.0 - 7.0) Use the XML configuration interface with XML input files specified according to the latest schema file:\u202fPortalConfig_8.5.0.xsd. - Documentation resource: Using the XML configuration command line client Developing Enabler, Builder, and Mashups components and API The\u202f Mashup Enabler and the Builder \u202fcomponents are deprecated. The\u202f Mashups Enabler \u202fand the\u202f Builder \u202fAPI are deprecated. - Documentation resource: Mashup integration post migration steps Personalization LikeMinds The\u202f LikeMinds \u202fservices and database domains are deprecated.- Documentation resource: Using the XML configuration commands line client Go to Active Site Analytics to learn how to configure a variety of analysis tools to support your Digital Experience platform requirements. - Documentation resource: Enabling Active Site Analytics for your Marketing Center Spot Personalization Feedback The\u202f Feedback \u202fdatabase domains are deprecated. - Documentation resource: Using the XML configuration commands line client Go to Active Site Analytics to learn how to configure a variety of analysis tools to support your Digital Experience platform requirements.- Documentation resource: Enabling Active Site Analytics for your Marketing Center Spot Security Active Credentials Active credentials are deprecated from the Credential Vault portlet. Passive credentials are still available. - Documentation resource: Credential Vault Administering Shared private pages The Sharing private pages service is deprecated. - Documentation resource: Sharing pages with other users Go to Managing Pages for the set of services available in HCL Digital Experience 8.5 and 9 releases. - Documentation resource: Managing pages Administering Parallel Portlet rendering The Parallel Portlet rendering feature is deprecated. See the\u202f HCL Digital Experience\u202fPerformance Tuning Guide \u202ffor recommendations to optimize digital experience application performance. Security Stand-alone LDAP user registry The stand-alone LDAP user registry configuration is deprecated. Instead, configure the federated LDAP user registry. If you upgraded from HCL Digital Experience 7.0 or 8.0 with a stand-alone LDAP user registry, you can continue to use your stand-alone LDAP user registry. However, run the\u202fwp-modify-federated-security\u202fto change to a federated LDAP user registry. - Documentation resource: Changing from a stand-alone repository to a federated repository Administering URL mappings URL mappings are deprecated. If you upgrade from HCL Digital Experience 8.0 to 8.5, you can continue to use your existing URL mappings, but creating new URL mappings is no longer supported. Use Vanity URLs or Friendly URLs Installation Full and Base installation options Before HCL Digital Experience 8.5, a customer chose either a full deployment with all the same pages and artifacts or a base deployment to customize their portal. Starting with HCL Digital Experience 8.5, the\u202f Configuration Wizard installs the full deployment. Customers can then remove pages to customize their portal. Then, they can package their customizations as a Portal Application Archive (PAA) file. Finally, customers can install their production server, run the\u202fempty-portal\u202ftask, and install the customization PAA file. Security LTPA version 1 token support The LTPA version 1 token is deprecated.\u202fWebSphere\u00ae\u202fApplication Server\u202f8.5.5. disables the LTPA version 1 token by default. If you are integrating with third-party applications that rely on LTPA version 1, update the application to support LTPA version 2. If you cannot update the application, you must manually re-enable LTPA version 1 support after you complete the migration. For information on updating your application to support LTPA version 2, see the documentation for the application Enabling or disabling single sign-on interoperability mode for the LTPA token for WebSphere\u00ae\u202fApplication Server\u202f8.5.5 and Enabling or disabling single sign-on interoperability mode for the LTPA token for WebSphere\u00ae\u202fApplication Server\u202f9.0.0","title":"Previously Deprecated Features"},{"location":"whatsnew/unsupported_features/","text":"Unsupported features Warning Unsupported features and themes must be removed prior to migration. Recentlty unsupported features Newly unsupported features and themes must be removed prior to migration. Category Unsupported Recommended Action Effective Date Configuring Web Content Manager Ephox EditLive! Java based Rich Text Editor The Ephox EditLive! Java based Rich Text Editor is no longer supported. You can also use the CK Editor as an alternative to Ephox EditLive! December 2020 Previously unsupported features Unsupported in HCL Digital Experience 8.5 and later Category Unsupported Recommended Action Site set up Web Clipper The Web Clipper portlet is no longer supported. Use the Web Application Bridge solution instead. Developing HCL themes from a previous version The themes Portal , PortalWeb2 and Tab Menu - Page Builder are no longer supported. They no longer work and are no longer supported. You must manually update these themes. Merge their function into a clean copy of a Portal 8.5 theme on the target server. The PageBuilder2 and Portal 7002 themes are deprecated in 8.5.- Documentation resource: Enabling a new functionality in a migrated portal Administering Composite applications Composite applications are no longer supported. If you have a composite application in your system and you are migrating to HCL Digital Experience 8.5, the migration fails. Ensure that all composite applications are deleted before you start the migration. When you delete a composite application, you must also run the resource cleaner, otherwise pages can still exist in the database. Documentation resource: Enabling new functionality in a migrated portal Developing CAI/TAI portlets When you are migrating from one version to another, your script can contain references to the CAI/TAI portlets. These portlets are no longer available and any reference to these portlets cause your script to fail. For more information, see\u202f Virtual Portal tasks . Developing IBM Portlet API The IBM Portlet API is no longer supported. Go to\u202f Converting HCL Digital Experience portlets (AIX IBM i Linux Solaris Windows) \u202fto learn how to convert your portlets that are based on IBM Portlet API to the Standard Portlet API. The JSR 168 standard API is still available and supported. Developing Struts Portlet Framework The historical struts-based portlet frameworks are deprecated. Both the struts-based portlet framework based on the IBM Portlet API and the struts-based portlet framework based on the JSR 168 standard API are no longer available and are removed from support. Move all struts-based portlets that are based on this HCL framework to the Apache Struts Portlet Framework. Portlet Plugin Integrating Brightcove player The Brightcove player is no longer supported. Administering SOAP support for remote search configuration SOAP support for remote search is no longer supported. EJB is still supported. Administering Login screen Login screens are no longer supported. If you need to customize your login process, you must use custom authentication filters instead of login screens. Developing Deprecated Business portlets The following portlets Reminder , Document Viewer , Webpage Portlet , My Query Reports and Microsoft Exchange 2003 were removed and are no longer supported. Integrating Deprecated Sametime portlets The Sametime portlets Who Is Here (SametimeWhoIsHere.war) Sametime Contact List (SametimeContactList.war) were removed and are no longer supported. Developing Deprecated Themes and Tags The <portal-core:cacheProxyUrl/> tag was removed and is no longer supported: - All Legacy themes Unsupported in HCL Digital Experience 8.0 and later Unsupported Recommended Action Policy Editor portlet for Mail CPP portlet is no longer shipped in WebSphere Portal Version 8.0 Composite applications Starting with Version 8.0, WebSphere Portal no longer supports Composite applications. You can still use the portal XML configuration interface to do this type of work. Computer Associates eTrust SiteMinder eTrust SiteMinder installations are not supported in this release. Domino Extended Product Portlet wizard The configuration wizard to configure the Domino and Extended Product portlet was supported with the InstallShield. Because the IBM Installation Manager is now supported, the Domino and Extended Product portlet configuration wizard is no longer supported. Integration with the IBM Mashup Center WebSphere Portal Version 8.0 provides direct support for widget integration to portal pages, which enables development of mashup style applications. WebSphere Portal Version 8.0 does not support promotion of assets from the IBM Mashup Center catalog directly into the portal. For information about how to add iWidgets to your portal see Managing iWidgets in your portal. For information about how to construct mashup page applications by using WebSphere Portal Version 8.0 features see the section about Managed pages of the WebSphere Portal and Web Content Manager Version 8.0 product documentation. Site management Earlier versions of WebSphere Portal provided the Resource Manager portlet for performing site management. With Version 8.0 of WebSphere Portal and Web Content Manager, this site management functionality has been replaced by the new functionality for managing pages. For more information refer to Managed pages. Session persistence on Struts portlets Struts portlets do not support session replication in WebSphere Portal Version 8.0. IBM Portlet API web content viewer portlet The web content viewer portlet based on the IBM Portlet API has been deprecated and is no longer supported. It was replaced with the JSR 286 web content viewer portlet. IBM Portlet API remote web content viewer portlet The remote web content viewer portlet based on the IBM Portlet API has been deprecated and is no longer supported. To display web content on a portal where Web Content Manager is not installed, use WSRP and the JSR 286 web content viewer. IBM API Rendering Portlet The IBM API Rendering Portlet has been deprecated and is no longer supported. Private wire APIs The WireModel artifacts referring to private wires have been deprecated and is no longer supported.","title":"Unsupported features"},{"location":"whatsnew/unsupported_features/#unsupported-features","text":"Warning Unsupported features and themes must be removed prior to migration.","title":"Unsupported features"},{"location":"whatsnew/unsupported_features/#recentlty-unsupported-features","text":"Newly unsupported features and themes must be removed prior to migration. Category Unsupported Recommended Action Effective Date Configuring Web Content Manager Ephox EditLive! Java based Rich Text Editor The Ephox EditLive! Java based Rich Text Editor is no longer supported. You can also use the CK Editor as an alternative to Ephox EditLive! December 2020","title":"Recentlty unsupported features"},{"location":"whatsnew/unsupported_features/#previously-unsupported-features","text":"","title":"Previously unsupported features"},{"location":"whatsnew/unsupported_features/#unsupported-in-hcl-digital-experience-85-and-later","text":"Category Unsupported Recommended Action Site set up Web Clipper The Web Clipper portlet is no longer supported. Use the Web Application Bridge solution instead. Developing HCL themes from a previous version The themes Portal , PortalWeb2 and Tab Menu - Page Builder are no longer supported. They no longer work and are no longer supported. You must manually update these themes. Merge their function into a clean copy of a Portal 8.5 theme on the target server. The PageBuilder2 and Portal 7002 themes are deprecated in 8.5.- Documentation resource: Enabling a new functionality in a migrated portal Administering Composite applications Composite applications are no longer supported. If you have a composite application in your system and you are migrating to HCL Digital Experience 8.5, the migration fails. Ensure that all composite applications are deleted before you start the migration. When you delete a composite application, you must also run the resource cleaner, otherwise pages can still exist in the database. Documentation resource: Enabling new functionality in a migrated portal Developing CAI/TAI portlets When you are migrating from one version to another, your script can contain references to the CAI/TAI portlets. These portlets are no longer available and any reference to these portlets cause your script to fail. For more information, see\u202f Virtual Portal tasks . Developing IBM Portlet API The IBM Portlet API is no longer supported. Go to\u202f Converting HCL Digital Experience portlets (AIX IBM i Linux Solaris Windows) \u202fto learn how to convert your portlets that are based on IBM Portlet API to the Standard Portlet API. The JSR 168 standard API is still available and supported. Developing Struts Portlet Framework The historical struts-based portlet frameworks are deprecated. Both the struts-based portlet framework based on the IBM Portlet API and the struts-based portlet framework based on the JSR 168 standard API are no longer available and are removed from support. Move all struts-based portlets that are based on this HCL framework to the Apache Struts Portlet Framework. Portlet Plugin Integrating Brightcove player The Brightcove player is no longer supported. Administering SOAP support for remote search configuration SOAP support for remote search is no longer supported. EJB is still supported. Administering Login screen Login screens are no longer supported. If you need to customize your login process, you must use custom authentication filters instead of login screens. Developing Deprecated Business portlets The following portlets Reminder , Document Viewer , Webpage Portlet , My Query Reports and Microsoft Exchange 2003 were removed and are no longer supported. Integrating Deprecated Sametime portlets The Sametime portlets Who Is Here (SametimeWhoIsHere.war) Sametime Contact List (SametimeContactList.war) were removed and are no longer supported. Developing Deprecated Themes and Tags The <portal-core:cacheProxyUrl/> tag was removed and is no longer supported: - All Legacy themes","title":"Unsupported in HCL Digital Experience 8.5 and later"},{"location":"whatsnew/unsupported_features/#unsupported-in-hcl-digital-experience-80-and-later","text":"Unsupported Recommended Action Policy Editor portlet for Mail CPP portlet is no longer shipped in WebSphere Portal Version 8.0 Composite applications Starting with Version 8.0, WebSphere Portal no longer supports Composite applications. You can still use the portal XML configuration interface to do this type of work. Computer Associates eTrust SiteMinder eTrust SiteMinder installations are not supported in this release. Domino Extended Product Portlet wizard The configuration wizard to configure the Domino and Extended Product portlet was supported with the InstallShield. Because the IBM Installation Manager is now supported, the Domino and Extended Product portlet configuration wizard is no longer supported. Integration with the IBM Mashup Center WebSphere Portal Version 8.0 provides direct support for widget integration to portal pages, which enables development of mashup style applications. WebSphere Portal Version 8.0 does not support promotion of assets from the IBM Mashup Center catalog directly into the portal. For information about how to add iWidgets to your portal see Managing iWidgets in your portal. For information about how to construct mashup page applications by using WebSphere Portal Version 8.0 features see the section about Managed pages of the WebSphere Portal and Web Content Manager Version 8.0 product documentation. Site management Earlier versions of WebSphere Portal provided the Resource Manager portlet for performing site management. With Version 8.0 of WebSphere Portal and Web Content Manager, this site management functionality has been replaced by the new functionality for managing pages. For more information refer to Managed pages. Session persistence on Struts portlets Struts portlets do not support session replication in WebSphere Portal Version 8.0. IBM Portlet API web content viewer portlet The web content viewer portlet based on the IBM Portlet API has been deprecated and is no longer supported. It was replaced with the JSR 286 web content viewer portlet. IBM Portlet API remote web content viewer portlet The remote web content viewer portlet based on the IBM Portlet API has been deprecated and is no longer supported. To display web content on a portal where Web Content Manager is not installed, use WSRP and the JSR 286 web content viewer. IBM API Rendering Portlet The IBM API Rendering Portlet has been deprecated and is no longer supported. Private wire APIs The WireModel artifacts referring to private wires have been deprecated and is no longer supported.","title":"Unsupported in HCL Digital Experience 8.0 and later"},{"location":"whatsnew/cf17/","text":"CF17 to CF173 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. CF173 Overview This HCL Digital Experience 9.5 release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. CF172 Overview The release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. CF171 Overview The releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"CF17 to CF173"},{"location":"whatsnew/cf17/#cf17-to-cf173","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases.","title":"CF17 to CF173"},{"location":"whatsnew/cf17/#cf173-overview","text":"This HCL Digital Experience 9.5 release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more.","title":"CF173 Overview"},{"location":"whatsnew/cf17/#cf172-overview","text":"The release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates.","title":"CF172 Overview"},{"location":"whatsnew/cf17/#cf171-overview","text":"The releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"CF171 Overview"},{"location":"whatsnew/cf17/new_cf17/","text":"What's new in CF17 Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance","title":"What's new in CF17"},{"location":"whatsnew/cf17/new_cf17/#whats-new-in-cf17","text":"Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance","title":"What's new in CF17"},{"location":"whatsnew/cf17/new_cf171/","text":"What's new in CF171 The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Password override in Docker Added option to override password in Docker. See Docker deployment . Password override in OpenShift Added option to override password in OpenShift. See OpenShift deployment . Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS) Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS . Support for Auto-scaling and Route configuration Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment . Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF171"},{"location":"whatsnew/cf17/new_cf171/#whats-new-in-cf171","text":"The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"What's new in CF171"},{"location":"whatsnew/cf17/new_cf171/#password-override-in-docker","text":"Added option to override password in Docker. See Docker deployment .","title":"Password override in Docker"},{"location":"whatsnew/cf17/new_cf171/#password-override-in-openshift","text":"Added option to override password in OpenShift. See OpenShift deployment .","title":"Password override in OpenShift"},{"location":"whatsnew/cf17/new_cf171/#support-for-kubernetes-as-verified-in-amazon-elastic-container-service-for-kubernetes-eks","text":"Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS .","title":"Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS)"},{"location":"whatsnew/cf17/new_cf171/#support-for-auto-scaling-and-route-configuration","text":"Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment .","title":"Support for Auto-scaling and Route configuration"},{"location":"whatsnew/cf17/new_cf171/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/cf17/new_cf172/","text":"What's new in CF172 The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details. New Web Content Query Parameter APIs New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details. New Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Updated HCL Digital Experience 9.5 platform support statements Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details. Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF172"},{"location":"whatsnew/cf17/new_cf172/#whats-new-in-cf172","text":"The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates.","title":"What's new in CF172"},{"location":"whatsnew/cf17/new_cf172/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"whatsnew/cf17/new_cf172/#new-web-content-query-parameter-apis","text":"New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details.","title":"New Web Content Query Parameter APIs"},{"location":"whatsnew/cf17/new_cf172/#new-enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"New Enhanced Content Template API"},{"location":"whatsnew/cf17/new_cf172/#updated-hcl-digital-experience-95-platform-support-statements","text":"Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details.","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"whatsnew/cf17/new_cf172/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/cf17/new_cf173/","text":"What's new in CF173 This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. Web Content Manager Mirror syndication - Disable full build option An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items . New WCM Restore Version REST API The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions . New Enhanced WCM Content Template API Element Configuration The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST . New WCM Export Digital Asset Management references API The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items . New Experience API samples Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls . New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details. New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details. Downloading DX products and accessing Customer Support The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support .","title":"What's new in CF173"},{"location":"whatsnew/cf17/new_cf173/#whats-new-in-cf173","text":"This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more.","title":"What's new in CF173"},{"location":"whatsnew/cf17/new_cf173/#web-content-manager-mirror-syndication-disable-full-build-option","text":"An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items .","title":"Web Content Manager Mirror syndication - Disable full build option"},{"location":"whatsnew/cf17/new_cf173/#new-wcm-restore-version-rest-api","text":"The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions .","title":"New WCM Restore Version REST API"},{"location":"whatsnew/cf17/new_cf173/#new-enhanced-wcm-content-template-api-element-configuration","text":"The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST .","title":"New Enhanced WCM Content Template API Element Configuration"},{"location":"whatsnew/cf17/new_cf173/#new-wcm-export-digital-asset-management-references-api","text":"The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items .","title":"New WCM Export Digital Asset Management references API"},{"location":"whatsnew/cf17/new_cf173/#new-experience-api-samples","text":"Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls .","title":"New Experience API samples"},{"location":"whatsnew/cf17/new_cf173/#new-hcl-content-composer-tech-preview-in-hcl-digital-experience-95-cf173","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details.","title":"New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"whatsnew/cf17/new_cf173/#new-digital-asset-management-tech-preview-in-hcl-digital-experience-95-cf173","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details.","title":"New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"whatsnew/cf17/new_cf173/#downloading-dx-products-and-accessing-customer-support","text":"The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support .","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/cf18/","text":"CF18 to CF184 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. CF184 Overview This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. CF183 Overview This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. CF182 Overview This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. CF181 Overview This HCL Digital Experience 9.5 release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. CF18 Overview This HCL Digital Experience 9.5 release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more.","title":"CF18 to CF184"},{"location":"whatsnew/cf18/#cf18-to-cf184","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases.","title":"CF18 to CF184"},{"location":"whatsnew/cf18/#cf184-overview","text":"This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"CF184 Overview"},{"location":"whatsnew/cf18/#cf183-overview","text":"This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"CF183 Overview"},{"location":"whatsnew/cf18/#cf182-overview","text":"This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more.","title":"CF182 Overview"},{"location":"whatsnew/cf18/#cf181-overview","text":"This HCL Digital Experience 9.5 release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more.","title":"CF181 Overview"},{"location":"whatsnew/cf18/#cf18-overview","text":"This HCL Digital Experience 9.5 release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more.","title":"CF18 Overview"},{"location":"whatsnew/cf18/new_cf18/","text":"What's new in CF18 This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site.","title":"What's new in CF18"},{"location":"whatsnew/cf18/new_cf18/#whats-new-in-cf18","text":"This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more.","title":"What's new in CF18"},{"location":"whatsnew/cf18/new_cf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"whatsnew/cf18/new_cf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"whatsnew/cf18/new_cf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"whatsnew/cf18/new_cf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"whatsnew/cf18/new_cf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"whatsnew/cf18/new_cf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"whatsnew/cf18/new_cf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"whatsnew/cf18/new_cf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site.","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"whatsnew/cf18/new_cf181/","text":"What's new in CF181 This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Content Composer Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details. Digital Asset Management Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details. Experience API The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details. OpenLDAP Container integration OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details. Transfer default Container database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details. New Digital Experience WCM Workflow REST APIs Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details. New Web Content Manager Reference REST API The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details. New Web Content Text Search REST API The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details. New Digital Experience Core Configuration REST API The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details.","title":"What's new in CF181"},{"location":"whatsnew/cf18/new_cf181/#whats-new-in-cf181","text":"This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more.","title":"What's new in CF181"},{"location":"whatsnew/cf18/new_cf181/#content-composer","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details.","title":"Content Composer"},{"location":"whatsnew/cf18/new_cf181/#digital-asset-management","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details.","title":"Digital Asset Management"},{"location":"whatsnew/cf18/new_cf181/#experience-api","text":"The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details.","title":"Experience API"},{"location":"whatsnew/cf18/new_cf181/#openldap-container-integration","text":"OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details.","title":"OpenLDAP Container integration"},{"location":"whatsnew/cf18/new_cf181/#transfer-default-container-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details.","title":"Transfer default Container database to IBM DB2"},{"location":"whatsnew/cf18/new_cf181/#new-digital-experience-wcm-workflow-rest-apis","text":"Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details.","title":"New Digital Experience WCM Workflow REST APIs"},{"location":"whatsnew/cf18/new_cf181/#new-web-content-manager-reference-rest-api","text":"The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details.","title":"New Web Content Manager Reference REST API"},{"location":"whatsnew/cf18/new_cf181/#new-web-content-text-search-rest-api","text":"The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details.","title":"New Web Content Text Search REST API"},{"location":"whatsnew/cf18/new_cf181/#new-digital-experience-core-configuration-rest-api","text":"The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details.","title":"New Digital Experience Core Configuration REST API"},{"location":"whatsnew/cf18/new_cf181/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details.","title":"Web Developer Toolkit"},{"location":"whatsnew/cf18/newcf182/","text":"What's new in CF182 This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Deploy HCL DX 9.5 Container updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details. Optional Digital Asset Management Storage Configuration Settings This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details.","title":"What's new in CF182"},{"location":"whatsnew/cf18/newcf182/#whats-new-in-cf182","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more.","title":"What's new in CF182"},{"location":"whatsnew/cf18/newcf182/#deploy-hcl-dx-95-container-updates-with-minimal-operations-downtime","text":"This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details.","title":"Deploy HCL DX 9.5 Container updates with minimal operations downtime"},{"location":"whatsnew/cf18/newcf182/#optional-digital-asset-management-storage-configuration-settings","text":"This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details.","title":"Optional Digital Asset Management Storage Configuration Settings"},{"location":"whatsnew/cf18/newcf183/","text":"What's new in CF183 This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information. Web Content Manager Lock/Unlock API The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information. Content Template Create/Update Option Element Selection API The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information. Search Component Results Display examples A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis","title":"What's new in CF183"},{"location":"whatsnew/cf18/newcf183/#whats-new-in-cf183","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"What's new in CF183"},{"location":"whatsnew/cf18/newcf183/#deploy-hcl-dx-95-container-cf182-or-higher-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information.","title":"Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS)"},{"location":"whatsnew/cf18/newcf183/#web-content-manager-lockunlock-api","text":"The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information.","title":"Web Content Manager Lock/Unlock API"},{"location":"whatsnew/cf18/newcf183/#content-template-createupdate-option-element-selection-api","text":"The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information.","title":"Content Template Create/Update Option Element Selection API"},{"location":"whatsnew/cf18/newcf183/#search-component-results-display-examples","text":"A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results.","title":"Search Component Results Display examples"},{"location":"whatsnew/cf18/newcf183/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf18/newcf184/","text":"What's new in CF184 This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Web Content Manager Syndication REST APIs The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information. Access the HCL Experience API in HCL DX GitHub The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift","title":"What's new in CF184"},{"location":"whatsnew/cf18/newcf184/#whats-new-in-cf184","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"What's new in CF184"},{"location":"whatsnew/cf18/newcf184/#web-content-manager-syndication-rest-apis","text":"The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information.","title":"Web Content Manager Syndication REST APIs"},{"location":"whatsnew/cf18/newcf184/#access-the-hcl-experience-api-in-hcl-dx-github","text":"The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information.","title":"Access the HCL Experience API in HCL DX GitHub"},{"location":"whatsnew/cf18/newcf184/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/","text":"CF19 to CF199 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Update Strategy Beginning with release CF196, CFs contain updates for both traditional and containerised deployments. CF199 Overview This HCL Digital Experience 9.5 and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. CF198 Overview This HCL Digital Experience 9.5 and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. CF197 Overview This HCL Digital Experience 9.5 and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. CF196 Overview This HCL Digital Experience 9.5 release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. CF195 Overview This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. CF194 Overview This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. CF193 Overview This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. CF192 Overview This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. CF191 Overview This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. CF19 Overview This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"CF19 to CF199"},{"location":"whatsnew/cf19/#cf19-to-cf199","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Update Strategy Beginning with release CF196, CFs contain updates for both traditional and containerised deployments.","title":"CF19 to CF199"},{"location":"whatsnew/cf19/#cf199-overview","text":"This HCL Digital Experience 9.5 and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more.","title":"CF199 Overview"},{"location":"whatsnew/cf19/#cf198-overview","text":"This HCL Digital Experience 9.5 and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more.","title":"CF198 Overview"},{"location":"whatsnew/cf19/#cf197-overview","text":"This HCL Digital Experience 9.5 and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here.","title":"CF197 Overview"},{"location":"whatsnew/cf19/#cf196-overview","text":"This HCL Digital Experience 9.5 release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here.","title":"CF196 Overview"},{"location":"whatsnew/cf19/#cf195-overview","text":"This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here.","title":"CF195 Overview"},{"location":"whatsnew/cf19/#cf194-overview","text":"This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update.","title":"CF194 Overview"},{"location":"whatsnew/cf19/#cf193-overview","text":"This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here.","title":"CF193 Overview"},{"location":"whatsnew/cf19/#cf192-overview","text":"This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images.","title":"CF192 Overview"},{"location":"whatsnew/cf19/#cf191-overview","text":"This HCL Digital Experience 9.5 release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported.","title":"CF191 Overview"},{"location":"whatsnew/cf19/#cf19-overview","text":"This HCL Digital Experience 9.5 release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"CF19 Overview"},{"location":"whatsnew/cf19/newcf19/","text":"What's new in CF19 This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Digital Asset Management and Kaltura Integration Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers","title":"What's new in CF19"},{"location":"whatsnew/cf19/newcf19/#whats-new-in-cf19","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more.","title":"What's new in CF19"},{"location":"whatsnew/cf19/newcf19/#deploy-hcl-digital-experience-95-container-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information.","title":"Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE)"},{"location":"whatsnew/cf19/newcf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"whatsnew/cf19/newcf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"whatsnew/cf19/newcf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"whatsnew/cf19/newcf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"whatsnew/cf19/newcf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"whatsnew/cf19/newcf19/#digital-asset-management-and-kaltura-integration","text":"Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information.","title":"Digital Asset Management and Kaltura Integration"},{"location":"whatsnew/cf19/newcf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets","title":"New Digital Experience REST APIs"},{"location":"whatsnew/cf19/newcf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf191/","text":"What's new in CF191 This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Deploy HCL Digital Experience 9.5 on HCL Solution Factory The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d. HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites.","title":"What's new in CF191"},{"location":"whatsnew/cf19/newcf191/#whats-new-in-cf191","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported.","title":"What's new in CF191"},{"location":"whatsnew/cf19/newcf191/#deploy-hcl-digital-experience-95-on-hcl-solution-factory","text":"The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d.","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory"},{"location":"whatsnew/cf19/newcf191/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites.","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"whatsnew/cf19/newcf192/","text":"What's new in CF192 This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. HCL Digital Experience 9.5 Docker and Container Initialization Performance Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information. HCL Digital Experience 9.5 Container Core Transaction Logging Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Content Composer Features New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer topic for additional information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5","title":"What's new in CF192"},{"location":"whatsnew/cf19/newcf192/#whats-new-in-cf192","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images.","title":"What's new in CF192"},{"location":"whatsnew/cf19/newcf192/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information.","title":"HCL Digital Experience 9.5 Docker and Container Initialization Performance"},{"location":"whatsnew/cf19/newcf192/#hcl-digital-experience-95-container-core-transaction-logging","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Core Transaction Logging"},{"location":"whatsnew/cf19/newcf192/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf192/#new-content-composer-features","text":"New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer topic for additional information.","title":"New Content Composer Features"},{"location":"whatsnew/cf19/newcf192/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"whatsnew/cf19/newcf192/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"whatsnew/cf19/newcf192/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"whatsnew/cf19/newcf192/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"whatsnew/cf19/newcf192/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf193/","text":"What's new in CF193 This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. HCL Digital Experience 9.5 Container Custom Context Root URL Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository","title":"What's new in CF193"},{"location":"whatsnew/cf19/newcf193/#whats-new-in-cf193","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here.","title":"What's new in CF193"},{"location":"whatsnew/cf19/newcf193/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"whatsnew/cf19/newcf193/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"whatsnew/cf19/newcf193/#hcl-digital-experience-95-container-custom-context-root-url","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information.","title":"HCL Digital Experience 9.5 Container Custom Context Root URL"},{"location":"whatsnew/cf19/newcf193/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"whatsnew/cf19/newcf193/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"whatsnew/cf19/newcf193/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf193/#new-digital-experience-rest-apis","text":"New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"whatsnew/cf19/newcf193/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/cf19/newcf193/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf194/","text":"What's new in CF194 This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF194"},{"location":"whatsnew/cf19/newcf194/#whats-new-in-cf194","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF194"},{"location":"whatsnew/cf19/newcf195/","text":"What's new in CF195 This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Web Content Manager Multilingual Solution Library Export and Import The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient ) topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information. Define No Context Root in for HCL Digital Experience 9.5 container deployments Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 ){:target=\"_blank\"} Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience","title":"What's new in CF195"},{"location":"whatsnew/cf19/newcf195/#whats-new-in-cf195","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here.","title":"What's new in CF195"},{"location":"whatsnew/cf19/newcf195/#web-content-manager-multilingual-solution-library-export-and-import","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient ) topic for more information.","title":"Web Content Manager Multilingual Solution Library Export and Import"},{"location":"whatsnew/cf19/newcf195/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"whatsnew/cf19/newcf195/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"whatsnew/cf19/newcf195/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf195/#remote-search-configuration-for-hcl-digital-experience-95-deployments-on-kubernetes-platforms","text":"Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information.","title":"Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms"},{"location":"whatsnew/cf19/newcf195/#define-no-context-root-in-for-hcl-digital-experience-95-container-deployments","text":"Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information.","title":"Define No Context Root in for HCL Digital Experience 9.5 container deployments"},{"location":"whatsnew/cf19/newcf195/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/cf19/newcf195/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 ){:target=\"_blank\"} Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf196/","text":"What's new in CF196 This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Note For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information. Deploy HCL DX CF196 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5","title":"What's new in CF196"},{"location":"whatsnew/cf19/newcf196/#whats-new-in-cf196","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Note For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic.","title":"What's new in CF196"},{"location":"whatsnew/cf19/newcf196/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf19/newcf196/#deploy-hcl-dx-cf196-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF196 to container platforms using Helm"},{"location":"whatsnew/cf19/newcf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"whatsnew/cf19/newcf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"whatsnew/cf19/newcf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf196/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/cf19/newcf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf197/","text":"What's new in CF197 This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF197 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"What's new in CF197"},{"location":"whatsnew/cf19/newcf197/#whats-new-in-cf197","text":"This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here.","title":"What's new in CF197"},{"location":"whatsnew/cf19/newcf197/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf19/newcf197/#deploy-hcl-dx-cf197-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF197 to container platforms using Helm"},{"location":"whatsnew/cf19/newcf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf197/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/cf19/newcf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf198/","text":"What's new in CF198 This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF198 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster {:target=\"_blank\"} Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"What's new in CF198"},{"location":"whatsnew/cf19/newcf198/#whats-new-in-cf198","text":"This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more.","title":"What's new in CF198"},{"location":"whatsnew/cf19/newcf198/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf19/newcf198/#deploy-hcl-dx-cf198-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF198 to container platforms using Helm"},{"location":"whatsnew/cf19/newcf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"whatsnew/cf19/newcf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL Experience API topic for more information","title":"New Experience APIs"},{"location":"whatsnew/cf19/newcf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"whatsnew/cf19/newcf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster {:target=\"_blank\"} Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf19/newcf198/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/cf19/newcf199/","text":"What's new in CF199 This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Migrate from HCL DX 9.5 Operator to Helm Deployments Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management Staging New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Digital Experience Remote Model REST API Explorer topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs","title":"What's new in CF199"},{"location":"whatsnew/cf19/newcf199/#whats-new-in-cf199","text":"This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more.","title":"What's new in CF199"},{"location":"whatsnew/cf19/newcf199/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf19/newcf199/#migrate-from-hcl-dx-95-operator-to-helm-deployments","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Migrate from HCL DX 9.5 Operator to Helm Deployments"},{"location":"whatsnew/cf19/newcf199/#digital-asset-management-staging","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information.","title":"Digital Asset Management Staging"},{"location":"whatsnew/cf19/newcf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/cf19/newcf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"whatsnew/cf19/newcf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"whatsnew/cf19/newcf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Digital Experience Remote Model REST API Explorer topic for more information.","title":"New REST APIs to Configure Remote Search"},{"location":"whatsnew/cf19/newcf199/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/cf20/","text":"CF200 and later The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Update Strategy Beginning with release CF200, the DX kubernetes operators are not longer supported. CF203 Overview HCL Digital Experience 9.5 Container Update and CF203 release includes new Design Studio (Beta) Content list and container reuse features, Helm configuration of metrics enablement, deploy to internal network, and Ambassador to HA Proxy migration support. The release also adds support for DAM metadata configurations using Extensibility functions, Content Composer in Virtual Portal configuration, Personalization Visibility rules in anonymous use case sample and updates, support for Dynamic Properties, Profiler Rule and update, options to customize the DX Site Manager interface, Notice of deprecation and replacement of Document Conversion services notice, new complementary DX training modules in the HCL Software Academy, and more. CF202 Overview HCL Digital Experience 9.5 Container Update and CF202 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new Remote Model REST APIs, Personalization REST APIs, updated Helm deployment guidance to create Persistent Volume Claims for DX Core, configure credentials, also new HCL Digital Experience Cloud Native 9.5 offering, new Digital Experience How-To Videos and Lab exercises, and more. CF201 Overview HCL Digital Experience 9.5 and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. CF200 Overview HCL Digital Experience 9.5 and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more.","title":"CF200 and later"},{"location":"whatsnew/cf20/#cf200-and-later","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Update Strategy Beginning with release CF200, the DX kubernetes operators are not longer supported.","title":"CF200 and later"},{"location":"whatsnew/cf20/#cf203-overview","text":"HCL Digital Experience 9.5 Container Update and CF203 release includes new Design Studio (Beta) Content list and container reuse features, Helm configuration of metrics enablement, deploy to internal network, and Ambassador to HA Proxy migration support. The release also adds support for DAM metadata configurations using Extensibility functions, Content Composer in Virtual Portal configuration, Personalization Visibility rules in anonymous use case sample and updates, support for Dynamic Properties, Profiler Rule and update, options to customize the DX Site Manager interface, Notice of deprecation and replacement of Document Conversion services notice, new complementary DX training modules in the HCL Software Academy, and more.","title":"CF203 Overview"},{"location":"whatsnew/cf20/#cf202-overview","text":"HCL Digital Experience 9.5 Container Update and CF202 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new Remote Model REST APIs, Personalization REST APIs, updated Helm deployment guidance to create Persistent Volume Claims for DX Core, configure credentials, also new HCL Digital Experience Cloud Native 9.5 offering, new Digital Experience How-To Videos and Lab exercises, and more.","title":"CF202 Overview"},{"location":"whatsnew/cf20/#cf201-overview","text":"HCL Digital Experience 9.5 and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more.","title":"CF201 Overview"},{"location":"whatsnew/cf20/#cf200-overview","text":"HCL Digital Experience 9.5 and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more.","title":"CF200 Overview"},{"location":"whatsnew/cf20/newcf200/","text":"What's new in CF200 This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF200 include Page creation shortcuts, Delete Site from Overview menu, Define Site base stylesheet, Rename Content container and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF200, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information. Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF200, new servies and updates include support for Sidecars for logging of Remote Search, define central logs location, Incubator section for future DX 9.5 Container Update features, Configuration of labels and annotations, also environment variables for different DX resources, Use of Persistent Volumes for DX 9.5 Core, Digital Asset Management, and Persistence services file storage, migration process for the Core profile from Operator to Helm deployment, and Helm based Version to Version Update process. Important Beginning with HCL DX Container Update CF200, use of the Operator (dxctl) method of container deployment is not supported. Customers should use Helm deployments and migrate existing Operator-based deployments to Helm. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging Help Center topic for more information. Support to migrate from the old to new Digital Asset Management database in the Helm-based deployments. See the Migrate to new DAM DB in Helm-based deployments Help Center topic for more information. Enhancements to DXClient The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export or Import of Web Content Manager libraries from source server to target server location, also ability to generate differential reports for DX Server configurations. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New User and Groups REST API Explorer The remote PUMA SPI gives you access to user profiles through REST services. It provides a remote interface for user and group management for the configured HCL DX user repository. Beginning with HCL DX 9.5 Container Update and CF 199, a new API explorer is available that allows developers using the Portal User Interface APIs to explore and test these APIs. See the Help Center topic Remote REST service for PUMA for additional information. New Personalization REST APIs and Explorer Beginning with HCL DX 9.5 Container Update and CF200, a new API explorer is available that allows developers using the new Digital Experience Personalization REST APIs to explore and test these APIs. See the Digital Experience Personalization Help Center topic for more information. New Search REST APIs Explorer The Digital Experience Search REST API provides developers programmatic access to search indexed Digital Experience content and web pages. Beginning with Container and CF Update CF200, a new Digital Experience Search REST API Explorer allows developers to explore and test the Digital Experience Search REST APIs. See the HCL Digital Experience Search REST API Specification Help Center topic for more information. Access and Deploy HCL Digital Experience 9.5 on HCL Sofy HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. Using HCL SoFy to access and deploy HCL Digital Experience 9.5, and other HCL software offerings and demo packs, you can quickly gain hands-on experience working with these cloud-native solutions. See the Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) Help Center topic for more information. New How-To Video Take advantage of step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos included in HCL Digital Experience Help Center topics. New : Learn how to manage and monitor HCL DX 9.5 Container Deployment Liveliness and Readiness probes. See the Help Center topic: Operations using Helm . End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with HCL Digital Experience Container update and CF 200. Reference the Help Center topic Deprecated features and themes for HCL Digital Experience 9.5 .","title":"What's new in CF200"},{"location":"whatsnew/cf20/newcf200/#whats-new-in-cf200","text":"This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more.","title":"What's new in CF200"},{"location":"whatsnew/cf20/newcf200/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF200 include Page creation shortcuts, Delete Site from Overview menu, Define Site base stylesheet, Rename Content container and more. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF200, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf20/newcf200/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF200, new servies and updates include support for Sidecars for logging of Remote Search, define central logs location, Incubator section for future DX 9.5 Container Update features, Configuration of labels and annotations, also environment variables for different DX resources, Use of Persistent Volumes for DX 9.5 Core, Digital Asset Management, and Persistence services file storage, migration process for the Core profile from Operator to Helm deployment, and Helm based Version to Version Update process. Important Beginning with HCL DX Container Update CF200, use of the Operator (dxctl) method of container deployment is not supported. Customers should use Helm deployments and migrate existing Operator-based deployments to Helm. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/cf20/newcf200/#digital-asset-management","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging Help Center topic for more information. Support to migrate from the old to new Digital Asset Management database in the Helm-based deployments. See the Migrate to new DAM DB in Helm-based deployments Help Center topic for more information.","title":"Digital Asset Management"},{"location":"whatsnew/cf20/newcf200/#enhancements-to-dxclient","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export or Import of Web Content Manager libraries from source server to target server location, also ability to generate differential reports for DX Server configurations. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"Enhancements to DXClient"},{"location":"whatsnew/cf20/newcf200/#new-user-and-groups-rest-api-explorer","text":"The remote PUMA SPI gives you access to user profiles through REST services. It provides a remote interface for user and group management for the configured HCL DX user repository. Beginning with HCL DX 9.5 Container Update and CF 199, a new API explorer is available that allows developers using the Portal User Interface APIs to explore and test these APIs. See the Help Center topic Remote REST service for PUMA for additional information.","title":"New User and Groups REST API Explorer"},{"location":"whatsnew/cf20/newcf200/#new-personalization-rest-apis-and-explorer","text":"Beginning with HCL DX 9.5 Container Update and CF200, a new API explorer is available that allows developers using the new Digital Experience Personalization REST APIs to explore and test these APIs. See the Digital Experience Personalization Help Center topic for more information.","title":"New Personalization REST APIs and Explorer"},{"location":"whatsnew/cf20/newcf200/#new-search-rest-apis-explorer","text":"The Digital Experience Search REST API provides developers programmatic access to search indexed Digital Experience content and web pages. Beginning with Container and CF Update CF200, a new Digital Experience Search REST API Explorer allows developers to explore and test the Digital Experience Search REST APIs. See the HCL Digital Experience Search REST API Specification Help Center topic for more information.","title":"New Search REST APIs Explorer"},{"location":"whatsnew/cf20/newcf200/#access-and-deploy-hcl-digital-experience-95-on-hcl-sofy","text":"HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. Using HCL SoFy to access and deploy HCL Digital Experience 9.5, and other HCL software offerings and demo packs, you can quickly gain hands-on experience working with these cloud-native solutions. See the Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) Help Center topic for more information.","title":"Access and Deploy HCL Digital Experience 9.5 on HCL Sofy"},{"location":"whatsnew/cf20/newcf200/#new-how-to-video","text":"Take advantage of step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos included in HCL Digital Experience Help Center topics. New : Learn how to manage and monitor HCL DX 9.5 Container Deployment Liveliness and Readiness probes. See the Help Center topic: Operations using Helm .","title":"New How-To Video"},{"location":"whatsnew/cf20/newcf200/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with HCL Digital Experience Container update and CF 200. Reference the Help Center topic Deprecated features and themes for HCL Digital Experience 9.5 .","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"whatsnew/cf20/newcf201/","text":"What's new in CF201 This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF201: Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF201, updated capacity requirements are published for Digital Experience components, services, and logging. Options to configure environment values, expanded LDAP configuration guidance, and Prometheus metrics and Grafana visualization dashboard samples are provided for administrators to manage, monitor and optimize their deployments. See the following Help Center topics for more information: Containerization overview Containerization Requirements and Limitations Additional Helm Tasks Monitor the HCL Digital Experience Container Deployment using Metrics Digital Asset Management New Digital Asset Management (DAM) capability enables developers to use and customize an extensibility plugin, with a customized sample provided use with Digital Asset Management. The DAM Extensibility capability adds support DAM to process user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, while also supporting default and custom renditions. See the Using DAM Extensibility Help Center topic for more information. Script Application Updates New features and updates available for the Script Application include a new configuration task for use to configure Web Content Manager properties used to run Single Page Applications (SPAs) that use React or Angular. Improvements are added to support uploads of minified content for use in production SPAs. An out-of-the-box React profile, \u201cDeferred with React,\u201d is added for use with the Digital Experience 8.5 Theme . See the Script Application Improvements topic for more information: HCL Digital Experience 9.5 . Enhancements to DXClient The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to view and manipulate custom Resource Environment Provider (REP) settings, with examples. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Personalization REST APIs New Personalization REST APIs supporting Personalization Folder operations are available with HCL Digital Experience Container Update and CF201. See the Help Center topic Personalization Folder APIs for more information. Language switcher update Introduced in Container Update CF201, you can disable the language switcher by setting disable.languageSwitcher to true at the root page level in the configuration page. When set to true , the language switcher is hidden from the portal interface. See the Help Center topic Disabling the language switcher for more information. Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy for more information.","title":"What's new in CF201"},{"location":"whatsnew/cf20/newcf201/#whats-new-in-cf201","text":"This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF201:","title":"What's new in CF201"},{"location":"whatsnew/cf20/newcf201/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF201, updated capacity requirements are published for Digital Experience components, services, and logging. Options to configure environment values, expanded LDAP configuration guidance, and Prometheus metrics and Grafana visualization dashboard samples are provided for administrators to manage, monitor and optimize their deployments. See the following Help Center topics for more information: Containerization overview Containerization Requirements and Limitations Additional Helm Tasks Monitor the HCL Digital Experience Container Deployment using Metrics","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/cf20/newcf201/#digital-asset-management","text":"New Digital Asset Management (DAM) capability enables developers to use and customize an extensibility plugin, with a customized sample provided use with Digital Asset Management. The DAM Extensibility capability adds support DAM to process user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, while also supporting default and custom renditions. See the Using DAM Extensibility Help Center topic for more information.","title":"Digital Asset Management"},{"location":"whatsnew/cf20/newcf201/#script-application-updates","text":"New features and updates available for the Script Application include a new configuration task for use to configure Web Content Manager properties used to run Single Page Applications (SPAs) that use React or Angular. Improvements are added to support uploads of minified content for use in production SPAs. An out-of-the-box React profile, \u201cDeferred with React,\u201d is added for use with the Digital Experience 8.5 Theme . See the Script Application Improvements topic for more information: HCL Digital Experience 9.5 .","title":"Script Application Updates"},{"location":"whatsnew/cf20/newcf201/#enhancements-to-dxclient","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to view and manipulate custom Resource Environment Provider (REP) settings, with examples. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"Enhancements to DXClient"},{"location":"whatsnew/cf20/newcf201/#new-personalization-rest-apis","text":"New Personalization REST APIs supporting Personalization Folder operations are available with HCL Digital Experience Container Update and CF201. See the Help Center topic Personalization Folder APIs for more information.","title":"New Personalization REST APIs"},{"location":"whatsnew/cf20/newcf201/#language-switcher-update","text":"Introduced in Container Update CF201, you can disable the language switcher by setting disable.languageSwitcher to true at the root page level in the configuration page. When set to true , the language switcher is hidden from the portal interface. See the Help Center topic Disabling the language switcher for more information.","title":"Language switcher update"},{"location":"whatsnew/cf20/newcf201/#access-the-latest-hcl-digital-experience-95-education-materials-on-hcl-software-academy","text":"The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy for more information.","title":"Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy"},{"location":"whatsnew/cf20/newcf202/","text":"What's new in CF202 This HCL Digital Experience 9.5 Container Update and CF202 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new Remote Model REST APIs, Personalization REST APIs, updated Helm deployment guidance to create Persistent Volume Claims for DX Core, configure credentials, also new HCL Digital Experience Cloud Native 9.5 offering, new Digital Experience How-To Videos and Lab exercises, and more. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF202: Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF202, guidance is added to configure or adjust default credentials for Helm deployments, and configure additional DX Core Persistent Volume Claims (PVCs). See the following Help Center topics for more information: HCL DX 9.5 Helm deployment Configure PersistentVolumeClaims (PVCs) Additional Helm Tasks Digital Asset Management New Digital Asset Management (DAM) capability enables developers and release managers to use the DXClient capability to Export and Import DAM assets from source to a File system and Import from File system to target DX 9.5 deployment. Enhancements are also available for content authors to fix the duplicate DAM asset names in a particular collection, and there are new DAM APIs that allow you to access the assets by versions, name, and as reflected in the friendly URLs. See the DXClient and DXConnect tooling supporting CICD release processes and Digital Asset Management Help Center topics for more information. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF202 include baseline stylesheet upload and download in Site overview, guidance when styling elements, replacing images, using the Layers panel, and re-use of Content containers. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF202, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information. Digital Experience Cloud Native 9.5 offering Beginning with HCL Digital Experience 9.5 Container Update CF202, HCL Digital Experience Cloud Native 9.5 is available as a new offering option for customers to deploy the cloud-native Digital Experience 9.5 components and services on the Kubernetes container environments. HCL Digital Experience Cloud Native 9.5 is available with cloud-friendly subscription pricing. See the Digital Experience Cloud Native 9.5 details in the Digital Experience Overview Help Center topic. Digital Experience REST API Explorers Developers can accelerate development tasks using a set of Explorers provided for REST APIs available for use with HCL Digital Experience deployments to on premises platforms, and HCL Digital Experience 9.5 deployments to supported Kubernetes Container platforms. Developers can also initiate and test transactions or other capabilities using the DX REST API explorers. See the Digital Experience REST API Explorers Help Center topic for more information. Digital Experience Remote Model REST API Explorer The Digital Experience Remote Model REST API Explorer can be used by developers creating solutions for HCL DX on premises and HCL DX 9.5 container deployments to explore and test the Remote Model APIs. See the Digital Experience Remote Model REST API Explorer Help Center topic for more information. Enhancements to DXClient The HCL Digital Experience DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to Export and Import DAM assets from source to a File system and Import from File system to target DX 9.5 deployment, and DXConnect parameter updates. See the Deploy DX components using HCL DXClient and DXConnect Help Center topic for more information. Personalization REST APIs Personalization REST APIs supporting Personalization Folder Update and Delete operations are available with HCL Digital Experience Container Update and CF202. See the Help Center topics Personalization Folder APIs and Personalization Rules APIs for more information. Track User sessions in Digital Experience Deployments using Google Analytics Guidance is added to the Active Site Analytics integration for Google Analytics to track user sessions in Digital Experience deployments. See the Integrate Google Analytics with HCL Digital Experience Help Center topic for more information. Updates to Woodburn Studio Demo Site customizations during upgrades A new optional process is added to retain customizations made to the Woodburn Studio demonstration site during a Digital Experience upgrade. See the Upgrade to Woodburn Studio Pages Optional Help Center topic for additional information. Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy and What\u2019s New for Digital Experience section for more information. New Digital Experience 9.5 How-To Video: Update HCL DX 9.5 Container to a later DX version using Helm","title":"What's new in CF202"},{"location":"whatsnew/cf20/newcf202/#whats-new-in-cf202","text":"This HCL Digital Experience 9.5 Container Update and CF202 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new Remote Model REST APIs, Personalization REST APIs, updated Helm deployment guidance to create Persistent Volume Claims for DX Core, configure credentials, also new HCL Digital Experience Cloud Native 9.5 offering, new Digital Experience How-To Videos and Lab exercises, and more. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF202:","title":"What's new in CF202"},{"location":"whatsnew/cf20/newcf202/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF202, guidance is added to configure or adjust default credentials for Helm deployments, and configure additional DX Core Persistent Volume Claims (PVCs). See the following Help Center topics for more information: HCL DX 9.5 Helm deployment Configure PersistentVolumeClaims (PVCs) Additional Helm Tasks","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/cf20/newcf202/#digital-asset-management","text":"New Digital Asset Management (DAM) capability enables developers and release managers to use the DXClient capability to Export and Import DAM assets from source to a File system and Import from File system to target DX 9.5 deployment. Enhancements are also available for content authors to fix the duplicate DAM asset names in a particular collection, and there are new DAM APIs that allow you to access the assets by versions, name, and as reflected in the friendly URLs. See the DXClient and DXConnect tooling supporting CICD release processes and Digital Asset Management Help Center topics for more information.","title":"Digital Asset Management"},{"location":"whatsnew/cf20/newcf202/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF202 include baseline stylesheet upload and download in Site overview, guidance when styling elements, replacing images, using the Layers panel, and re-use of Content containers. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF202, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf20/newcf202/#digital-experience-cloud-native-95-offering","text":"Beginning with HCL Digital Experience 9.5 Container Update CF202, HCL Digital Experience Cloud Native 9.5 is available as a new offering option for customers to deploy the cloud-native Digital Experience 9.5 components and services on the Kubernetes container environments. HCL Digital Experience Cloud Native 9.5 is available with cloud-friendly subscription pricing. See the Digital Experience Cloud Native 9.5 details in the Digital Experience Overview Help Center topic.","title":"Digital Experience Cloud Native 9.5 offering"},{"location":"whatsnew/cf20/newcf202/#digital-experience-rest-api-explorers","text":"Developers can accelerate development tasks using a set of Explorers provided for REST APIs available for use with HCL Digital Experience deployments to on premises platforms, and HCL Digital Experience 9.5 deployments to supported Kubernetes Container platforms. Developers can also initiate and test transactions or other capabilities using the DX REST API explorers. See the Digital Experience REST API Explorers Help Center topic for more information.","title":"Digital Experience REST API Explorers"},{"location":"whatsnew/cf20/newcf202/#digital-experience-remote-model-rest-api-explorer","text":"The Digital Experience Remote Model REST API Explorer can be used by developers creating solutions for HCL DX on premises and HCL DX 9.5 container deployments to explore and test the Remote Model APIs. See the Digital Experience Remote Model REST API Explorer Help Center topic for more information.","title":"Digital Experience Remote Model REST API Explorer"},{"location":"whatsnew/cf20/newcf202/#enhancements-to-dxclient","text":"The HCL Digital Experience DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to Export and Import DAM assets from source to a File system and Import from File system to target DX 9.5 deployment, and DXConnect parameter updates. See the Deploy DX components using HCL DXClient and DXConnect Help Center topic for more information.","title":"Enhancements to DXClient"},{"location":"whatsnew/cf20/newcf202/#personalization-rest-apis","text":"Personalization REST APIs supporting Personalization Folder Update and Delete operations are available with HCL Digital Experience Container Update and CF202. See the Help Center topics Personalization Folder APIs and Personalization Rules APIs for more information.","title":"Personalization REST APIs"},{"location":"whatsnew/cf20/newcf202/#track-user-sessions-in-digital-experience-deployments-using-google-analytics","text":"Guidance is added to the Active Site Analytics integration for Google Analytics to track user sessions in Digital Experience deployments. See the Integrate Google Analytics with HCL Digital Experience Help Center topic for more information.","title":"Track User sessions in Digital Experience Deployments using Google Analytics"},{"location":"whatsnew/cf20/newcf202/#updates-to-woodburn-studio-demo-site-customizations-during-upgrades","text":"A new optional process is added to retain customizations made to the Woodburn Studio demonstration site during a Digital Experience upgrade. See the Upgrade to Woodburn Studio Pages Optional Help Center topic for additional information.","title":"Updates to Woodburn Studio Demo Site customizations during upgrades"},{"location":"whatsnew/cf20/newcf202/#access-the-latest-hcl-digital-experience-95-education-materials-on-hcl-software-academy","text":"The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy and What\u2019s New for Digital Experience section for more information.","title":"Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy"},{"location":"whatsnew/cf20/newcf202/#new-digital-experience-95-how-to-video","text":"Update HCL DX 9.5 Container to a later DX version using Helm","title":"New Digital Experience 9.5 How-To Video:"},{"location":"whatsnew/cf20/newcf203/","text":"What's new in CF203 HCL Digital Experience 9.5 Container Update and CF203 release includes new Design Studio (Beta) Content list and container reuse features, Helm configuration of metrics enablement, deploy to internal network, and Ambassador to HA Proxy migration support. The release also adds support for DAM metadata configurations using Extensibility functions, Content Composer in Virtual Portal configuration, Personalization Visibility rules in anonymous use case sample and updates, support for Dynamic Properties, Profiler Rule and update, options to customize the DX Site Manager interface, Notice of deprecation and replacement of Document Conversion services notice, new complementary DX training modules in the HCL Software Academy, and more. The following features and updates are available to customers installing HCL Digital Experience CF203 on supported container platforms: Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to the supported container platforms using Helm. Deployment using the Helm Chart can provide administrators more transparency and control in the deployment operations. Beginning with Container Update CF203, the Ambassador shipped as part of the DX Helm deployment is deprecated and will be removed in a subsequent HCL DX 9.5 Container Update release. HAProxy is taking its place and replaces functions performed by the Ambassador in the DX namespace. In CF203, Helm configuration settings for metrics are enabled by default, new guidance is added to deploy to an internal network, and guidance is provided explaining the process to migrate from the Ambassador to new HAProxy service. See the following Help Center topics HCL DX 9.5 Kubernetes , Monitoring , Deploying DX on internal network , and Migrate from Ambassador to HAProxy for more information. Important In Container Update CF203, the migration from Ambassador to HAProxy must be completed as a required step, in preparation for the removal of Ambassador in the upcoming Container Update release. Refer to the following links for guidance on the migration. See the following Help Center topics for more information: HCL DX 9.5 Helm deployment HAProxy overview Migrate from Ambassador to HAProxy Digital Asset Management Digital Asset Management (DAM) Extensibility adds capability to support user-defined custom renditions and configure transformations for assets. Updates add options to configure metadata generation specifically for MIME types and their renditions. See the Metadata configuration through DAM Extensibility Help Center topic for more information. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF203 include Content list and container re-use features. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF203,and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information. Configure Content Composer to Virtual Portals Instructions to configure Content Composer to Virtual Portals is available. See Content Composer - Configure to Virtual Portals for more information. Personalization REST APIs Personalization Visibility Rules REST APIs adds an anonymous use case sample and updates, and ability to apply Dynamic Properties. New Profile Create Rule APIs and update based on UUIDs are also added. See Personalization Rules APIs for more information. Customize the Site Manager Interface Guidance and examples are added to customize the Site Manager interface to support specific site and content management requirements. See the following Help Center topics for more information: - HCL DX 9.5: Customizing the Site Manager User Interface - HCL DX 8.5 and 9.0: Customizing the Site Manager User Interface Notice of deprecation and replacement of Document Conversion Services Document Conversion Services components in HCL Digital Experience software will be updated and replaced in 2022 in a subsequent HCL DX CF Update release. HCL Digital Experience will remove the third-party component providing these capabilities, supplied by Oracle, and replace with HCL supported functions. After that point, HCL Digital Experience v8.5, v9 and v9.5 Container Update and CF releases will include the newer HCL supported component. See the Replacemt of DCS component knowledge article for additional information. Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New \u201cWhat\u2019s New in the latest DX CF release\u2019 modules are available for Digital Experience business users, developers and administrators. See the HCL Digital Experience section of the HCL Software Academy and What\u2019s New for Digital Experience section for more information.","title":"What's new in CF203"},{"location":"whatsnew/cf20/newcf203/#whats-new-in-cf203","text":"HCL Digital Experience 9.5 Container Update and CF203 release includes new Design Studio (Beta) Content list and container reuse features, Helm configuration of metrics enablement, deploy to internal network, and Ambassador to HA Proxy migration support. The release also adds support for DAM metadata configurations using Extensibility functions, Content Composer in Virtual Portal configuration, Personalization Visibility rules in anonymous use case sample and updates, support for Dynamic Properties, Profiler Rule and update, options to customize the DX Site Manager interface, Notice of deprecation and replacement of Document Conversion services notice, new complementary DX training modules in the HCL Software Academy, and more. The following features and updates are available to customers installing HCL Digital Experience CF203 on supported container platforms:","title":"What's new in CF203"},{"location":"whatsnew/cf20/newcf203/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to the supported container platforms using Helm. Deployment using the Helm Chart can provide administrators more transparency and control in the deployment operations. Beginning with Container Update CF203, the Ambassador shipped as part of the DX Helm deployment is deprecated and will be removed in a subsequent HCL DX 9.5 Container Update release. HAProxy is taking its place and replaces functions performed by the Ambassador in the DX namespace. In CF203, Helm configuration settings for metrics are enabled by default, new guidance is added to deploy to an internal network, and guidance is provided explaining the process to migrate from the Ambassador to new HAProxy service. See the following Help Center topics HCL DX 9.5 Kubernetes , Monitoring , Deploying DX on internal network , and Migrate from Ambassador to HAProxy for more information. Important In Container Update CF203, the migration from Ambassador to HAProxy must be completed as a required step, in preparation for the removal of Ambassador in the upcoming Container Update release. Refer to the following links for guidance on the migration. See the following Help Center topics for more information: HCL DX 9.5 Helm deployment HAProxy overview Migrate from Ambassador to HAProxy","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/cf20/newcf203/#digital-asset-management","text":"Digital Asset Management (DAM) Extensibility adds capability to support user-defined custom renditions and configure transformations for assets. Updates add options to configure metadata generation specifically for MIME types and their renditions. See the Metadata configuration through DAM Extensibility Help Center topic for more information.","title":"Digital Asset Management"},{"location":"whatsnew/cf20/newcf203/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF203 include Content list and container re-use features. Note Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF203,and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/cf20/newcf203/#configure-content-composer-to-virtual-portals","text":"Instructions to configure Content Composer to Virtual Portals is available. See Content Composer - Configure to Virtual Portals for more information.","title":"Configure Content Composer to Virtual Portals"},{"location":"whatsnew/cf20/newcf203/#personalization-rest-apis","text":"Personalization Visibility Rules REST APIs adds an anonymous use case sample and updates, and ability to apply Dynamic Properties. New Profile Create Rule APIs and update based on UUIDs are also added. See Personalization Rules APIs for more information.","title":"Personalization REST APIs"},{"location":"whatsnew/cf20/newcf203/#customize-the-site-manager-interface","text":"Guidance and examples are added to customize the Site Manager interface to support specific site and content management requirements. See the following Help Center topics for more information: - HCL DX 9.5: Customizing the Site Manager User Interface - HCL DX 8.5 and 9.0: Customizing the Site Manager User Interface","title":"Customize the Site Manager Interface"},{"location":"whatsnew/cf20/newcf203/#notice-of-deprecation-and-replacement-of-document-conversion-services","text":"Document Conversion Services components in HCL Digital Experience software will be updated and replaced in 2022 in a subsequent HCL DX CF Update release. HCL Digital Experience will remove the third-party component providing these capabilities, supplied by Oracle, and replace with HCL supported functions. After that point, HCL Digital Experience v8.5, v9 and v9.5 Container Update and CF releases will include the newer HCL supported component. See the Replacemt of DCS component knowledge article for additional information.","title":"Notice of deprecation and replacement of Document Conversion Services"},{"location":"whatsnew/cf20/newcf203/#access-the-latest-hcl-digital-experience-95-education-materials-on-hcl-software-academy","text":"The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New \u201cWhat\u2019s New in the latest DX CF release\u2019 modules are available for Digital Experience business users, developers and administrators. See the HCL Digital Experience section of the HCL Software Academy and What\u2019s New for Digital Experience section for more information.","title":"Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy"}]}